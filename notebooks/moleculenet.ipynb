{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run fine-tuning on MoleculeNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26dcdcd117802dc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam/Projects/hybrid-transformer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T17:06:02.532440569Z",
     "start_time": "2024-01-04T17:06:02.528345688Z"
    }
   },
   "id": "422d4f35c0246db1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-04 18:09:38.115854: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-04 18:09:38.117617: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-04 18:09:38.141349: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-04 18:09:38.141364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-04 18:09:38.141982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-04 18:09:38.145979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-04 18:09:38.712423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from hybrid_transformer.configs.task import TaskConfig\n",
    "from hybrid_transformer.configs.model import ModelConfig\n",
    "from hybrid_transformer.configs.trainer import TrainerConfig\n",
    "from hybrid_transformer.configs.logger import LoggerConfig\n",
    "\n",
    "from hybrid_transformer.utils.datasets.auto import AutoDataset\n",
    "from hybrid_transformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hybrid_transformer.models.auto import AutoModel\n",
    "from hybrid_transformer.utils.loggers.wandb import WandbLogger\n",
    "\n",
    "from hybrid_transformer.trainers.trainer import Trainer\n",
    "\n",
    "from scripts.joint_learning.train import DEFAULT_CONFIG_FILES\n",
    "\n",
    "from hybrid_transformer.utils.objectives.guacamol.objective import GUACAMOL_TASKS\n",
    "from hybrid_transformer.models.prediction import PREDICTION_MODEL_CONFIGS\n",
    "from hybrid_transformer.utils.objectives.molecule_net.objective import MOLECULE_NET_REGRESSION_TASKS\n",
    "\n",
    "\n",
    "from scripts.pretrain.eval import DEFAULT_REFERENCE_FILE, evaluate_distribution_learning\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T17:09:39.311117759Z",
     "start_time": "2024-01-04T17:09:36.543292947Z"
    }
   },
   "id": "96dad29e95f4047e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 61,440\n",
      "Using cuda device\n",
      "Random seed set to 1337\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:09<?, ?it/s]\n",
      "  0%|          | 0/3 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nBrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 39\u001B[0m\n\u001B[1;32m     36\u001B[0m logger \u001B[38;5;241m=\u001B[39m WandbLogger(logger_config, [task_config, model_config, trainer_config])\n\u001B[1;32m     37\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(config\u001B[38;5;241m=\u001B[39mtrainer_config, model\u001B[38;5;241m=\u001B[39mmodel, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, logger\u001B[38;5;241m=\u001B[39mlogger)\n\u001B[0;32m---> 39\u001B[0m results_prediction \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/trainers/trainer.py:355\u001B[0m, in \u001B[0;36mTrainer.test\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    352\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mget_inputs(\n\u001B[1;32m    353\u001B[0m     dataset\u001B[38;5;241m=\u001B[39mdataset, task\u001B[38;5;241m=\u001B[39mtask, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, idx\u001B[38;5;241m=\u001B[39midx)\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx:\n\u001B[0;32m--> 355\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meos_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meos_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    359\u001B[0m predictions\u001B[38;5;241m.\u001B[39mextend(dataset\u001B[38;5;241m.\u001B[39mundo_target_transform(outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()))\n\u001B[1;32m    360\u001B[0m targets\u001B[38;5;241m.\u001B[39mextend(dataset\u001B[38;5;241m.\u001B[39mundo_target_transform(inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()))\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:490\u001B[0m, in \u001B[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001B[0;34m(frame, cache_entry, frame_state)\u001B[0m\n\u001B[1;32m    487\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compile_lock, _disable_current_modes():\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_entry\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:641\u001B[0m, in \u001B[0;36mconvert_frame.<locals>._convert_frame\u001B[0;34m(frame, cache_size, hooks, frame_state)\u001B[0m\n\u001B[1;32m    639\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 641\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43minner_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    642\u001B[0m     counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:133\u001B[0m, in \u001B[0;36mwrap_convert_context.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m cleanup \u001B[38;5;241m=\u001B[39m setup_compile_debug()\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     cleanup\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:389\u001B[0m, in \u001B[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001B[0;34m(frame, cache_entry, hooks, frame_state)\u001B[0m\n\u001B[1;32m    376\u001B[0m compile_id \u001B[38;5;241m=\u001B[39m CompileId(frame_id, frame_compile_id)\n\u001B[1;32m    378\u001B[0m signpost_event(\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_convert_frame_assert._compile\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     },\n\u001B[1;32m    387\u001B[0m )\n\u001B[0;32m--> 389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_globals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_locals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_builtins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompiler_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43mone_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport_constraints\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframe_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompile_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompile_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:569\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compile_context(CompileContext(compile_id)):\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 569\u001B[0m         guarded_code \u001B[38;5;241m=\u001B[39m \u001B[43mcompile_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mone_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    570\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m guarded_code\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    572\u001B[0m         Unsupported,\n\u001B[1;32m    573\u001B[0m         TorchRuntimeError,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    578\u001B[0m         ValidationException,\n\u001B[1;32m    579\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:491\u001B[0m, in \u001B[0;36m_compile.<locals>.compile_inner\u001B[0;34m(code, one_graph, hooks, transform)\u001B[0m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mcount():\n\u001B[1;32m    490\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 491\u001B[0m         out_code \u001B[38;5;241m=\u001B[39m \u001B[43mtransform_code_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    492\u001B[0m         orig_code_map[out_code] \u001B[38;5;241m=\u001B[39m code\n\u001B[1;32m    493\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1028\u001B[0m, in \u001B[0;36mtransform_code_object\u001B[0;34m(code, transformations, safe)\u001B[0m\n\u001B[1;32m   1025\u001B[0m instructions \u001B[38;5;241m=\u001B[39m cleaned_instructions(code, safe)\n\u001B[1;32m   1026\u001B[0m propagate_line_nums(instructions)\n\u001B[0;32m-> 1028\u001B[0m \u001B[43mtransformations\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstructions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:458\u001B[0m, in \u001B[0;36m_compile.<locals>.transform\u001B[0;34m(instructions, code_options)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tracing(tracer\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mtracing_context):\n\u001B[0;32m--> 458\u001B[0m         \u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (exc\u001B[38;5;241m.\u001B[39mRestartAnalysis, exc\u001B[38;5;241m.\u001B[39mSkipFrame):\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2074\u001B[0m, in \u001B[0;36mInstructionTranslator.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2073\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m-> 2074\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:724\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mpush_tx(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m (\n\u001B[1;32m    722\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstruction_pointer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    723\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mshould_exit\n\u001B[0;32m--> 724\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    725\u001B[0m     ):\n\u001B[1;32m    726\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BackendCompilerFailed:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:688\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    684\u001B[0m         unimplemented(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minst\u001B[38;5;241m.\u001B[39mopname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    685\u001B[0m     TracingContext\u001B[38;5;241m.\u001B[39mset_current_loc(\n\u001B[1;32m    686\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_filename, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineno, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\n\u001B[1;32m    687\u001B[0m     )\n\u001B[0;32m--> 688\u001B[0m     \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inst\u001B[38;5;241m.\u001B[39mopname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2162\u001B[0m, in \u001B[0;36mInstructionTranslator.RETURN_VALUE\u001B[0;34m(self, inst)\u001B[0m\n\u001B[1;32m   2157\u001B[0m _step_logger()(\n\u001B[1;32m   2158\u001B[0m     logging\u001B[38;5;241m.\u001B[39mINFO,\n\u001B[1;32m   2159\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchdynamo done tracing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (RETURN_VALUE)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2160\u001B[0m )\n\u001B[1;32m   2161\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE triggered compile\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2162\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_subgraph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2163\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2164\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreason\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mGraphCompileReason\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreturn_value\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mframe_summary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_break\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m   2166\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2167\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39madd_output_instructions([create_instruction(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE\u001B[39m\u001B[38;5;124m\"\u001B[39m)])\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:857\u001B[0m, in \u001B[0;36mOutputGraph.compile_subgraph\u001B[0;34m(self, tx, partial_convert, reason)\u001B[0m\n\u001B[1;32m    854\u001B[0m output \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count_calls(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pass2\u001B[38;5;241m.\u001B[39mgraph_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    856\u001B[0m     output\u001B[38;5;241m.\u001B[39mextend(\n\u001B[0;32m--> 857\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_and_call_fx_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpass2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_output_vars\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    858\u001B[0m     )\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pass2\u001B[38;5;241m.\u001B[39mgraph_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    861\u001B[0m         output\u001B[38;5;241m.\u001B[39mappend(pass2\u001B[38;5;241m.\u001B[39mcreate_store(graph_output_var))\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/contextlib.py:81\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:957\u001B[0m, in \u001B[0;36mOutputGraph.compile_and_call_fx_graph\u001B[0;34m(self, tx, rv, root)\u001B[0m\n\u001B[1;32m    952\u001B[0m graph_tabular_log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, lazy_format_graph_tabular(name, gm))\n\u001B[1;32m    953\u001B[0m graph_sizes_log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    954\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, LazyString(\u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_graph_sizes_log_str(name))\n\u001B[1;32m    955\u001B[0m )\n\u001B[0;32m--> 957\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_user_compiler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    958\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m disable(compiled_fn)\n\u001B[1;32m    960\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstats\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munique_graphs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1024\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     unimplemented_with_warning(e, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_tx\u001B[38;5;241m.\u001B[39mf_code, msg)\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BackendCompilerFailed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiler_fn, e)\u001B[38;5;241m.\u001B[39mwith_traceback(\n\u001B[1;32m   1025\u001B[0m         e\u001B[38;5;241m.\u001B[39m__traceback__\n\u001B[1;32m   1026\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m signpost_event(\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutputGraph.call_user_compiler\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1036\u001B[0m     },\n\u001B[1;32m   1037\u001B[0m )\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_fn\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1009\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mverify_correctness:\n\u001B[1;32m   1008\u001B[0m     compiler_fn \u001B[38;5;241m=\u001B[39m WrapperBackend(compiler_fn)\n\u001B[0;32m-> 1009\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexample_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1010\u001B[0m _step_logger()(logging\u001B[38;5;241m.\u001B[39mINFO, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone compiler function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(compiled_fn), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompiler_fn did not return callable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001B[0m, in \u001B[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001B[0;34m(gm, example_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     compiled_gm \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_gm\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/__init__.py:1568\u001B[0m, in \u001B[0;36m_TorchCompileInductorWrapper.__call__\u001B[0;34m(self, model_, inputs_)\u001B[0m\n\u001B[1;32m   1565\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_, inputs_):\n\u001B[1;32m   1566\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_inductor\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompile_fx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compile_fx\n\u001B[0;32m-> 1568\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompile_fx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig_patches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1150\u001B[0m, in \u001B[0;36mcompile_fx\u001B[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001B[0m\n\u001B[1;32m   1143\u001B[0m tracing_context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1144\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext\u001B[38;5;241m.\u001B[39mget() \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext(fake_mode)\n\u001B[1;32m   1145\u001B[0m )\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m V\u001B[38;5;241m.\u001B[39mset_fake_mode(fake_mode), torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mtracing(  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1148\u001B[0m     tracing_context\n\u001B[1;32m   1149\u001B[0m ), compiled_autograd\u001B[38;5;241m.\u001B[39mdisable():\n\u001B[0;32m-> 1150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43maot_autograd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfw_compiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_compiler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbw_compiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbw_compiler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1153\u001B[0m \u001B[43m        \u001B[49m\u001B[43minference_compiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minference_compiler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecompositions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecompositions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartition_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartition_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_inference_input_mutations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1157\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs_\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:55\u001B[0m, in \u001B[0;36maot_autograd.<locals>.compiler_fn\u001B[0;34m(gm, example_inputs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# NB: NOT cloned!\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m enable_aot_logging(), patch_config:\n\u001B[0;32m---> 55\u001B[0m         cg \u001B[38;5;241m=\u001B[39m \u001B[43maot_module_simplified\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m         counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maot_autograd\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m disable(cg)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3891\u001B[0m, in \u001B[0;36maot_module_simplified\u001B[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001B[0m\n\u001B[1;32m   3875\u001B[0m aot_config \u001B[38;5;241m=\u001B[39m AOTConfig(\n\u001B[1;32m   3876\u001B[0m     fw_compiler\u001B[38;5;241m=\u001B[39mfw_compiler,\n\u001B[1;32m   3877\u001B[0m     bw_compiler\u001B[38;5;241m=\u001B[39mbw_compiler,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3887\u001B[0m     no_tangents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   3888\u001B[0m )\n\u001B[1;32m   3890\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compiled_autograd\u001B[38;5;241m.\u001B[39mdisable():\n\u001B[0;32m-> 3891\u001B[0m     compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_aot_dispatcher_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunctional_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3893\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3894\u001B[0m \u001B[43m        \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3895\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3897\u001B[0m \u001B[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001B[39;00m\n\u001B[1;32m   3898\u001B[0m \u001B[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001B[39;00m\n\u001B[1;32m   3899\u001B[0m \u001B[38;5;66;03m# historically returned a function that was not the boxed calling\u001B[39;00m\n\u001B[1;32m   3900\u001B[0m \u001B[38;5;66;03m# convention.  This should get fixed...\u001B[39;00m\n\u001B[1;32m   3901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39mruntime_args):\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3429\u001B[0m, in \u001B[0;36mcreate_aot_dispatcher_function\u001B[0;34m(flat_fn, flat_args, aot_config)\u001B[0m\n\u001B[1;32m   3426\u001B[0m compiler_fn \u001B[38;5;241m=\u001B[39m partial(aot_wrapper_dedupe, compiler_fn\u001B[38;5;241m=\u001B[39mcompiler_fn)\n\u001B[1;32m   3427\u001B[0m \u001B[38;5;66;03m# You can put more passes here\u001B[39;00m\n\u001B[0;32m-> 3429\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfake_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfw_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_config\u001B[38;5;241m.\u001B[39mis_export:\n\u001B[1;32m   3432\u001B[0m     mutated_user_inp_locs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3433\u001B[0m         idx \u001B[38;5;241m-\u001B[39m aot_config\u001B[38;5;241m.\u001B[39mnum_params_buffers\n\u001B[1;32m   3434\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m fw_metadata\u001B[38;5;241m.\u001B[39mmutated_inp_indices\n\u001B[1;32m   3435\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m aot_config\u001B[38;5;241m.\u001B[39mnum_params_buffers\n\u001B[1;32m   3436\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2212\u001B[0m, in \u001B[0;36maot_wrapper_dedupe\u001B[0;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001B[0m\n\u001B[1;32m   2209\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   2211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ok:\n\u001B[0;32m-> 2212\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mleaf_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfw_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2214\u001B[0m \u001B[38;5;66;03m# export path: ban duplicate inputs for now, add later if requested.\u001B[39;00m\n\u001B[1;32m   2215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_config\u001B[38;5;241m.\u001B[39mis_export:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2392\u001B[0m, in \u001B[0;36maot_wrapper_synthetic_base\u001B[0;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001B[0m\n\u001B[1;32m   2390\u001B[0m \u001B[38;5;66;03m# Happy path: we don't need synthetic bases\u001B[39;00m\n\u001B[1;32m   2391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synthetic_base_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfw_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2394\u001B[0m \u001B[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001B[39;00m\n\u001B[1;32m   2395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_config\u001B[38;5;241m.\u001B[39mis_export:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1573\u001B[0m, in \u001B[0;36maot_dispatch_base\u001B[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001B[0m\n\u001B[1;32m   1571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext\u001B[38;5;241m.\u001B[39mget():\n\u001B[1;32m   1572\u001B[0m         torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext\u001B[38;5;241m.\u001B[39mget()\u001B[38;5;241m.\u001B[39mfw_metadata \u001B[38;5;241m=\u001B[39m fw_metadata\n\u001B[0;32m-> 1573\u001B[0m     compiled_fw \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfw_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1575\u001B[0m \u001B[38;5;66;03m# This boxed_call handling happens inside create_runtime_wrapper as well.\u001B[39;00m\n\u001B[1;32m   1576\u001B[0m \u001B[38;5;66;03m# However, create_runtime_wrapper does not expect the rng offsets in the\u001B[39;00m\n\u001B[1;32m   1577\u001B[0m \u001B[38;5;66;03m# output. So, we have to create another wrapper and take out the offset. As\u001B[39;00m\n\u001B[1;32m   1578\u001B[0m \u001B[38;5;66;03m# a result, we have to account for not boxed_call compilers as well.\u001B[39;00m\n\u001B[1;32m   1579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(compiled_fw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1092\u001B[0m, in \u001B[0;36mcompile_fx.<locals>.fw_compiler_base\u001B[0;34m(model, example_inputs, is_inference)\u001B[0m\n\u001B[1;32m   1070\u001B[0m     \u001B[38;5;66;03m# We makes the following assumption\u001B[39;00m\n\u001B[1;32m   1071\u001B[0m     \u001B[38;5;66;03m# For inference\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m     \u001B[38;5;66;03m#   len(orig_model_outputs) == len(model_outputs)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1080\u001B[0m     \u001B[38;5;66;03m# To make things safe, we'll use original_output_start_index field\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m     \u001B[38;5;66;03m# set by AOTAutograd to decide where the original module outputs start.\u001B[39;00m\n\u001B[1;32m   1083\u001B[0m     user_visible_outputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   1084\u001B[0m         n\u001B[38;5;241m.\u001B[39mname\n\u001B[1;32m   1085\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m model_outputs[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1089\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(n, torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mNode)\n\u001B[1;32m   1090\u001B[0m     }\n\u001B[0;32m-> 1092\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1093\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1094\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1095\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_fixed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1096\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcudagraphs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcudagraphs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgraph_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_inference\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_inference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1099\u001B[0m \u001B[43m    \u001B[49m\u001B[43mboxed_forward_device_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforward_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1100\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_visible_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_visible_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py:80\u001B[0m, in \u001B[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001B[0;34m(gm, example_inputs, **kwargs)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m config\u001B[38;5;241m.\u001B[39mrepro_after \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maot\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001B[39;00m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# with fake inputs\u001B[39;00m\n\u001B[0;32m---> 80\u001B[0m     inner_compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001B[39;00m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;66;03m# need a different serialization strategy\u001B[39;00m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mrepro_after \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maot\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/debug.py:228\u001B[0m, in \u001B[0;36mDebugContext.wrap.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m DebugContext():\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/contextlib.py:81\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:54\u001B[0m, in \u001B[0;36mtime_and_log.<locals>.wrap.<locals>.newFunction\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(old_func)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnewFunction\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mold_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:341\u001B[0m, in \u001B[0;36mcompile_fx_inner\u001B[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt)\u001B[0m\n\u001B[1;32m    328\u001B[0m graph_args \u001B[38;5;241m=\u001B[39m [gm, example_inputs]\n\u001B[1;32m    329\u001B[0m graph_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcudagraphs\u001B[39m\u001B[38;5;124m\"\u001B[39m: cudagraphs,\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_fixed\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_fixed,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayout_opt\u001B[39m\u001B[38;5;124m\"\u001B[39m: layout_opt,\n\u001B[1;32m    339\u001B[0m }\n\u001B[0;32m--> 341\u001B[0m compiled_graph: CompiledFxGraph \u001B[38;5;241m=\u001B[39m \u001B[43mfx_codegen_and_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgraph_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgraph_kwargs\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m    343\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_mode:\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m compiled_graph\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:565\u001B[0m, in \u001B[0;36mfx_codegen_and_compile\u001B[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt)\u001B[0m\n\u001B[1;32m    563\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    564\u001B[0m             context\u001B[38;5;241m.\u001B[39moutput_strides\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 565\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_to_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m graph\u001B[38;5;241m.\u001B[39mdisable_cudagraphs:\n\u001B[1;32m    568\u001B[0m     BoxedBool\u001B[38;5;241m.\u001B[39mdisable(cudagraphs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:970\u001B[0m, in \u001B[0;36mGraphLowering.compile_to_fn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m AotCodeCache\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;28mself\u001B[39m, code, cuda\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcuda)\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_to_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcall\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:941\u001B[0m, in \u001B[0;36mGraphLowering.compile_to_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    939\u001B[0m linemap \u001B[38;5;241m=\u001B[39m [(line_no, node\u001B[38;5;241m.\u001B[39mstack_trace) \u001B[38;5;28;01mfor\u001B[39;00m line_no, node \u001B[38;5;129;01min\u001B[39;00m linemap]\n\u001B[1;32m    940\u001B[0m key, path \u001B[38;5;241m=\u001B[39m PyCodeCache\u001B[38;5;241m.\u001B[39mwrite(code)\n\u001B[0;32m--> 941\u001B[0m mod \u001B[38;5;241m=\u001B[39m \u001B[43mPyCodeCache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_by_key_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlinemap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlinemap\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    942\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_key \u001B[38;5;241m=\u001B[39m key\n\u001B[1;32m    943\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_path \u001B[38;5;241m=\u001B[39m path\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:1139\u001B[0m, in \u001B[0;36mPyCodeCache.load_by_key_path\u001B[0;34m(cls, key, path, linemap)\u001B[0m\n\u001B[1;32m   1137\u001B[0m mod\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__file__\u001B[39m \u001B[38;5;241m=\u001B[39m path\n\u001B[1;32m   1138\u001B[0m mod\u001B[38;5;241m.\u001B[39mkey \u001B[38;5;241m=\u001B[39m key\n\u001B[0;32m-> 1139\u001B[0m \u001B[43mexec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__dict__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1140\u001B[0m sys\u001B[38;5;241m.\u001B[39mmodules[mod\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m] \u001B[38;5;241m=\u001B[39m mod\n\u001B[1;32m   1141\u001B[0m \u001B[38;5;66;03m# another thread might set this first\u001B[39;00m\n",
      "File \u001B[0;32m/tmp/torchinductor_adam/6u/c6uccdososh3sdbrmzn367eyy7xesx4ttmudmxwrwpv37mgnt23n.py:29\u001B[0m\n\u001B[1;32m     19\u001B[0m async_compile \u001B[38;5;241m=\u001B[39m AsyncCompile()\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# kernel path: /tmp/torchinductor_adam/su/csuifwrnqtn6rf3psmljoyw7y3cftrcrozprof4gfrh7et3rxiqa.py\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Source Nodes: [add, arange, l__self___transformer_wpe, l__self___transformer_wte, layer_norm], Original ATen: [aten.add, aten.arange, aten.embedding, aten.native_layer_norm]\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# add => add\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# l__self___transformer_wte => embedding\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# layer_norm => add_1, mul, mul_1, rsqrt, sub, var_mean\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m triton_red_fused_add_arange_embedding_native_layer_norm_0 \u001B[38;5;241m=\u001B[39m \u001B[43masync_compile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtriton\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtriton_\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'''\u001B[39;49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;43mimport triton\u001B[39;49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;43mimport triton.language as tl\u001B[39;49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;43mfrom torch._inductor.ir import ReductionHint\u001B[39;49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;43mfrom torch._inductor.ir import TileHint\u001B[39;49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;43mfrom torch._inductor.triton_heuristics import AutotuneHint, reduction\u001B[39;49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;43mfrom torch._inductor.utils import instance_descriptor\u001B[39;49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;43mfrom torch._inductor import triton_helpers\u001B[39;49m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;43m@reduction(\u001B[39;49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;43m    size_hints=[8192, 512],\u001B[39;49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;43m    reduction_hint=ReductionHint.DEFAULT,\u001B[39;49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;43m    filename=__file__,\u001B[39;49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;43m    meta=\u001B[39;49m\u001B[38;5;124;43m{\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msignature\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;124;43m{\u001B[39;49m\u001B[38;5;124;43m0: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m*i64\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, 1: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m*fp32\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, 2: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m*fp32\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, 3: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m*fp32\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, 4: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m*fp32\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, 5: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mi32\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, 6: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mi32\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m}, \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: 0, \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice_type\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mconstants\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmutated_arg_names\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: [], \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mautotune_hints\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: set(), \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkernel_name\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtriton_red_fused_add_arange_embedding_native_layer_norm_0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mconfigs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]}\u001B[39;49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;43m)\u001B[39;49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;43m@triton.jit\u001B[39;49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;43mdef triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):\u001B[39;49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;43m    xnumel = 8192\u001B[39;49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;43m    rnumel = 512\u001B[39;49m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;43m    xoffset = tl.program_id(0) * XBLOCK\u001B[39;49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;43m    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\u001B[39;49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;43m    xmask = xindex < xnumel\u001B[39;49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;124;43m    rbase = tl.arange(0, RBLOCK)[None, :]\u001B[39;49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124;43m    x3 = xindex\u001B[39;49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124;43m    tmp0 = tl.load(in_ptr0 + (x3), None, eviction_policy=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mevict_last\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124;43m    x0 = xindex \u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m 128\u001B[39;49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124;43m    tmp6_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\u001B[39;49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;43m    tmp6_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\u001B[39;49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;43m    tmp6_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\u001B[39;49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;43m    for roffset in range(0, rnumel, RBLOCK):\u001B[39;49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;124;43m        rindex = roffset + rbase\u001B[39;49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;43m        rmask = rindex < rnumel\u001B[39;49m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;124;43m        r2 = rindex\u001B[39;49m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;124;43m        tmp3 = tl.load(in_ptr2 + (r2 + (512*x0)), rmask, eviction_policy=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mevict_last\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, other=0)\u001B[39;49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;124;43m        tmp1 = tl.where(tmp0 < 0, tmp0 + 588, tmp0)\u001B[39;49m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;43m        tl.device_assert((0 <= tmp1) & (tmp1 < 588), \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mindex out of bounds: 0 <= tmp1 < 588\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;43m        tmp2 = tl.load(in_ptr1 + (r2 + (512*tmp1)), rmask, eviction_policy=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mevict_last\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, other=0)\u001B[39;49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;124;43m        tmp4 = tmp2 + tmp3\u001B[39;49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;43m        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])\u001B[39;49m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;124;43m        tmp6_mean_next, tmp6_m2_next, tmp6_weight_next = triton_helpers.welford_reduce(\u001B[39;49m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;124;43m            tmp5, tmp6_mean, tmp6_m2, tmp6_weight,\u001B[39;49m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;43m        )\u001B[39;49m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;43m        tmp6_mean = tl.where(rmask, tmp6_mean_next, tmp6_mean)\u001B[39;49m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;43m        tmp6_m2 = tl.where(rmask, tmp6_m2_next, tmp6_m2)\u001B[39;49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;43m        tmp6_weight = tl.where(rmask, tmp6_weight_next, tmp6_weight)\u001B[39;49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;124;43m    tmp6_tmp, tmp7_tmp, tmp8_tmp = triton_helpers.welford(\u001B[39;49m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;43m        tmp6_mean, tmp6_m2, tmp6_weight, 1\u001B[39;49m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;43m    )\u001B[39;49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;43m    tmp6 = tmp6_tmp[:, None]\u001B[39;49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;43m    tmp7 = tmp7_tmp[:, None]\u001B[39;49m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;43m    tmp8 = tmp8_tmp[:, None]\u001B[39;49m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;43m    for roffset in range(0, rnumel, RBLOCK):\u001B[39;49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;43m        rindex = roffset + rbase\u001B[39;49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;43m        rmask = rindex < rnumel\u001B[39;49m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;43m        r2 = rindex\u001B[39;49m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;43m        tmp11 = tl.load(in_ptr2 + (r2 + (512*x0)), rmask, eviction_policy=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mevict_last\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, other=0)\u001B[39;49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;43m        tmp20 = tl.load(in_ptr3 + (r2), rmask, eviction_policy=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mevict_last\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, other=0)\u001B[39;49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;43m        tmp9 = tl.where(tmp0 < 0, tmp0 + 588, tmp0)\u001B[39;49m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;43m        tl.device_assert((0 <= tmp9) & (tmp9 < 588), \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mindex out of bounds: 0 <= tmp9 < 588\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;124;43m        tmp10 = tl.load(in_ptr1 + (r2 + (512*tmp9)), rmask, other=0)\u001B[39;49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;43m        tmp12 = tmp10 + tmp11\u001B[39;49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;124;43m        tmp13 = tmp12 - tmp6\u001B[39;49m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;124;43m        tmp14 = 512.0\u001B[39;49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;124;43m        tmp15 = tmp7 / tmp14\u001B[39;49m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;43m        tmp16 = 1e-05\u001B[39;49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;124;43m        tmp17 = tmp15 + tmp16\u001B[39;49m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;124;43m        tmp18 = tl.math.rsqrt(tmp17)\u001B[39;49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;124;43m        tmp19 = tmp13 * tmp18\u001B[39;49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;43m        tmp21 = tmp19 * tmp20\u001B[39;49m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;43m        tl.store(out_ptr2 + (r2 + (512*x3)), tmp21, rmask)\u001B[39;49m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;43m'''\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtriton\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtl\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:1386\u001B[0m, in \u001B[0;36mAsyncCompile.triton\u001B[0;34m(self, kernel_name, source_code)\u001B[0m\n\u001B[1;32m   1384\u001B[0m     device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()\n\u001B[1;32m   1385\u001B[0m     cc \u001B[38;5;241m=\u001B[39m major \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m+\u001B[39m minor\n\u001B[0;32m-> 1386\u001B[0m     future \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_pool\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubmit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1387\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_worker_compile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m   1388\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m TritonFuture(kernel_name, source_code, future)\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/concurrent/futures/process.py:791\u001B[0m, in \u001B[0;36mProcessPoolExecutor.submit\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown_lock:\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_broken:\n\u001B[0;32m--> 791\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m BrokenProcessPool(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_broken)\n\u001B[1;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown_thread:\n\u001B[1;32m    793\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcannot schedule new futures after shutdown\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m: backend='inductor' raised:\nBrokenProcessPool: A child process terminated abruptly, the process pool is not usable anymore\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_CONFIG_FILES = {\n",
    "    'trainer': \"./configs/trainers/finetune-prediction/\",\n",
    "    'logger': \"./configs/loggers/wandb/\"\n",
    "}\n",
    "\n",
    "task_config_path = lambda: f'./configs/tasks/molecule_net/{task}/config.json'\n",
    "TASKS = MOLECULE_NET_REGRESSION_TASKS\n",
    "try:\n",
    "    PREDICTION_MODEL_CONFIGS.pop('GPTForPrediction')\n",
    "    PREDICTION_MODEL_CONFIGS.pop('JointGPT')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for task in tqdm(TASKS):\n",
    "    task_config = TaskConfig.from_pretrained(task_config_path())\n",
    "    task_config.augmentation_prob = 0.0  # disable augmentation\n",
    "    dataset = AutoDataset.from_config(task_config, split='test')\n",
    "    tokenizer = AutoTokenizer.from_config(task_config)\n",
    "\n",
    "    for model_name, path_to_model_config in tqdm(PREDICTION_MODEL_CONFIGS.items()):\n",
    "\n",
    "        model_config = ModelConfig.from_pretrained(path_to_model_config)\n",
    "\n",
    "        trainer_config = TrainerConfig.from_pretrained(DEFAULT_CONFIG_FILES['trainer'])\n",
    "        logger_config = LoggerConfig.from_pretrained(DEFAULT_CONFIG_FILES['logger'])\n",
    "        logger_config.wandb_log = False\n",
    "\n",
    "        run_dir = f'{model_name}/{task}'\n",
    "        out_dir = os.path.join('./notebooks', run_dir)\n",
    "        trainer_config.out_dir = out_dir\n",
    "        \n",
    "        logger_config.project = 'Table 1 ' + 'bug'\n",
    "        logger_config.name = model_name + '_' + task\n",
    "\n",
    "        model = AutoModel.from_config(model_config)\n",
    "        logger = WandbLogger(logger_config, [task_config, model_config, trainer_config])\n",
    "        trainer = Trainer(config=trainer_config, model=model, tokenizer=tokenizer, logger=logger)\n",
    "        \n",
    "        results_prediction = trainer.test(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T17:06:41.543974368Z",
     "start_time": "2024-01-04T17:06:30.342378265Z"
    }
   },
   "id": "63dbec52fc985c06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "70cd935b23575eac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
