{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Joint Learning Benchmark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26dcdcd117802dc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam/Projects/hybrid-transformer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T21:05:35.968053977Z",
     "start_time": "2023-12-30T21:05:35.964560673Z"
    }
   },
   "id": "422d4f35c0246db1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-30 22:05:39.050616: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-30 22:05:39.083278: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-30 22:05:39.235551: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-30 22:05:39.235585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-30 22:05:39.264343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-30 22:05:39.324205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-30 22:05:40.295390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from hybrid_transformer.configs.task import TaskConfig\n",
    "from hybrid_transformer.configs.model import ModelConfig\n",
    "from hybrid_transformer.configs.trainer import TrainerConfig\n",
    "from hybrid_transformer.configs.logger import LoggerConfig\n",
    "\n",
    "from hybrid_transformer.utils.datasets.auto import AutoDataset\n",
    "from hybrid_transformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hybrid_transformer.models.auto import AutoModel\n",
    "from hybrid_transformer.utils.loggers.wandb import WandbLogger\n",
    "\n",
    "from hybrid_transformer.trainers.trainer import Trainer\n",
    "\n",
    "from scripts.pretrain.train import DEFAULT_CONFIG_FILES\n",
    "\n",
    "from hybrid_transformer.utils.objectives.guacamol.objective import GUACAMOL_TASKS\n",
    "from hybrid_transformer.models.prediction import PREDICTION_MODEL_CONFIGS\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T21:05:41.554074038Z",
     "start_time": "2023-12-30T21:05:36.608053433Z"
    }
   },
   "id": "388ac09832efcd15"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Guacamol dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79568it [00:10, 7837.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guacamol dataset loaded with 79568 valid examples!\n",
      "Loaded osmb data.\n",
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 512\n",
      "Using cuda device\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:cfmfejow) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">JointGPT_osimertinib</strong> at: <a href='https://wandb.ai/adam-izdebski/debug/runs/cfmfejow' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/cfmfejow</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231230_221019-cfmfejow/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:cfmfejow). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adam/Projects/hybrid-transformer/wandb/run-20231230_224652-g28lfcng</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/adam-izdebski/debug/runs/g28lfcng' target=\"_blank\">GPTForPrediction_osimertinib</a></strong> to <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">https://wandb.ai/adam-izdebski/debug</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/adam-izdebski/debug/runs/g28lfcng' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/g28lfcng</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 0: train loss 0.0276, val loss 0.0550, percent 0.0000\n",
      "Evaluation at iter 1: train loss 0.0747, val loss 0.0538, percent 0.0000\n",
      "Evaluation at iter 2: train loss 0.0438, val loss 0.0227, percent 0.0000\n",
      "Training finished.0, time 2473.47ms.\n",
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 512\n",
      "Using cuda device\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:g28lfcng) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▅█</td></tr><tr><td>train/loss</td><td>▁█▃</td></tr><tr><td>train/loss_supervised</td><td>▁█▃</td></tr><tr><td>train/loss_unsupervised</td><td>█▁▃</td></tr><tr><td>val/loss</td><td>██▁</td></tr><tr><td>val/loss_supervised</td><td>██▁</td></tr><tr><td>val/loss_unsupervised</td><td>▁▃█</td></tr><tr><td>val/valid</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>2</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.04376</td></tr><tr><td>train/loss_supervised</td><td>0.04376</td></tr><tr><td>train/loss_unsupervised</td><td>3.61361</td></tr><tr><td>val/loss</td><td>0.02273</td></tr><tr><td>val/loss_supervised</td><td>0.02273</td></tr><tr><td>val/loss_unsupervised</td><td>3.88283</td></tr><tr><td>val/valid</td><td>0.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">GPTForPrediction_osimertinib</strong> at: <a href='https://wandb.ai/adam-izdebski/debug/runs/g28lfcng' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/g28lfcng</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231230_224652-g28lfcng/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:g28lfcng). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adam/Projects/hybrid-transformer/wandb/run-20231230_224742-a93gkppi</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/adam-izdebski/debug/runs/a93gkppi' target=\"_blank\">JointGPT_osimertinib</a></strong> to <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">https://wandb.ai/adam-izdebski/debug</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/adam-izdebski/debug/runs/a93gkppi' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/a93gkppi</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 0: train loss 4.3206, val loss 3.7270, percent 0.0000\n",
      "Evaluation at iter 1: train loss 3.4616, val loss 3.8427, percent 0.0000\n",
      "Evaluation at iter 2: train loss 3.3065, val loss 3.5399, percent 0.0000\n",
      "Training finished.7, time 2456.78ms.\n",
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 512\n",
      "Using cuda device\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:a93gkppi) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▅█</td></tr><tr><td>train/loss</td><td>█▂▁</td></tr><tr><td>train/loss_supervised</td><td>█▅▁</td></tr><tr><td>train/loss_unsupervised</td><td>█▁▂</td></tr><tr><td>val/loss</td><td>▅█▁</td></tr><tr><td>val/loss_supervised</td><td>▅█▁</td></tr><tr><td>val/loss_unsupervised</td><td>▃▁█</td></tr><tr><td>val/valid</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>2</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.30649</td></tr><tr><td>train/loss_supervised</td><td>0.19248</td></tr><tr><td>train/loss_unsupervised</td><td>3.11402</td></tr><tr><td>val/loss</td><td>3.53988</td></tr><tr><td>val/loss_supervised</td><td>0.29079</td></tr><tr><td>val/loss_unsupervised</td><td>3.24909</td></tr><tr><td>val/valid</td><td>0.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">JointGPT_osimertinib</strong> at: <a href='https://wandb.ai/adam-izdebski/debug/runs/a93gkppi' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/a93gkppi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231230_224742-a93gkppi/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:a93gkppi). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adam/Projects/hybrid-transformer/wandb/run-20231230_224951-8dcpfjtc</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/adam-izdebski/debug/runs/8dcpfjtc' target=\"_blank\">JointGPTNonLikelihood_osimertinib</a></strong> to <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">https://wandb.ai/adam-izdebski/debug</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/adam-izdebski/debug/runs/8dcpfjtc' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/8dcpfjtc</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 0: train loss 4.3445, val loss 3.5670, percent 0.0000\n",
      "Evaluation at iter 1: train loss 3.5383, val loss 3.8854, percent 0.0000\n",
      "Evaluation at iter 2: train loss 3.2390, val loss 3.5561, percent 0.0000\n",
      "Training finished.1, time 2458.56ms.\n",
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 512\n",
      "Using cuda device\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:8dcpfjtc) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▅█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>train/loss_supervised</td><td>█▅▁</td></tr><tr><td>train/loss_unsupervised</td><td>█▁▁</td></tr><tr><td>val/loss</td><td>▁█▁</td></tr><tr><td>val/loss_supervised</td><td>▅█▁</td></tr><tr><td>val/loss_unsupervised</td><td>▁▅█</td></tr><tr><td>val/valid</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>2</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.23899</td></tr><tr><td>train/loss_supervised</td><td>0.1892</td></tr><tr><td>train/loss_unsupervised</td><td>3.04979</td></tr><tr><td>val/loss</td><td>3.55609</td></tr><tr><td>val/loss_supervised</td><td>0.28659</td></tr><tr><td>val/loss_unsupervised</td><td>3.2695</td></tr><tr><td>val/valid</td><td>0.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">JointGPTNonLikelihood_osimertinib</strong> at: <a href='https://wandb.ai/adam-izdebski/debug/runs/8dcpfjtc' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/8dcpfjtc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231230_224951-8dcpfjtc/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:8dcpfjtc). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adam/Projects/hybrid-transformer/wandb/run-20231230_225040-u2s50mt5</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/adam-izdebski/debug/runs/u2s50mt5' target=\"_blank\">HybridTransformerGPTInit_osimertinib</a></strong> to <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">https://wandb.ai/adam-izdebski/debug</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/adam-izdebski/debug/runs/u2s50mt5' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/u2s50mt5</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 0: train loss 1.9493, val loss 1.4402, percent 0.0000\n",
      "Evaluation at iter 1: train loss 1.6378, val loss 1.6417, percent 0.0000\n",
      "Evaluation at iter 2: train loss 1.4221, val loss 1.4515, percent 0.0000\n",
      "Training finished.1, time 2439.95ms.\n",
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 512\n",
      "Using cuda device\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:u2s50mt5) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▅█</td></tr><tr><td>train/loss</td><td>█▄▁</td></tr><tr><td>train/loss_supervised</td><td>█▅▁</td></tr><tr><td>train/loss_unsupervised</td><td>█▅▁</td></tr><tr><td>val/loss</td><td>▁█▁</td></tr><tr><td>val/loss_supervised</td><td>█▇▁</td></tr><tr><td>val/loss_unsupervised</td><td>▂█▁</td></tr><tr><td>val/valid</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>2</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.42208</td></tr><tr><td>train/loss_supervised</td><td>0.15594</td></tr><tr><td>train/loss_unsupervised</td><td>2.23001</td></tr><tr><td>val/loss</td><td>1.45151</td></tr><tr><td>val/loss_supervised</td><td>0.20537</td></tr><tr><td>val/loss_unsupervised</td><td>2.2453</td></tr><tr><td>val/valid</td><td>0.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">HybridTransformerGPTInit_osimertinib</strong> at: <a href='https://wandb.ai/adam-izdebski/debug/runs/u2s50mt5' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/u2s50mt5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231230_225040-u2s50mt5/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:u2s50mt5). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adam/Projects/hybrid-transformer/wandb/run-20231230_225146-hlbi7h0q</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/adam-izdebski/debug/runs/hlbi7h0q' target=\"_blank\">HybridTransformer_osimertinib</a></strong> to <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">https://wandb.ai/adam-izdebski/debug</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/adam-izdebski/debug/runs/hlbi7h0q' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/hlbi7h0q</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 0: train loss 1.9106, val loss 1.5000, percent 0.0000\n",
      "Evaluation at iter 1: train loss 1.5707, val loss 1.6001, percent 0.0000\n",
      "Evaluation at iter 2: train loss 1.3970, val loss 1.4243, percent 0.0000\n",
      "Training finished.8, time 2454.09ms.\n",
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 512\n",
      "Using cuda device\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "Compiling model..\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:hlbi7h0q) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▅█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>train/loss_supervised</td><td>█▅▁</td></tr><tr><td>train/loss_unsupervised</td><td>█▄▁</td></tr><tr><td>val/loss</td><td>▄█▁</td></tr><tr><td>val/loss_supervised</td><td>█▆▁</td></tr><tr><td>val/loss_unsupervised</td><td>█▇▁</td></tr><tr><td>val/valid</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>2</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.39704</td></tr><tr><td>train/loss_supervised</td><td>0.15206</td></tr><tr><td>train/loss_unsupervised</td><td>2.20391</td></tr><tr><td>val/loss</td><td>1.42426</td></tr><tr><td>val/loss_supervised</td><td>0.19925</td></tr><tr><td>val/loss_unsupervised</td><td>2.20747</td></tr><tr><td>val/valid</td><td>0.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">HybridTransformer_osimertinib</strong> at: <a href='https://wandb.ai/adam-izdebski/debug/runs/hlbi7h0q' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/hlbi7h0q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231230_225146-hlbi7h0q/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:hlbi7h0q). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/adam/Projects/hybrid-transformer/wandb/run-20231230_225248-oxsj9vfh</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/adam-izdebski/debug/runs/oxsj9vfh' target=\"_blank\">HybridTransformerPenalty_osimertinib</a></strong> to <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/adam-izdebski/debug' target=\"_blank\">https://wandb.ai/adam-izdebski/debug</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/adam-izdebski/debug/runs/oxsj9vfh' target=\"_blank\">https://wandb.ai/adam-izdebski/debug/runs/oxsj9vfh</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 0: train loss 2.7217, val loss 2.3374, percent 0.0000\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacty of 1.83 GiB of which 94.94 MiB is free. Including non-PyTorch memory, this process has 1.73 GiB memory in use. Of the allocated memory 1.44 GiB is allocated by PyTorch, and 218.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 35\u001B[0m\n\u001B[1;32m     32\u001B[0m logger \u001B[38;5;241m=\u001B[39m WandbLogger(logger_config, [task_config, model_config, trainer_config])\n\u001B[1;32m     33\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(config\u001B[38;5;241m=\u001B[39mtrainer_config, model\u001B[38;5;241m=\u001B[39mmodel, train_dataset\u001B[38;5;241m=\u001B[39mdataset, eval_dataset\u001B[38;5;241m=\u001B[39mdataset, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, logger\u001B[38;5;241m=\u001B[39mlogger)\n\u001B[0;32m---> 35\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/trainers/trainer.py:333\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    331\u001B[0m     task \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_task(task_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtask_p)\n\u001B[1;32m    332\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_batch(split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, task\u001B[38;5;241m=\u001B[39mtask)\n\u001B[0;32m--> 333\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrad_clip \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m    336\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39munscale_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/autograd/function.py:288\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImplementing both \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackward\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvjp\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for a custom \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction is not allowed. You should only implement one \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof them.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m     )\n\u001B[1;32m    287\u001B[0m user_fn \u001B[38;5;241m=\u001B[39m vjp_fn \u001B[38;5;28;01mif\u001B[39;00m vjp_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Function\u001B[38;5;241m.\u001B[39mvjp \u001B[38;5;28;01melse\u001B[39;00m backward_fn\n\u001B[0;32m--> 288\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43muser_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3232\u001B[0m, in \u001B[0;36maot_dispatch_autograd.<locals>.CompiledFunction.backward\u001B[0;34m(ctx, *flat_args)\u001B[0m\n\u001B[1;32m   3230\u001B[0m     out \u001B[38;5;241m=\u001B[39m CompiledFunctionBackward\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;241m*\u001B[39mall_args)\n\u001B[1;32m   3231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3232\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcall_compiled_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3204\u001B[0m, in \u001B[0;36maot_dispatch_autograd.<locals>.CompiledFunction.backward.<locals>.call_compiled_backward\u001B[0;34m()\u001B[0m\n\u001B[1;32m   3199\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tracing(saved_context), context(), track_graph_compiling(aot_config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbackward\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   3200\u001B[0m         CompiledFunction\u001B[38;5;241m.\u001B[39mcompiled_bw \u001B[38;5;241m=\u001B[39m aot_config\u001B[38;5;241m.\u001B[39mbw_compiler(\n\u001B[1;32m   3201\u001B[0m             bw_module, placeholder_list\n\u001B[1;32m   3202\u001B[0m         )\n\u001B[0;32m-> 3204\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3205\u001B[0m \u001B[43m    \u001B[49m\u001B[43mCompiledFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_bw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3206\u001B[0m \u001B[43m    \u001B[49m\u001B[43mall_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3207\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteal_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3209\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3211\u001B[0m out \u001B[38;5;241m=\u001B[39m functionalized_rng_runtime_epilogue(CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata, out)\n\u001B[1;32m   3212\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(out)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1506\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1506\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1507\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1511\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1512\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1513\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1514\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:374\u001B[0m, in \u001B[0;36mCompiledFxGraph.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 374\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_current_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:628\u001B[0m, in \u001B[0;36malign_inputs_from_check_idxs.<locals>.run\u001B[0;34m(new_inputs)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(new_inputs):\n\u001B[1;32m    627\u001B[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001B[0;32m--> 628\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:401\u001B[0m, in \u001B[0;36m_run_from_cache\u001B[0;34m(compiled_graph, inputs)\u001B[0m\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcodecache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyCodeCache\n\u001B[1;32m    393\u001B[0m     compiled_graph\u001B[38;5;241m.\u001B[39mcompiled_artifact \u001B[38;5;241m=\u001B[39m PyCodeCache\u001B[38;5;241m.\u001B[39mload_by_key_path(\n\u001B[1;32m    394\u001B[0m         compiled_graph\u001B[38;5;241m.\u001B[39mcache_key,\n\u001B[1;32m    395\u001B[0m         compiled_graph\u001B[38;5;241m.\u001B[39martifact_path,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m (),\n\u001B[1;32m    399\u001B[0m     )\u001B[38;5;241m.\u001B[39mcall\n\u001B[0;32m--> 401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_artifact\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/tmp/torchinductor_adam/u7/cu7zqqfp6sg5n4ldmb2ymgdqqueqsb36lxnderfhyodtprp7knbm.py:1298\u001B[0m, in \u001B[0;36mcall\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1296\u001B[0m buf7 \u001B[38;5;241m=\u001B[39m empty_strided((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m588\u001B[39m), (\u001B[38;5;241m75264\u001B[39m, \u001B[38;5;241m588\u001B[39m, \u001B[38;5;241m1\u001B[39m), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m   1297\u001B[0m \u001B[38;5;66;03m# Source Nodes: [], Original ATen: [aten.add, aten.slice_backward]\u001B[39;00m\n\u001B[0;32m-> 1298\u001B[0m \u001B[43mtriton_poi_fused_add_slice_backward_4\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtangents_4\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf5\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_38\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf6\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf7\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m150528\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m150528\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1299\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m buf5\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m buf6\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:401\u001B[0m, in \u001B[0;36mCachingAutotuner.run\u001B[0;34m(self, grid, stream, *args)\u001B[0m\n\u001B[1;32m    399\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecompile()\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautotune_to_one_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mconfig, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_by_coordesc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m config\u001B[38;5;241m.\u001B[39mcoordinate_descent_tuning\n\u001B[1;32m    406\u001B[0m ):\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    408\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoordinate_descent_tuning(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m*\u001B[39margs, grid\u001B[38;5;241m=\u001B[39mgrid)\n\u001B[1;32m    409\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:326\u001B[0m, in \u001B[0;36mCachingAutotuner.autotune_to_one_config\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mautotune_to_one_config\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    325\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Do the actual autotuning\"\"\"\u001B[39;00m\n\u001B[0;32m--> 326\u001B[0m     timings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbenchmark_all_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers \u001B[38;5;241m=\u001B[39m [builtins\u001B[38;5;241m.\u001B[39mmin(timings, key\u001B[38;5;241m=\u001B[39mtimings\u001B[38;5;241m.\u001B[39mget)]\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_cache_hook:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:302\u001B[0m, in \u001B[0;36mCachingAutotuner.benchmark_all_configs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;129m@dynamo_timed\u001B[39m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbenchmark_all_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 302\u001B[0m     timings \u001B[38;5;241m=\u001B[39m \u001B[43m{\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbench\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlaunchers\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m timings\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoordesc_tuner\u001B[38;5;241m.\u001B[39mcache_benchmark_result(k\u001B[38;5;241m.\u001B[39mconfig, v)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:303\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;129m@dynamo_timed\u001B[39m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbenchmark_all_configs\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    302\u001B[0m     timings \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m--> 303\u001B[0m         launcher: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbench\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlauncher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    304\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m launcher \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlaunchers\n\u001B[1;32m    305\u001B[0m     }\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m timings\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcoordesc_tuner\u001B[38;5;241m.\u001B[39mcache_benchmark_result(k\u001B[38;5;241m.\u001B[39mconfig, v)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:282\u001B[0m, in \u001B[0;36mCachingAutotuner.bench\u001B[0;34m(self, launcher, grid, *args)\u001B[0m\n\u001B[1;32m    275\u001B[0m     cloned_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    276\u001B[0m     launcher(\n\u001B[1;32m    277\u001B[0m         \u001B[38;5;241m*\u001B[39mcloned_args,\n\u001B[1;32m    278\u001B[0m         grid\u001B[38;5;241m=\u001B[39mgrid,\n\u001B[1;32m    279\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    280\u001B[0m     )\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdo_bench\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkernel_call\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m40\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfast_flush\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/utils.py:75\u001B[0m, in \u001B[0;36mdo_bench\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m quantile_field_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[1;32m     74\u001B[0m     kwargs[quantile_field_name] \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.8\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtriton_do_bench\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/triton/testing.py:111\u001B[0m, in \u001B[0;36mdo_bench\u001B[0;34m(fn, warmup, rep, grad_to_none, quantiles, fast_flush, return_mode)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# We maintain a buffer of 256 MB that we clear\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m# before each kernel call to make sure that the L2\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# doesn't contain any input data before the run\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fast_flush:\n\u001B[0;32m--> 111\u001B[0m     cache \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m256e6\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    113\u001B[0m     cache \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m256e6\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint8, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacty of 1.83 GiB of which 94.94 MiB is free. Including non-PyTorch memory, this process has 1.73 GiB memory in use. Of the allocated memory 1.44 GiB is allocated by PyTorch, and 218.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load configs\n",
    "\n",
    "task_config_path = lambda: f'./configs/tasks/guacamol/{guacamol_task}/config.json'\n",
    "\n",
    "\n",
    "for guacamol_task in GUACAMOL_TASKS:\n",
    "    \n",
    "    task_config = TaskConfig.from_pretrained(task_config_path())\n",
    "    task_config.validate = True\n",
    "    task_config.split = 'val'\n",
    "    dataset = AutoDataset.from_config(task_config)\n",
    "    print(f\"Loaded {task_config.target_label} data.\")\n",
    "    \n",
    "    for model_name, path_to_model_config in PREDICTION_MODEL_CONFIGS.items():\n",
    "        \n",
    "        model_config = ModelConfig.from_pretrained(path_to_model_config)\n",
    "\n",
    "        trainer_config = TrainerConfig.from_pretrained('./configs/trainers/debug/')\n",
    "        logger_config = LoggerConfig.from_pretrained(DEFAULT_CONFIG_FILES['logger'])\n",
    "        \n",
    "        out_dir = f'./results/regression_task/guacamol/{model_name}/{guacamol_task}'\n",
    "        trainer_config.out_dir = out_dir\n",
    "        logger_config.name = model_name + '_' + guacamol_task\n",
    "        logger_config.project = 'debug'\n",
    "        trainer_config.enable_save_checkpoint = False\n",
    "        task_config.validate = False\n",
    "        logger_config.wandb_log = True\n",
    "        \n",
    "        dataset = AutoDataset.from_config(task_config)\n",
    "        tokenizer = AutoTokenizer.from_config(task_config)                \n",
    "        model = AutoModel.from_config(model_config)\n",
    "        logger = WandbLogger(logger_config, [task_config, model_config, trainer_config])\n",
    "        trainer = Trainer(config=trainer_config, model=model, train_dataset=dataset, eval_dataset=dataset, tokenizer=tokenizer, logger=logger)\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-30T21:53:55.905651127Z",
     "start_time": "2023-12-30T21:46:40.241717126Z"
    }
   },
   "id": "3f4cb44e009419ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "caca436fa0224b62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
