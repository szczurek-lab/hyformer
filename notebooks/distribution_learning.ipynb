{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Distribution Learning Benchmark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26dcdcd117802dc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam/Projects/hybrid-transformer\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T17:00:48.432583199Z",
     "start_time": "2023-12-26T17:00:48.431849653Z"
    }
   },
   "id": "422d4f35c0246db1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-26 18:00:50.670684: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-26 18:00:50.672554: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-26 18:00:50.697962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 18:00:50.697985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 18:00:50.698750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-26 18:00:50.703010: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 18:00:51.266821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "from hybrid_transformer.configs.task import TaskConfig\n",
    "from hybrid_transformer.configs.model import ModelConfig\n",
    "from hybrid_transformer.configs.trainer import TrainerConfig\n",
    "from hybrid_transformer.configs.logger import LoggerConfig\n",
    "\n",
    "from hybrid_transformer.utils.datasets.auto import AutoDataset\n",
    "from hybrid_transformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hybrid_transformer.models.auto import AutoModel\n",
    "from hybrid_transformer.utils.loggers.wandb import WandbLogger\n",
    "\n",
    "from hybrid_transformer.trainers.trainer import Trainer\n",
    "\n",
    "from scripts.train import DEFAULT_CONFIG_FILES\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T17:00:51.827108753Z",
     "start_time": "2023-12-26T17:00:49.090037896Z"
    }
   },
   "id": "388ac09832efcd15"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type GPT to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "task_config = TaskConfig.from_pretrained(DEFAULT_CONFIG_FILES['task'])\n",
    "model_config = ModelConfig.from_pretrained(DEFAULT_CONFIG_FILES['model'])\n",
    "trainer_config = TrainerConfig.from_pretrained(DEFAULT_CONFIG_FILES['trainer'])\n",
    "logger_config = LoggerConfig.from_pretrained(DEFAULT_CONFIG_FILES['logger'])\n",
    "task_config.validate = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T17:00:52.549457568Z",
     "start_time": "2023-12-26T17:00:52.518043283Z"
    }
   },
   "id": "ea1ae3c22872b6df"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 38.06M\n",
      "tokens per iteration will be: 61,440\n",
      "Using cuda device\n",
      "Successfully resumed from ./results/lm/pre-trained/...\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n"
     ]
    }
   ],
   "source": [
    "dataset = AutoDataset.from_config(task_config)\n",
    "tokenizer = AutoTokenizer.from_config(task_config)\n",
    "model = AutoModel.from_config(model_config)\n",
    "logger = WandbLogger(logger_config, [task_config, model_config, trainer_config])\n",
    "trainer = Trainer(config=trainer_config, model=model, train_dataset=dataset, eval_dataset=dataset, tokenizer=tokenizer, logger=logger)\n",
    "trainer.load_checkpoint()\n",
    "trainer._train_init()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T17:03:30.033678737Z",
     "start_time": "2023-12-26T17:03:28.115183922Z"
    }
   },
   "id": "b6acebc3b3f46dc4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 1.83 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 1.82 GiB memory in use. Of the allocated memory 1.69 GiB is allocated by PyTorch, and 56.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 8\u001B[0m\n\u001B[1;32m      2\u001B[0m mlm_inputs \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mget_batch(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmlm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m lm_outputs \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[1;32m      5\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39mlm_inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m], attention_mask\u001B[38;5;241m=\u001B[39mlm_inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m], \n\u001B[1;32m      6\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlm_inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m], target\u001B[38;5;241m=\u001B[39mlm_inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m], eos_mask\u001B[38;5;241m=\u001B[39mlm_inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meos_mask\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 8\u001B[0m mlm_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlm_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlm_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlm_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlm_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meos_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlm_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meos_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/models/gpt.py:73\u001B[0m, in \u001B[0;36mGPT.forward\u001B[0;34m(self, input_ids, labels, target, eos_mask, attention_mask)\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(module, nn\u001B[38;5;241m.\u001B[39mEmbedding):\n\u001B[1;32m     71\u001B[0m         torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mnormal_(module\u001B[38;5;241m.\u001B[39mweight, mean\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m)\n\u001B[0;32m---> 73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, target\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, eos_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     74\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     75\u001B[0m     supervised_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3905\u001B[0m, in \u001B[0;36maot_module_simplified.<locals>.forward\u001B[0;34m(*runtime_args)\u001B[0m\n\u001B[1;32m   3903\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(params_flat)\n\u001B[1;32m   3904\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(runtime_args)\n\u001B[0;32m-> 3905\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1482\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1481\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m-> 1482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2527\u001B[0m, in \u001B[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m   2525\u001B[0m             args_[idx] \u001B[38;5;241m=\u001B[39m args_[idx]\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m   2526\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39m_force_original_view_tracking(\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m-> 2527\u001B[0m         all_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2528\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2529\u001B[0m \u001B[43m            \u001B[49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2530\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2531\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2533\u001B[0m     all_outs \u001B[38;5;241m=\u001B[39m call_func_with_args(\n\u001B[1;32m   2534\u001B[0m         compiled_fn,\n\u001B[1;32m   2535\u001B[0m         args,\n\u001B[1;32m   2536\u001B[0m         disable_amp\u001B[38;5;241m=\u001B[39mdisable_amp,\n\u001B[1;32m   2537\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1506\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1506\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1507\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1511\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1512\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1513\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1514\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1482\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1481\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m-> 1482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/autograd/function.py:539\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    538\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    547\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3010\u001B[0m, in \u001B[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001B[0;34m(ctx, *deduped_flat_tensor_args)\u001B[0m\n\u001B[1;32m   3004\u001B[0m     args \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m*\u001B[39margs, seed, offset)\n\u001B[1;32m   3005\u001B[0m \u001B[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001B[39;00m\n\u001B[1;32m   3006\u001B[0m \u001B[38;5;66;03m# The full list of outputs and their relative order is:\u001B[39;00m\n\u001B[1;32m   3007\u001B[0m \u001B[38;5;66;03m# (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001B[39;00m\n\u001B[1;32m   3008\u001B[0m \u001B[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001B[39;00m\n\u001B[1;32m   3009\u001B[0m \u001B[38;5;66;03m#   of the original view, and not the synthetic base\u001B[39;00m\n\u001B[0;32m-> 3010\u001B[0m fw_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mCompiledFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_fw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3012\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3013\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3014\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3016\u001B[0m num_outputs \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs\n\u001B[1;32m   3017\u001B[0m num_outputs_aliased \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs_aliased\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1506\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1506\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1507\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1511\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1512\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1513\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1514\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:374\u001B[0m, in \u001B[0;36mCompiledFxGraph.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 374\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_current_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:628\u001B[0m, in \u001B[0;36malign_inputs_from_check_idxs.<locals>.run\u001B[0;34m(new_inputs)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(new_inputs):\n\u001B[1;32m    627\u001B[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001B[0;32m--> 628\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:401\u001B[0m, in \u001B[0;36m_run_from_cache\u001B[0;34m(compiled_graph, inputs)\u001B[0m\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcodecache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyCodeCache\n\u001B[1;32m    393\u001B[0m     compiled_graph\u001B[38;5;241m.\u001B[39mcompiled_artifact \u001B[38;5;241m=\u001B[39m PyCodeCache\u001B[38;5;241m.\u001B[39mload_by_key_path(\n\u001B[1;32m    394\u001B[0m         compiled_graph\u001B[38;5;241m.\u001B[39mcache_key,\n\u001B[1;32m    395\u001B[0m         compiled_graph\u001B[38;5;241m.\u001B[39martifact_path,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m (),\n\u001B[1;32m    399\u001B[0m     )\u001B[38;5;241m.\u001B[39mcall\n\u001B[0;32m--> 401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_artifact\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/tmp/torchinductor_adam/e3/ce3cxfxiyibxiq7y4bgkycc64zjnhblddfl5bpzy7ifvlewtpcxx.py:1207\u001B[0m, in \u001B[0;36mcall\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;66;03m# Source Nodes: [l__self___transformer_h_7_mlp_fc], Original ATen: [aten.mm]\u001B[39;00m\n\u001B[1;32m   1206\u001B[0m extern_kernels\u001B[38;5;241m.\u001B[39mmm(buf262, reinterpret_tensor(primals_66, (\u001B[38;5;241m512\u001B[39m, \u001B[38;5;241m2048\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m512\u001B[39m), \u001B[38;5;241m0\u001B[39m), out\u001B[38;5;241m=\u001B[39mbuf263)\n\u001B[0;32m-> 1207\u001B[0m buf264 \u001B[38;5;241m=\u001B[39m \u001B[43mempty_strided\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1536\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[38;5;66;03m# Source Nodes: [l__self___transformer_h_7_mlp_gelu, l__self___transformer_h_7_mlp_proj], Original ATen: [aten.gelu, aten.view]\u001B[39;00m\n\u001B[1;32m   1209\u001B[0m triton_poi_fused_gelu_view_9\u001B[38;5;241m.\u001B[39mrun(buf263, buf264, \u001B[38;5;241m3145728\u001B[39m, grid\u001B[38;5;241m=\u001B[39mgrid(\u001B[38;5;241m3145728\u001B[39m), stream\u001B[38;5;241m=\u001B[39mstream0)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacty of 1.83 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 1.82 GiB memory in use. Of the allocated memory 1.69 GiB is allocated by PyTorch, and 56.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "lm_inputs = trainer.get_batch('train', 'lm')\n",
    "mlm_inputs = trainer.get_batch('train', 'mlm')\n",
    "\n",
    "lm_outputs = trainer.model(\n",
    "    input_ids=lm_inputs['input_ids'], attention_mask=lm_inputs['attention_mask'], \n",
    "    labels=lm_inputs['labels'], target=lm_inputs['target'], eos_mask=lm_inputs['eos_mask'])\n",
    "\n",
    "mlm_outputs = trainer.model(\n",
    "    input_ids=mlm_inputs['input_ids'], attention_mask=mlm_inputs['attention_mask'], \n",
    "    labels=mlm_inputs['labels'], target=mlm_inputs['target'], eos_mask=mlm_inputs['eos_mask'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T17:03:55.826978300Z",
     "start_time": "2023-12-26T17:03:35.918609689Z"
    }
   },
   "id": "ef2b8284b5fd1672"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.4458, device='cuda:0')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tra"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:21:29.228254983Z",
     "start_time": "2023-12-26T12:21:29.180509547Z"
    }
   },
   "id": "fcd354096ccfcd1d"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[13, 13, 13,  ...,  0,  0,  0],\n        [13, 13, 17,  ...,  0,  0,  0],\n        [16, 19, 13,  ...,  0,  0,  0],\n        ...,\n        [13, 16, 12,  ...,  0,  0,  0],\n        [13, 13, 20,  ...,  0,  0,  0],\n        [13, 12, 17,  ...,  0,  0,  0]], device='cuda:0')"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:19:17.083640528Z",
     "start_time": "2023-12-26T12:19:17.053573736Z"
    }
   },
   "id": "b4579e70ab5f4f30"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.8679e-01,  3.6928e+00,  1.1933e-01,  ...,  2.7387e-01,\n          -2.2925e-03,  2.4751e-01],\n         [-3.0744e-01,  2.4300e-01,  2.8138e-02,  ..., -1.0149e+00,\n          -3.9690e-01, -1.3168e+00],\n         [-2.9528e-01,  4.6905e-01, -1.7181e-01,  ..., -1.0297e+00,\n          -2.7817e-01, -1.1676e+00],\n         ...,\n         [ 4.8069e+00,  3.3868e-01,  1.2406e-01,  ..., -1.4735e-01,\n           4.3360e-01, -8.3324e-01],\n         [ 4.2229e+00,  2.2291e-01, -2.8840e-02,  ..., -6.9874e-02,\n           9.2272e-01, -4.1192e-01],\n         [ 4.2008e+00,  2.5254e-01,  4.7244e-01,  ..., -4.4597e-01,\n           1.9795e-02, -3.2272e-02]],\n\n        [[ 2.1462e-01,  4.4047e+00,  3.4294e-01,  ..., -5.2393e-02,\n          -1.6237e-01,  2.1099e-01],\n         [-2.6895e-01,  3.7325e-01, -2.9334e-01,  ..., -1.0901e+00,\n          -4.1790e-01, -1.2638e+00],\n         [-2.8949e-01,  5.7877e-01, -1.7921e-01,  ..., -1.0775e+00,\n          -3.5852e-01, -1.1731e+00],\n         ...,\n         [ 4.7025e+00,  3.0830e-01,  3.3791e-01,  ..., -4.9181e-01,\n           2.9894e-01, -4.4764e-01],\n         [ 4.2467e+00,  1.9670e-02,  3.9108e-02,  ..., -6.2681e-02,\n           5.8523e-01, -4.9044e-01],\n         [ 4.6060e+00,  2.0416e-01,  5.3426e-02,  ..., -8.8498e-01,\n           1.9222e-01, -3.5637e-01]],\n\n        [[ 3.2273e-01,  4.1659e+00,  5.3472e-01,  ..., -4.4307e-01,\n          -1.5544e-01,  2.4546e-01],\n         [-6.7871e-01,  3.1328e-01,  6.4686e-01,  ..., -6.9842e-02,\n          -1.4170e-01, -1.1977e-01],\n         [-2.2384e-02,  2.5843e-01,  6.4675e-01,  ..., -2.6321e-01,\n          -6.7130e-01, -3.2965e-01],\n         ...,\n         [ 4.7101e+00,  3.8543e-01,  7.1870e-02,  ...,  1.4738e-01,\n           6.1725e-01, -2.6183e-01],\n         [ 4.2469e+00,  2.8054e-01, -1.3600e-01,  ..., -1.3144e-01,\n           5.0716e-02, -8.4834e-01],\n         [ 4.4507e+00,  2.7791e-01,  4.0772e-01,  ..., -6.4711e-01,\n           1.2741e-03, -2.7553e-01]],\n\n        ...,\n\n        [[-2.0624e-01,  4.2513e+00,  3.6117e-01,  ..., -1.1479e-01,\n          -5.1293e-01,  1.7504e-01],\n         [-3.1877e-01,  3.0794e-01, -1.3202e-01,  ..., -8.0681e-01,\n          -2.9026e-01, -1.2487e+00],\n         [-7.4310e-01,  3.1643e-01,  4.0123e-01,  ..., -7.3476e-02,\n          -7.6195e-02, -3.2855e-01],\n         ...,\n         [ 4.7138e+00,  2.9631e-01,  3.0181e-01,  ...,  1.9063e-01,\n           6.7126e-01, -2.5178e-01],\n         [ 4.2592e+00, -1.7580e-01, -6.3752e-02,  ..., -5.6573e-02,\n           3.4961e-01, -1.7794e-02],\n         [ 3.9803e+00,  2.5829e-01,  4.7612e-01,  ..., -9.2860e-01,\n           4.7176e-02, -1.8392e-01]],\n\n        [[ 2.1473e-01,  3.3049e+00,  2.9379e-01,  ..., -3.9669e-01,\n          -2.5084e-01,  2.2214e-02],\n         [-3.7796e-01,  3.0815e-01, -3.5225e-02,  ..., -1.0579e+00,\n          -3.0946e-01, -1.1940e+00],\n         [-4.9841e-01,  4.4939e-01, -1.2976e-01,  ..., -1.0022e+00,\n          -4.4871e-01, -1.4572e+00],\n         ...,\n         [ 4.6076e+00,  4.7617e-01,  6.3358e-02,  ..., -1.9493e-01,\n           4.1349e-01, -6.6585e-01],\n         [ 4.4317e+00,  3.7589e-02, -1.5208e-01,  ..., -1.4547e-01,\n           5.4901e-01, -3.9306e-01],\n         [ 4.9780e+00,  1.2178e-01,  2.4492e-01,  ..., -4.9028e-01,\n           9.8581e-02, -3.4388e-01]],\n\n        [[ 1.0363e-01,  4.4900e+00,  5.4255e-01,  ...,  2.8892e-01,\n           5.8346e-02,  3.0908e-01],\n         [-3.7752e-01,  2.8223e-01, -2.0634e-01,  ..., -9.7133e-01,\n          -3.6126e-01, -1.0812e+00],\n         [-7.1369e-01,  5.6479e-01, -2.2903e-01,  ..., -6.7377e-01,\n           1.6902e-01, -6.1435e-01],\n         ...,\n         [ 4.5765e+00,  3.1760e-01,  3.2526e-01,  ...,  1.9353e-01,\n           3.2312e-01, -1.9114e-01],\n         [ 3.9204e+00, -1.7959e-01,  2.2242e-01,  ...,  1.2595e-01,\n           5.9874e-01, -4.4425e-01],\n         [ 4.7138e+00, -8.9845e-02,  4.4891e-01,  ..., -6.4682e-01,\n           3.9154e-01, -1.1531e-02]]], device='cuda:0')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:19:21.131695030Z",
     "start_time": "2023-12-26T12:19:21.096169833Z"
    }
   },
   "id": "c6dc909753fb4335"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m shift_logits \u001B[38;5;241m=\u001B[39m \u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlm_logits\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m shift_labels \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Flatten the tokens\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Flatten the tokens\n",
    "loss_fct = CrossEntropyLoss()\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:06:09.490963380Z",
     "start_time": "2023-12-26T12:06:09.443081199Z"
    }
   },
   "id": "1ae0e09f84dc98e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 1, 13, 13,  ...,  0,  0,  0],\n         [ 1, 13, 13,  ...,  0,  0,  0],\n         [ 1, 16, 19,  ...,  0,  0,  0],\n         ...,\n         [ 1, 13, 16,  ...,  0,  0,  0],\n         [ 1, 13, 13,  ...,  0,  0,  0],\n         [ 1, 13, 12,  ...,  0,  0,  0]], device='cuda:0'),\n 'attention_mask': None,\n 'labels': tensor([[ 1, 13, 13,  ...,  0,  0,  0],\n         [ 1, 13, 13,  ...,  0,  0,  0],\n         [ 1, 16, 19,  ...,  0,  0,  0],\n         ...,\n         [ 1, 13, 16,  ...,  0,  0,  0],\n         [ 1, 13, 13,  ...,  0,  0,  0],\n         [ 1, 13, 12,  ...,  0,  0,  0]], device='cuda:0'),\n 'target': None,\n 'eos_mask': tensor([[False, False, False,  ..., False, False, False],\n         [False, False, False,  ..., False, False, False],\n         [False, False, False,  ..., False, False, False],\n         ...,\n         [False, False, False,  ..., False, False, False],\n         [False, False, False,  ..., False, False, False],\n         [False, False, False,  ..., False, False, False]], device='cuda:0')}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:18:34.761978532Z",
     "start_time": "2023-12-26T12:18:34.718266914Z"
    }
   },
   "id": "fb50694bfb0b951d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mshift_labels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "shift_labels.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:06:07.763203357Z",
     "start_time": "2023-12-26T12:06:07.712563841Z"
    }
   },
   "id": "c76555f0de92bb00"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "idx = torch.ones(size=(8, 1), device=trainer.device) * tokenizer.generate_token_id\n",
    "idx = idx.long()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T11:40:02.304850136Z",
     "start_time": "2023-12-26T11:40:02.266885496Z"
    }
   },
   "id": "e1b3895912927b3d"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "\n",
    "# forward the model to get the logits for the index in the sequence\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=idx.long())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T11:45:11.482323414Z",
     "start_time": "2023-12-26T11:45:11.447976750Z"
    }
   },
   "id": "ac153d0b17f3e729"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "logits = outputs['lm_logits']\n",
    "logits = logits[:, -1, :] / 1.0\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "idx = torch.cat((idx, idx_next), dim=1)            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T11:45:11.724614005Z",
     "start_time": "2023-12-26T11:45:11.693845793Z"
    }
   },
   "id": "f28b164181aed3a3"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  1., 258., 229., 229.,  15.],\n        [  1., 307., 225., 225., 384.],\n        [  1., 146., 146.,  94.,  94.],\n        [  1., 564., 459., 226., 143.],\n        [  1., 429.,  83., 418., 419.],\n        [  1., 490., 120.,  41.,  41.],\n        [  1.,   1.,   1., 319., 316.],\n        [  1., 309., 573., 488., 509.]], device='cuda:0')"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T11:45:12.000203201Z",
     "start_time": "2023-12-26T11:45:11.961454764Z"
    }
   },
   "id": "55524be3faf8ed6"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "['[Zr+]', '=', '[K+]', '[Te]', '[Rb]', 'b', 'b', '[Dy]']"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(idx_next)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T11:44:37.168446345Z",
     "start_time": "2023-12-26T11:44:37.135149852Z"
    }
   },
   "id": "9ca45708460ef934"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43minputs\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "inputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:02:31.182566503Z",
     "start_time": "2023-12-26T12:02:31.116740207Z"
    }
   },
   "id": "e153834f0e2efd9b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T12:05:45.762160302Z",
     "start_time": "2023-12-26T12:05:45.729459555Z"
    }
   },
   "id": "df0b935946267e56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6b9a85d4ba1e4706"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
