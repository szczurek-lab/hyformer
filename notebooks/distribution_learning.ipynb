{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Distribution Learning Benchmark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26dcdcd117802dc"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "from hybrid_transformer.configs.task import TaskConfig\n",
    "from hybrid_transformer.configs.model import ModelConfig\n",
    "from hybrid_transformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from hybrid_transformer.utils.datasets.auto import AutoDataset\n",
    "from hybrid_transformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hybrid_transformer.models.auto import AutoModel\n",
    "\n",
    "from hybrid_transformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T16:27:35.155514794Z",
     "start_time": "2023-12-25T16:27:35.124769482Z"
    }
   },
   "id": "388ac09832efcd15"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.60M\n",
      "Resuming training from ./results/.ckpt...\n",
      "Running on a single device!\n",
      "tokens per iteration will be: 61,440\n",
      "Using cuda device\n",
      "compiling the model... (takes a ~minute)\n",
      "num decayed parameter tensors: 13, with 6,658,560 parameters\n",
      "num non-decayed parameter tensors: 5, with 2,560 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "task_config = TaskConfig.from_pretrained('./configs/tasks/distribution_learning/guacamol')\n",
    "model_config = ModelConfig(num_layers=2)\n",
    "trainer_config = TrainerConfig()\n",
    "\n",
    "dataset = AutoDataset.from_config(task_config)\n",
    "tokenizer = AutoTokenizer.from_config(task_config)\n",
    "model = AutoModel.from_config(model_config)\n",
    "trainer = Trainer(config=trainer_config, model=model, train_dataset=dataset, eval_dataset=dataset, tokenizer=tokenizer, wandb_log=)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T17:00:49.166131983Z",
     "start_time": "2023-12-25T17:00:48.375634505Z"
    }
   },
   "id": "b6acebc3b3f46dc4"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at iter 2000: train loss 0.0000, val loss 0.0000, percent 0.2183\n",
      "saving checkpoint to ./results/\n",
      "iter 2033: loss 0.0000, time 4367.66ms.\r.iter 2002: loss 0.0000, time 3277.53ms.iter 2004: loss 0.0000, time 3329.72ms.iter 2006: loss 0.0000, time 3387.49ms.iter 2008: loss 0.0000, time 3743.15ms.iter 2010: loss 0.0000, time 4134.58ms.iter 2012: loss 0.0000, time 3550.70ms.iter 2014: loss 0.0000, time 3998.52ms.iter 2016: loss 0.0000, time 3791.77ms.iter 2018: loss 0.0000, time 4659.63ms.iter 2020: loss 0.0000, time 4049.59ms.iter 2022: loss 0.0000, time 4528.29ms.iter 2024: loss 0.0000, time 3766.51ms.iter 2026: loss 0.0000, time 4358.83ms.iter 2028: loss 0.0000, time 5048.71ms.iter 2030: loss 0.0000, time 4514.07ms.iter 2032: loss 0.0000, time 4339.75ms."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/trainers/trainer.py:259\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mrequire_backward_grad_sync \u001B[38;5;241m=\u001B[39m (micro_step \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx:\n\u001B[0;32m--> 259\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meos_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meos_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n\u001B[1;32m    263\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_batch(split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, task\u001B[38;5;241m=\u001B[39mtask) \u001B[38;5;66;03m# Here, get task\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/models/gpt.py:73\u001B[0m, in \u001B[0;36mGPT.forward\u001B[0;34m(self, input_ids, labels, target, eos_mask, attention_mask)\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(module, nn\u001B[38;5;241m.\u001B[39mEmbedding):\n\u001B[1;32m     71\u001B[0m         torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mnormal_(module\u001B[38;5;241m.\u001B[39mweight, mean\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m)\n\u001B[0;32m---> 73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, target\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, eos_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     74\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     75\u001B[0m     supervised_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001B[0m, in \u001B[0;36mwrap_inline.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3905\u001B[0m, in \u001B[0;36maot_module_simplified.<locals>.forward\u001B[0;34m(*runtime_args)\u001B[0m\n\u001B[1;32m   3903\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(params_flat)\n\u001B[1;32m   3904\u001B[0m full_args\u001B[38;5;241m.\u001B[39mextend(runtime_args)\n\u001B[0;32m-> 3905\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1482\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1481\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m-> 1482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2527\u001B[0m, in \u001B[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m   2525\u001B[0m             args_[idx] \u001B[38;5;241m=\u001B[39m args_[idx]\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m   2526\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39m_force_original_view_tracking(\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m-> 2527\u001B[0m         all_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2528\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2529\u001B[0m \u001B[43m            \u001B[49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2530\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2531\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2533\u001B[0m     all_outs \u001B[38;5;241m=\u001B[39m call_func_with_args(\n\u001B[1;32m   2534\u001B[0m         compiled_fn,\n\u001B[1;32m   2535\u001B[0m         args,\n\u001B[1;32m   2536\u001B[0m         disable_amp\u001B[38;5;241m=\u001B[39mdisable_amp,\n\u001B[1;32m   2537\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1506\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1506\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1507\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1511\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1512\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1513\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1514\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1482\u001B[0m, in \u001B[0;36mmake_boxed_func.<locals>.g\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m   1481\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mg\u001B[39m(args):\n\u001B[0;32m-> 1482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/autograd/function.py:539\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    538\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    547\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3010\u001B[0m, in \u001B[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001B[0;34m(ctx, *deduped_flat_tensor_args)\u001B[0m\n\u001B[1;32m   3004\u001B[0m     args \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m*\u001B[39margs, seed, offset)\n\u001B[1;32m   3005\u001B[0m \u001B[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001B[39;00m\n\u001B[1;32m   3006\u001B[0m \u001B[38;5;66;03m# The full list of outputs and their relative order is:\u001B[39;00m\n\u001B[1;32m   3007\u001B[0m \u001B[38;5;66;03m# (*mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001B[39;00m\n\u001B[1;32m   3008\u001B[0m \u001B[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001B[39;00m\n\u001B[1;32m   3009\u001B[0m \u001B[38;5;66;03m#   of the original view, and not the synthetic base\u001B[39;00m\n\u001B[0;32m-> 3010\u001B[0m fw_outs \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mCompiledFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_fw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3012\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3013\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3014\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3016\u001B[0m num_outputs \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs\n\u001B[1;32m   3017\u001B[0m num_outputs_aliased \u001B[38;5;241m=\u001B[39m CompiledFunction\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mnum_outputs_aliased\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1506\u001B[0m, in \u001B[0;36mcall_func_with_args\u001B[0;34m(f, args, steal_args, disable_amp)\u001B[0m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   1505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(f, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1506\u001B[0m         out \u001B[38;5;241m=\u001B[39m normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1507\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[1;32m   1509\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1511\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt take boxed arguments. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1512\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1513\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1514\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:374\u001B[0m, in \u001B[0;36mCompiledFxGraph.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 374\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_current_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:628\u001B[0m, in \u001B[0;36malign_inputs_from_check_idxs.<locals>.run\u001B[0;34m(new_inputs)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(new_inputs):\n\u001B[1;32m    627\u001B[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001B[0;32m--> 628\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/codecache.py:401\u001B[0m, in \u001B[0;36m_run_from_cache\u001B[0;34m(compiled_graph, inputs)\u001B[0m\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcodecache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyCodeCache\n\u001B[1;32m    393\u001B[0m     compiled_graph\u001B[38;5;241m.\u001B[39mcompiled_artifact \u001B[38;5;241m=\u001B[39m PyCodeCache\u001B[38;5;241m.\u001B[39mload_by_key_path(\n\u001B[1;32m    394\u001B[0m         compiled_graph\u001B[38;5;241m.\u001B[39mcache_key,\n\u001B[1;32m    395\u001B[0m         compiled_graph\u001B[38;5;241m.\u001B[39martifact_path,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m (),\n\u001B[1;32m    399\u001B[0m     )\u001B[38;5;241m.\u001B[39mcall\n\u001B[0;32m--> 401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompiled_artifact\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/tmp/torchinductor_adam/e7/ce724awyshsolztv7odxu6sq65rcxtw7mygkemivkg5at5vxlfif.py:749\u001B[0m, in \u001B[0;36mcall\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    747\u001B[0m buf56 \u001B[38;5;241m=\u001B[39m empty_strided((\u001B[38;5;241m1536\u001B[39m, \u001B[38;5;241m512\u001B[39m), (\u001B[38;5;241m512\u001B[39m, \u001B[38;5;241m1\u001B[39m), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m    748\u001B[0m \u001B[38;5;66;03m# Source Nodes: [l__self___transformer_h_1_attn_1_out_proj], Original ATen: [aten.view]\u001B[39;00m\n\u001B[0;32m--> 749\u001B[0m \u001B[43mtriton_poi_fused_view_7\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf55\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf56\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m786432\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m786432\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream0\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    750\u001B[0m buf57 \u001B[38;5;241m=\u001B[39m reinterpret_tensor(buf55, (\u001B[38;5;241m1536\u001B[39m, \u001B[38;5;241m512\u001B[39m), (\u001B[38;5;241m512\u001B[39m, \u001B[38;5;241m1\u001B[39m)); \u001B[38;5;28;01mdel\u001B[39;00m buf55  \u001B[38;5;66;03m# reuse\u001B[39;00m\n\u001B[1;32m    751\u001B[0m \u001B[38;5;66;03m# Source Nodes: [l__self___transformer_h_1_attn_1_out_proj], Original ATen: [aten.mm]\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/triton_heuristics.py:433\u001B[0m, in \u001B[0;36mCachingAutotuner.run\u001B[0;34m(self, grid, stream, *args)\u001B[0m\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m launcher(\n\u001B[1;32m    428\u001B[0m             \u001B[38;5;241m*\u001B[39margs,\n\u001B[1;32m    429\u001B[0m             grid\u001B[38;5;241m=\u001B[39mgrid,\n\u001B[1;32m    430\u001B[0m             stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    431\u001B[0m         )\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlauncher\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    437\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<string>:13\u001B[0m, in \u001B[0;36mlauncher\u001B[0;34m(in_ptr0, out_ptr0, xnumel, grid, stream)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T19:40:08.750965675Z",
     "start_time": "2023-12-25T17:00:50.085817251Z"
    }
   },
   "id": "90267322f41e1692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add logger"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7898c51dab1353af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# task_p is in 0, 1 and then -1 for both"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33dae047f55b05a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add task selector\n",
    "\n",
    "# PreTrainedGPT\n",
    "# PreTrainedHT\n",
    "# GPTForPrediction\n",
    "# JointGPT\n",
    "# HybridTransformer\n",
    "\n",
    "# Trainer + task_p"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f507a69a1e9c5af7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "1 == torch.bernoulli(torch.Tensor([0.5]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T22:41:22.456841031Z",
     "start_time": "2023-12-25T22:41:22.450403088Z"
    }
   },
   "id": "eb7bfd0006088bfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae8906b91058c07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
