{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-09 15:03:48.874158: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-09 15:03:48.908302: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 15:03:48.908344: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 15:03:48.908373: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 15:03:48.916327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/projects/jointformer\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "REPOSITORY_DIR = '/home/adamizdebski/projects/jointformer'\n",
    "DATA_DIR = '/home/adamizdebski/files/data'\n",
    "OUTPUT_DIR = '/home/adamizdebski/files/jointformer/results/chemberta2/moleculenet'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = '/home/adamizdebski/projects/jointformer/configs/datasets/molecule_net/freesolv'\n",
    "PATH_TO_TOKENIZER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/tokenizers/chemberta'\n",
    "PATH_TO_CHEMBERTA_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/chemberta'\n",
    "PATH_TO_MODEL_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/jointformer_test'\n",
    "PATH_TO_TRAINER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/trainers/finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mchdir(REPOSITORY_DIR)\n\u001b[1;32m      3\u001b[0m dataset_config \u001b[38;5;241m=\u001b[39m DatasetConfig\u001b[38;5;241m.\u001b[39mfrom_config_file(PATH_TO_DATASET_CONFIG)\n\u001b[1;32m      4\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m TokenizerConfig\u001b[38;5;241m.\u001b[39mfrom_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(REPOSITORY_DIR)\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='val')\n",
    "test_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='test')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ChemBERTa were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig.from_config_file(PATH_TO_CHEMBERTA_CONFIG)\n",
    "model = AutoModel.from_config(model_config)\n",
    "model.set_prediction_task(task_type='regression', out_size=1, hidden_size=384, dropout=0.144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig.from_config_file(PATH_TO_TRAINER_CONFIG)\n",
    "\n",
    "trainer = Trainer(\n",
    "    config=trainer_config,\n",
    "    model=model,\n",
    "    test_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    "    )\n",
    "trainer._init_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.064919906278261"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust trainer config to train dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jointformer(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(593, 16)\n",
       "    (wpe): Embedding(128, 16)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn_1): SelfAttention(\n",
       "          (qkv_proj): Linear(in_features=16, out_features=48, bias=False)\n",
       "          (out_proj): Linear(in_features=16, out_features=16, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (fc): Linear(in_features=16, out_features=64, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (proj): Linear(in_features=64, out_features=16, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=16, out_features=593, bias=False)\n",
       "  (mlm_head): Linear(in_features=16, out_features=593, bias=False)\n",
       "  (physchem_head): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=2048, out_features=16, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=16, out_features=200, bias=True)\n",
       "  )\n",
       "  (prediction_head): Linear(in_features=2048, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DeepChem/SmilesTokenizer_PubChem_1M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[12, 19, 22, 23, 17, 19, 31, 18, 15, 20, 15, 15, 15, 17, 19, 18, 15, 15,\n",
       "         20, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    train_dataset[0][0],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=128,\n",
    "    return_special_tokens_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceTokenizerWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(tokenizer, '__len__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChemBERTa' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mstr\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChemBERTa' object has no attribute '__name__'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "str(model.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointformer.utils.properties.smiles.physchem import PhysChem\n",
    "\n",
    "phys_chem = PhysChem()\n",
    "phys_chem.descriptor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BalabanJ',\n",
       " 'BertzCT',\n",
       " 'Chi0',\n",
       " 'Chi0n',\n",
       " 'Chi0v',\n",
       " 'Chi1',\n",
       " 'Chi1n',\n",
       " 'Chi1v',\n",
       " 'Chi2n',\n",
       " 'Chi2v',\n",
       " 'Chi3n',\n",
       " 'Chi3v',\n",
       " 'Chi4n',\n",
       " 'Chi4v',\n",
       " 'EState_VSA1',\n",
       " 'EState_VSA10',\n",
       " 'EState_VSA11',\n",
       " 'EState_VSA2',\n",
       " 'EState_VSA3',\n",
       " 'EState_VSA4',\n",
       " 'EState_VSA5',\n",
       " 'EState_VSA6',\n",
       " 'EState_VSA7',\n",
       " 'EState_VSA8',\n",
       " 'EState_VSA9',\n",
       " 'ExactMolWt',\n",
       " 'FpDensityMorgan1',\n",
       " 'FpDensityMorgan2',\n",
       " 'FpDensityMorgan3',\n",
       " 'FractionCSP3',\n",
       " 'HallKierAlpha',\n",
       " 'HeavyAtomCount',\n",
       " 'HeavyAtomMolWt',\n",
       " 'Ipc',\n",
       " 'Kappa1',\n",
       " 'Kappa2',\n",
       " 'Kappa3',\n",
       " 'LabuteASA',\n",
       " 'MaxAbsEStateIndex',\n",
       " 'MaxAbsPartialCharge',\n",
       " 'MaxEStateIndex',\n",
       " 'MaxPartialCharge',\n",
       " 'MinAbsEStateIndex',\n",
       " 'MinAbsPartialCharge',\n",
       " 'MinEStateIndex',\n",
       " 'MinPartialCharge',\n",
       " 'MolLogP',\n",
       " 'MolMR',\n",
       " 'MolWt',\n",
       " 'NHOHCount',\n",
       " 'NOCount',\n",
       " 'NumAliphaticCarbocycles',\n",
       " 'NumAliphaticHeterocycles',\n",
       " 'NumAliphaticRings',\n",
       " 'NumAromaticCarbocycles',\n",
       " 'NumAromaticHeterocycles',\n",
       " 'NumAromaticRings',\n",
       " 'NumHAcceptors',\n",
       " 'NumHDonors',\n",
       " 'NumHeteroatoms',\n",
       " 'NumRadicalElectrons',\n",
       " 'NumRotatableBonds',\n",
       " 'NumSaturatedCarbocycles',\n",
       " 'NumSaturatedHeterocycles',\n",
       " 'NumSaturatedRings',\n",
       " 'NumValenceElectrons',\n",
       " 'PEOE_VSA1',\n",
       " 'PEOE_VSA10',\n",
       " 'PEOE_VSA11',\n",
       " 'PEOE_VSA12',\n",
       " 'PEOE_VSA13',\n",
       " 'PEOE_VSA14',\n",
       " 'PEOE_VSA2',\n",
       " 'PEOE_VSA3',\n",
       " 'PEOE_VSA4',\n",
       " 'PEOE_VSA5',\n",
       " 'PEOE_VSA6',\n",
       " 'PEOE_VSA7',\n",
       " 'PEOE_VSA8',\n",
       " 'PEOE_VSA9',\n",
       " 'RingCount',\n",
       " 'SMR_VSA1',\n",
       " 'SMR_VSA10',\n",
       " 'SMR_VSA2',\n",
       " 'SMR_VSA3',\n",
       " 'SMR_VSA4',\n",
       " 'SMR_VSA5',\n",
       " 'SMR_VSA6',\n",
       " 'SMR_VSA7',\n",
       " 'SMR_VSA8',\n",
       " 'SMR_VSA9',\n",
       " 'SlogP_VSA1',\n",
       " 'SlogP_VSA10',\n",
       " 'SlogP_VSA11',\n",
       " 'SlogP_VSA12',\n",
       " 'SlogP_VSA2',\n",
       " 'SlogP_VSA3',\n",
       " 'SlogP_VSA4',\n",
       " 'SlogP_VSA5',\n",
       " 'SlogP_VSA6',\n",
       " 'SlogP_VSA7',\n",
       " 'SlogP_VSA8',\n",
       " 'SlogP_VSA9',\n",
       " 'TPSA',\n",
       " 'VSA_EState1',\n",
       " 'VSA_EState10',\n",
       " 'VSA_EState2',\n",
       " 'VSA_EState3',\n",
       " 'VSA_EState4',\n",
       " 'VSA_EState5',\n",
       " 'VSA_EState6',\n",
       " 'VSA_EState7',\n",
       " 'VSA_EState8',\n",
       " 'VSA_EState9',\n",
       " 'fr_Al_COO',\n",
       " 'fr_Al_OH',\n",
       " 'fr_Al_OH_noTert',\n",
       " 'fr_ArN',\n",
       " 'fr_Ar_COO',\n",
       " 'fr_Ar_N',\n",
       " 'fr_Ar_NH',\n",
       " 'fr_Ar_OH',\n",
       " 'fr_COO',\n",
       " 'fr_COO2',\n",
       " 'fr_C_O',\n",
       " 'fr_C_O_noCOO',\n",
       " 'fr_C_S',\n",
       " 'fr_HOCCN',\n",
       " 'fr_Imine',\n",
       " 'fr_NH0',\n",
       " 'fr_NH1',\n",
       " 'fr_NH2',\n",
       " 'fr_N_O',\n",
       " 'fr_Ndealkylation1',\n",
       " 'fr_Ndealkylation2',\n",
       " 'fr_Nhpyrrole',\n",
       " 'fr_SH',\n",
       " 'fr_aldehyde',\n",
       " 'fr_alkyl_carbamate',\n",
       " 'fr_alkyl_halide',\n",
       " 'fr_allylic_oxid',\n",
       " 'fr_amide',\n",
       " 'fr_amidine',\n",
       " 'fr_aniline',\n",
       " 'fr_aryl_methyl',\n",
       " 'fr_azide',\n",
       " 'fr_azo',\n",
       " 'fr_barbitur',\n",
       " 'fr_benzene',\n",
       " 'fr_benzodiazepine',\n",
       " 'fr_bicyclic',\n",
       " 'fr_diazo',\n",
       " 'fr_dihydropyridine',\n",
       " 'fr_epoxide',\n",
       " 'fr_ester',\n",
       " 'fr_ether',\n",
       " 'fr_furan',\n",
       " 'fr_guanido',\n",
       " 'fr_halogen',\n",
       " 'fr_hdrzine',\n",
       " 'fr_hdrzone',\n",
       " 'fr_imidazole',\n",
       " 'fr_imide',\n",
       " 'fr_isocyan',\n",
       " 'fr_isothiocyan',\n",
       " 'fr_ketone',\n",
       " 'fr_ketone_Topliss',\n",
       " 'fr_lactam',\n",
       " 'fr_lactone',\n",
       " 'fr_methoxy',\n",
       " 'fr_morpholine',\n",
       " 'fr_nitrile',\n",
       " 'fr_nitro',\n",
       " 'fr_nitro_arom',\n",
       " 'fr_nitro_arom_nonortho',\n",
       " 'fr_nitroso',\n",
       " 'fr_oxazole',\n",
       " 'fr_oxime',\n",
       " 'fr_para_hydroxylation',\n",
       " 'fr_phenol',\n",
       " 'fr_phenol_noOrthoHbond',\n",
       " 'fr_phos_acid',\n",
       " 'fr_phos_ester',\n",
       " 'fr_piperdine',\n",
       " 'fr_piperzine',\n",
       " 'fr_priamide',\n",
       " 'fr_prisulfonamd',\n",
       " 'fr_pyridine',\n",
       " 'fr_quatN',\n",
       " 'fr_sulfide',\n",
       " 'fr_sulfonamd',\n",
       " 'fr_sulfone',\n",
       " 'fr_term_acetylene',\n",
       " 'fr_tetrazole',\n",
       " 'fr_thiazole',\n",
       " 'fr_thiocyan',\n",
       " 'fr_thiophene',\n",
       " 'fr_unbrch_alkane',\n",
       " 'fr_urea',\n",
       " 'qed']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointformer-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
