{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyformer Examples\n",
    "\n",
    "This notebook shows how to use Hyformer, as a SMILES encoder, generator and how to train the model with default Trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from hyformer.configs.tokenizer import TokenizerConfig\n",
    "from hyformer.configs.model import ModelConfig\n",
    "\n",
    "from hyformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hyformer.models.auto import AutoModel\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from hyformer.configs.dataset import DatasetConfig\n",
    "from hyformer.configs.tokenizer import TokenizerConfig\n",
    "from hyformer.configs.model import ModelConfig\n",
    "from hyformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from hyformer.utils.datasets.auto import AutoDataset\n",
    "from hyformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hyformer.models.auto import AutoModel\n",
    "\n",
    "from hyformer.utils.runtime import set_seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# set seed\n",
    "SEED = 1337\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set working directory of the project\n",
    "\n",
    "REPOSITORY_DIR = '/home/aih/adam.izdebski/project/hyformer-interface/hyformer'\n",
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Fibrotic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = '/lustre/groups/aih/hyformer/icml25/data/fibrosis/raw/FDA_Div_Filtered_Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv(DATA_FILEPATH, index_col=0)\n",
    "SEQUENCE_COLUMN = 'standard_smiles'\n",
    "\n",
    "idx = _df.index.tolist()\n",
    "smiles = _df[SEQUENCE_COLUMN].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>standard_smiles</th>\n",
       "      <th>Hits</th>\n",
       "      <th>Library</th>\n",
       "      <th>SMILES_no_chiral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10166</th>\n",
       "      <td>CC1=NN(C2CCCCC2)C2=C1C(CC(=O)N2)C1=C(C)N=CN1</td>\n",
       "      <td>Cc1nc[nH]c1C1CC(=O)Nc2c1c(C)nn2C1CCCCC1</td>\n",
       "      <td>True</td>\n",
       "      <td>Diversity</td>\n",
       "      <td>Cc1nc[nH]c1C1CC(=O)Nc2c1c(C)nn2C1CCCCC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>CCCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)C</td>\n",
       "      <td>CCCCNC(=O)NS(=O)(=O)c1ccc(C)cc1</td>\n",
       "      <td>True</td>\n",
       "      <td>MCE</td>\n",
       "      <td>CCCCNC(=O)NS(=O)(=O)c1ccc(C)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>COC1=CC=C(NC2=C3C4=C(C=CC=C4)C(=O)C4=C3C(=CC=C...</td>\n",
       "      <td>COc1ccc(Nc2c3c4c(c(Br)ccc4n(C)c2=O)C(=O)c2cccc...</td>\n",
       "      <td>True</td>\n",
       "      <td>Diversity</td>\n",
       "      <td>COc1ccc(Nc2c3c4c(c(Br)ccc4n(C)c2=O)C(=O)c2cccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>CN1C2=C(C(=O)N(C1=O)C)N(C=N2)CC3OCCO3</td>\n",
       "      <td>Cn1c(=O)c2c(ncn2CC2OCCO2)n(C)c1=O</td>\n",
       "      <td>True</td>\n",
       "      <td>Prestwick</td>\n",
       "      <td>Cn1c(=O)c2c(ncn2CC2OCCO2)n(C)c1=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9049</th>\n",
       "      <td>[H]N([H])C12CC3CC(C1)CC(C3)(C2)N1C=NC=N1</td>\n",
       "      <td>NC12CC3CC(C1)CC(n1cncn1)(C3)C2</td>\n",
       "      <td>True</td>\n",
       "      <td>Diversity</td>\n",
       "      <td>NC12CC3CC(C1)CC(n1cncn1)(C3)C2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  SMILES  \\\n",
       "10166       CC1=NN(C2CCCCC2)C2=C1C(CC(=O)N2)C1=C(C)N=CN1   \n",
       "932                   CCCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)C   \n",
       "499    COC1=CC=C(NC2=C3C4=C(C=CC=C4)C(=O)C4=C3C(=CC=C...   \n",
       "758                CN1C2=C(C(=O)N(C1=O)C)N(C=N2)CC3OCCO3   \n",
       "9049            [H]N([H])C12CC3CC(C1)CC(C3)(C2)N1C=NC=N1   \n",
       "\n",
       "                                         standard_smiles  Hits    Library  \\\n",
       "10166            Cc1nc[nH]c1C1CC(=O)Nc2c1c(C)nn2C1CCCCC1  True  Diversity   \n",
       "932                      CCCCNC(=O)NS(=O)(=O)c1ccc(C)cc1  True        MCE   \n",
       "499    COc1ccc(Nc2c3c4c(c(Br)ccc4n(C)c2=O)C(=O)c2cccc...  True  Diversity   \n",
       "758                    Cn1c(=O)c2c(ncn2CC2OCCO2)n(C)c1=O  True  Prestwick   \n",
       "9049                      NC12CC3CC(C1)CC(n1cncn1)(C3)C2  True  Diversity   \n",
       "\n",
       "                                        SMILES_no_chiral  \n",
       "10166            Cc1nc[nH]c1C1CC(=O)Nc2c1c(C)nn2C1CCCCC1  \n",
       "932                      CCCCNC(=O)NS(=O)(=O)c1ccc(C)cc1  \n",
       "499    COc1ccc(Nc2c3c4c(c(Br)ccc4n(C)c2=O)C(=O)c2cccc...  \n",
       "758                    Cn1c(=O)c2c(ncn2CC2OCCO2)n(C)c1=O  \n",
       "9049                      NC12CC3CC(C1)CC(n1cncn1)(C3)C2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "PATH_TO_TOKENIZER_CONFIG = 'configs/tokenizers/smiles_separate_task_token/config.json'\n",
    "PATH_TO_MODEL_CONFIG = 'configs/models/hyformer_best_big/config.json'\n",
    "PATH_TO_MODEL_CKPT = '/lustre/groups/aih/hyformer/icml25/results/pretrain/unimol/hyformer/llama_backbone_best_big_250k_iters/pretrain_generation_mlm_physchem/ckpt.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyformerWithPrefix(\n",
       "  (token_embedding): Embedding(596, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TransformerLayer(\n",
       "      (attention_layer): Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (relative_embedding): RotaryPositionalEmbedding()\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (w2): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      )\n",
       "      (attention_layer_normalization): RMSNorm()\n",
       "      (feed_forward_normalization): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): RMSNorm()\n",
       "  (lm_head): Linear(in_features=512, out_features=596, bias=False)\n",
       "  (mlm_head): Linear(in_features=512, out_features=596, bias=False)\n",
       "  (physchem_head): RegressionHead(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=256, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)\n",
    "\n",
    "# Model\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config)\n",
    "model.load_pretrained(PATH_TO_MODEL_CKPT)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyformer as a SMILES Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Smiles Encoder as a wrapper for molecule encoding\n",
    "\n",
    "encoding_batch_size = 256\n",
    "smiles_encoder = model.to_smiles_encoder(tokenizer=tokenizer, device=device, batch_size=encoding_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding samples: 100%|██████████| 64/64 [00:39<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode smiles\n",
    "\n",
    "encoding = smiles_encoder.encode(smiles)  # setting the device and model.eval() under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16197, 512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILEPATH = '/lustre/groups/aih/hyformer/icml25/data/fibrosis/embeddings/hyformer.npz'\n",
    "if not os.path.exists(EMBEDDINGS_FILEPATH):\n",
    "    os.makedirs(os.path.dirname(EMBEDDINGS_FILEPATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(EMBEDDINGS_FILEPATH, embeddings=encoding, smiles=smiles, ids=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08279007, -0.25026557, -0.45100284, ..., -0.6174221 ,\n",
       "        -0.12018453,  0.20276001],\n",
       "       [ 0.0333407 ,  0.86286128, -0.60028887, ..., -0.70104438,\n",
       "        -0.51607561, -0.94197947],\n",
       "       [ 0.04557168, -0.53243083, -1.06156468, ..., -0.28652242,\n",
       "         0.22138278,  0.23150636],\n",
       "       ...,\n",
       "       [-0.06491564,  0.15110879, -0.62854481, ..., -0.85657644,\n",
       "        -0.8597644 , -1.67475033],\n",
       "       [-0.11813052,  0.62057179, -0.78862149, ..., -0.33209422,\n",
       "        -0.50159997, -0.81410062],\n",
       "       [ 0.19068508,  0.12450401, -0.81713712, ..., -0.18272324,\n",
       "        -0.41439158,  0.1988963 ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MolGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'molgpt'\n",
    "\n",
    "# MolGPT Configs\n",
    "\n",
    "PATH_TO_TOKENIZER_CONFIG = 'configs/tokenizers/gpt_tokenizer/config.json'\n",
    "PATH_TO_MODEL_CONFIG = 'configs/models/molgpt/config.json'\n",
    "PATH_TO_MODEL_CKPT = '/lustre/groups/aih/hyformer/icml25/results/pretrain/guacamol/molgpt/guacamol_nocond.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed key: prop_nn.weight from checkpoint; not found in model or size mismatch.\n",
      "Removed key: prop_nn.bias from checkpoint; not found in model or size mismatch.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)\n",
    "\n",
    "# Model\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config)\n",
    "smiles_encoder = model.to_smiles_encoder(tokenizer=tokenizer, device=device, batch_size=encoding_batch_size)\n",
    "smiles_encoder.load_pretrained(PATH_TO_MODEL_CKPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering data: 100%|██████████| 16197/16197 [00:00<00:00, 98742.36it/s] \n",
      "Filtered 5657 examples due to unknown characters.\n",
      "100%|██████████| 64/64 [00:15<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode smiles\n",
    "\n",
    "encoding = smiles_encoder.encode(smiles)  # setting the device and model.eval() under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16197, 256)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILEPATH = f'/lustre/groups/aih/hyformer/icml25/data/fibrosis/embeddings/{MODEL_NAME}.npz'\n",
    "if not os.path.exists(EMBEDDINGS_FILEPATH):\n",
    "    os.makedirs(os.path.dirname(EMBEDDINGS_FILEPATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(EMBEDDINGS_FILEPATH, embeddings=encoding, smiles=smiles, ids=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChemBERT-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'chemberta-2'\n",
    "\n",
    "# ChemBERT-a Configs\n",
    "\n",
    "PATH_TO_TOKENIZER_CONFIG = 'configs/tokenizers/chemberta/config.json'\n",
    "PATH_TO_MODEL_CONFIG = 'configs/models/chemberta_for_regression/config.json'\n",
    "PATH_TO_MODEL_CKPT = '/lustre/groups/aih/hyformer/icml25/results/pretrain/unimol/hyformer/llama_backbone_best_big_250k_iters/pretrain_generation_mlm_physchem/ckpt.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)\n",
    "\n",
    "# Model\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config)\n",
    "smiles_encoder = model.to_smiles_encoder(tokenizer=tokenizer, device=device, batch_size=encoding_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding samples: 100%|██████████| 64/64 [00:07<00:00,  9.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode smiles\n",
    "\n",
    "encoding = smiles_encoder.encode(smiles)  # setting the device and model.eval() under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILEPATH = f'/lustre/groups/aih/hyformer/icml25/data/fibrosis/embeddings/{MODEL_NAME}.npz'\n",
    "if not os.path.exists(EMBEDDINGS_FILEPATH):\n",
    "    os.makedirs(os.path.dirname(EMBEDDINGS_FILEPATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(EMBEDDINGS_FILEPATH, embeddings=encoding, smiles=smiles, ids=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lustre/groups/aih/hyformer/icml25/data/fibrosis/embeddings/chemberta-2.npz'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDINGS_FILEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni-Mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'unimol'\n",
    "\n",
    "# Uni-Mol Configs\n",
    "\n",
    "PATH_TO_MODEL_CONFIG = 'configs/models/unimol/config.json'\n",
    "PATH_TO_MODEL_CKPT = '/lustre/groups/aih/hyformer/icml25/results/pretrain/guacamol/unimol/mol_pre_no_h_220816.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:55:12 | unimol_tools/models/unimol.py | 135 | INFO | Uni-Mol Tools | Loading pretrained weights from /home/aih/adam.izdebski/miniconda3/envs/hyformer/lib/python3.9/site-packages/unimol_tools/weights/mol_pre_all_h_220816.pt\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "# Model\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config)\n",
    "smiles_encoder = model.to_smiles_encoder(tokenizer=tokenizer, device=device, batch_size=encoding_batch_size)\n",
    "smiles_encoder.load_pretrained(PATH_TO_MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:55:33 | unimol_tools/data/conformer.py | 150 | INFO | Uni-Mol Tools | Start generating conformers...\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "16197it [04:05, 66.04it/s] \n",
      "2025-04-01 15:59:39 | unimol_tools/data/conformer.py | 161 | INFO | Uni-Mol Tools | Succeeded in generating conformers for 100.00% of molecules.\n",
      "2025-04-01 15:59:39 | unimol_tools/data/conformer.py | 178 | INFO | Uni-Mol Tools | Succeeded in generating 3d conformers for 99.90% of molecules.\n",
      "2025-04-01 15:59:39 | unimol_tools/data/conformer.py | 187 | INFO | Uni-Mol Tools | Failed 3d conformers indices: [358, 434, 1095, 3283, 3290, 3358, 3457, 4362, 4554, 5181, 5462, 5601, 5875, 6003, 10728, 11087]\n",
      "2025-04-01 15:59:40 | unimol_tools/tasks/trainer.py | 77 | INFO | Uni-Mol Tools | Number of GPUs available: 1\n",
      "2025-04-01 15:59:40 | unimol_tools/tasks/trainer.py | 97 | INFO | Uni-Mol Tools | Using single GPU.\n",
      "100%|██████████| 507/507 [00:44<00:00, 11.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode smiles\n",
    "\n",
    "encoding = smiles_encoder.encode(smiles)  # setting the device and model.eval() under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_FILEPATH = f'/lustre/groups/aih/hyformer/icml25/data/fibrosis/embeddings/{MODEL_NAME}.npz'\n",
    "if not os.path.exists(EMBEDDINGS_FILEPATH):\n",
    "    os.makedirs(os.path.dirname(EMBEDDINGS_FILEPATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(EMBEDDINGS_FILEPATH, embeddings=encoding, smiles=smiles, ids=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unimol'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16197, 512)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
