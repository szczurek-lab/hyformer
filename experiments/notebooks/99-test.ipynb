{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "from hyformer.configs.dataset import DatasetConfig\n",
    "from hyformer.configs.tokenizer import TokenizerConfig\n",
    "from hyformer.configs.model import ModelConfig\n",
    "from hyformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from hyformer.utils.datasets.auto import AutoDataset\n",
    "from hyformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hyformer.models.auto import AutoModel\n",
    "# from hyformer.trainers.trainer import Trainer\n",
    "\n",
    "from hyformer.utils.runtime import set_seed\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory of the project\n",
    "\n",
    "REPOSITORY_DIR = '/home/aih/adam.izdebski/projects/hyformer'\n",
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "\n",
    "set_seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "DATA_DIR = '/lustre/groups/aih/jointformer/icml25/data'\n",
    "OUTPUT_DIR = '/lustre/groups/aih/jointformer/icml25/results'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = 'configs/datasets/guacamol/config.json'\n",
    "PATH_TO_TOKENIZER_CONFIG = 'configs/tokenizers/smiles/config.json'\n",
    "PATH_TO_MODEL_CONFIG = 'configs/models/hyformer_tiny/config.json'\n",
    "PATH_TO_TRAINER_CONFIG = 'configs/trainers/pretrain/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_path(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_path(PATH_TO_TOKENIZER_CONFIG)\n",
    "model_config = ModelConfig.from_config_path(PATH_TO_MODEL_CONFIG)\n",
    "trainer_config = TrainerConfig.from_config_path(PATH_TO_TRAINER_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, root=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, root=DATA_DIR, split='val')\n",
    "test_dataset = AutoDataset.from_config(dataset_config, root=DATA_DIR, split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0ad5afc4814acf8d76e1898ee3cf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1273104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))):\n\u001b[1;32m      4\u001b[0m     smiles \u001b[38;5;241m=\u001b[39m train_dataset[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m smiles \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(input_ids):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/utils/tokenizers/smiles.py:252\u001b[0m, in \u001b[0;36mSMILESTokenizer.__call__\u001b[0;34m(self, inputs, task, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# -3 for task, BOS, and EOS tokens\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Convert to IDs\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Add special tokens\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_special_tokens(token_ids, task)\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/utils/tokenizers/smiles.py:200\u001b[0m, in \u001b[0;36mSMILESTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_tokens_to_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Convert tokens to their IDs.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m        List of token IDs\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_token_id) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/utils/tokenizers/smiles.py:200\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_tokens_to_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Convert tokens to their IDs.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m        List of token IDs\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munk_token_id\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/utils/tokenizers/smiles.py:141\u001b[0m, in \u001b[0;36mSMILESTokenizer.unk_token_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munk_token_id\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"ID for the unknown token.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munk_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "not_found = 0\n",
    "\n",
    "for idx in tqdm(range(len(train_dataset))):\n",
    "    smiles = train_dataset[idx]['data']\n",
    "    input_ids = tokenizer(smiles, task=\"lm\")['input_ids']\n",
    "    if smiles != tokenizer.decode(input_ids):\n",
    "        print(\"-\"*100)\n",
    "        print(f\"IDX: {idx}\")\n",
    "        print(f\"UNK token found: {tokenizer.unk_token_id in input_ids}\")\n",
    "        print(smiles)\n",
    "        print(tokenizer.decode(tokenizer(smiles, task=\"lm\")['input_ids']))\n",
    "        not_found += 1\n",
    "        print(\"-\"*100)   \n",
    "print(not_found)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# assert attention mask is boolean\n",
    "\n",
    "model = AutoModel.from_config(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_prediction_head(num_prediction_tasks=1, prediction_task_type='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyformer(\n",
       "  (token_embedding): Embedding(596, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (attention_layer): Attention(\n",
       "        (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (out): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (relative_embedding): RotaryEmbedding()\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (w2): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      )\n",
       "      (attention_layer_normalization): RMSNorm()\n",
       "      (feed_forward_normalization): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): RMSNorm()\n",
       "  (lm_head): Linear(in_features=512, out_features=596, bias=False)\n",
       "  (mlm_head): Linear(in_features=512, out_features=596, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_token_id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/models/utils.py:72\u001b[0m, in \u001b[0;36minference.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Disable grad and run inference\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 72\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Restore training mode if needed\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m was_training:\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/models/hyformer.py:213\u001b[0m, in \u001b[0;36mHyformer.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k, eos_token_id, pad_token_id)\u001b[0m\n\u001b[1;32m    210\u001b[0m eos_flags \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39midx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[0;32m--> 213\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_token_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    222\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/lustre/groups/aih/hyformer/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/groups/aih/hyformer/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/models/hyformer.py:111\u001b[0m, in \u001b[0;36mHyformer.forward\u001b[0;34m(self, input_ids, task, attention_mask, next_token_only, past_key_values, use_cache)\u001b[0m\n\u001b[1;32m    108\u001b[0m     _is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     _attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[0;32m--> 111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_token_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    121\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/projects/hyformer/hyformer/models/llama_backbone.py:66\u001b[0m, in \u001b[0;36mLLAMABackbone.forward\u001b[0;34m(self, input_ids, is_causal, attention_mask, cls_context, next_token_only, past_key_values, use_cache)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# apply the transformer layers\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 66\u001b[0m     past_kv \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     x, present_kv \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m     68\u001b[0m         x,\n\u001b[1;32m     69\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache\n\u001b[1;32m     73\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "samples = model.generate(\n",
    "    idx=torch.tensor([[tokenizer.task_token_id('lm'), tokenizer.bos_token_id]], dtype=torch.long, device='cuda'),\n",
    "    max_new_tokens=10,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Test various components of the generation process.\n",
    "    \n",
    "    Args:\n",
    "        model: The Hyformer model\n",
    "        tokenizer: The tokenizer used with the model\n",
    "    \"\"\"\n",
    "    # Move model to cuda if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a simple input sequence\n",
    "    prefix = \"CC\"  # Simple SMILES string\n",
    "    input_ids = tokenizer(prefix, task=\"lm\")['input_ids']\n",
    "    prefix_input_ids = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    print(\"1. Testing basic generation:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        outputs = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=5,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=1.0,\n",
    "            top_k=25\n",
    "        )\n",
    "        print(\"✓ Basic generation successful\")\n",
    "        print(f\"Input SMILES: {prefix}\")\n",
    "        print(f\"Generated SMILES: {tokenizer.decode(outputs[0])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Basic generation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n2. Testing EOS token handling:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Force early EOS by using low temperature\n",
    "        outputs = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=10,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=0.1,\n",
    "            top_k=25\n",
    "        )\n",
    "        # Check if output contains EOS and PAD tokens\n",
    "        has_eos = tokenizer.eos_token_id in outputs[0]\n",
    "        has_pad = tokenizer.pad_token_id in outputs[0]\n",
    "        print(f\"✓ EOS token present: {has_eos}\")\n",
    "        print(f\"✓ PAD token present: {has_pad}\")\n",
    "        print(f\"Generated sequence: {[int(x) for x in outputs[0]]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EOS token test failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n3. Testing KV caching:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Generate with and without caching and compare time\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=10,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=False\n",
    "        )\n",
    "        no_cache_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=10,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        cache_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"✓ Generation time without cache: {no_cache_time:.4f}s\")\n",
    "        print(f\"✓ Generation time with cache: {cache_time:.4f}s\")\n",
    "        print(f\"✓ Speedup from caching: {no_cache_time/cache_time:.2f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ KV caching test failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n4. Testing batch generation:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Create a batch of 3 sequences\n",
    "        batch_input_ids = torch.cat([prefix_input_ids] * 3, dim=0)\n",
    "        outputs = model.generate(\n",
    "            prefix_input_ids=batch_input_ids,\n",
    "            num_tokens_to_generate=5,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        print(f\"✓ Batch generation successful\")\n",
    "        print(f\"Batch size: {outputs.shape[0]}\")\n",
    "        for i, seq in enumerate(outputs):\n",
    "            print(f\"Sequence {i}: {tokenizer.decode(seq)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch generation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n5. Testing sampling parameters:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Test different temperatures\n",
    "        temps = [0.1, 1.0, 2.0]\n",
    "        for temp in temps:\n",
    "            outputs = model.generate(\n",
    "                prefix_input_ids=prefix_input_ids,\n",
    "                num_tokens_to_generate=5,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                temperature=temp,\n",
    "                top_k=25\n",
    "            )\n",
    "            print(f\"Temperature {temp}: {tokenizer.decode(outputs[0])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Sampling parameter test failed: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "test_generation(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
