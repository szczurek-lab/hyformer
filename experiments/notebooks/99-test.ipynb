{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "from hyformer.configs.dataset import DatasetConfig\n",
    "from hyformer.configs.tokenizer import TokenizerConfig\n",
    "from hyformer.configs.model import ModelConfig\n",
    "from hyformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from hyformer.utils.datasets.auto import AutoDataset\n",
    "from hyformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from hyformer.models.auto import AutoModel\n",
    "from hyformer.trainers.trainer import Trainer\n",
    "\n",
    "from hyformer.utils.runtime import set_seed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory of the project\n",
    "\n",
    "REPOSITORY_DIR = '/home/aih/adam.izdebski/project/jointformer-interface/jointformer'\n",
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "\n",
    "set_seed(1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "DATA_DIR = '/lustre/groups/aih/jointformer/icml25/data'\n",
    "OUTPUT_DIR = '/lustre/groups/aih/jointformer/icml25/results'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = 'configs/datasets/guacamol/config.json'\n",
    "PATH_TO_TOKENIZER_CONFIG = 'configs/tokenizers/regex_smiles/config.json'\n",
    "PATH_TO_MODEL_CONFIG = 'configs/models/hyformer/config.json'\n",
    "PATH_TO_TRAINER_CONFIG = 'configs/trainers/pretrain/config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "trainer_config = TrainerConfig.from_config_file(PATH_TO_TRAINER_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, root=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, root=DATA_DIR, split='val')\n",
    "test_dataset = AutoDataset.from_config(dataset_config, root=DATA_DIR, split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer SMILESRegexTokenizer not available. Available options: 'SMILESTokenizer', 'HFTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ictstr01/home/aih/adam.izdebski/project/jointformer-interface/jointformer/hyformer/utils/tokenizers/auto.py:35\u001b[0m, in \u001b[0;36mAutoTokenizer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyformer.utils.tokenizers.hf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHFTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_config(config)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mtokenizer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not available. Available options: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILESTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHFTokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer SMILESRegexTokenizer not available. Available options: 'SMILESTokenizer', 'HFTokenizer'"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenizerConfig(path_to_vocabulary='data/vocabularies/deepchem.txt', tokenizer_type='SMILESTokenizer')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 'CCC(C)(C)Br', 'target': None}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([591,  16,  16,  16,  17,  16,  18,  17,  16,  18,  37,  11]),\n",
       " 'attention_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True]),\n",
       " 'special_tokens_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_dataset[0]['data'], task='lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requriments - transformers, tokenizers\n",
    "# Right now, the Smiles Tokenizer uses an exiesting vocab file from rxnfp that is fairly comprehensive and from the USPTO dataset.\n",
    "# The vocab may be expanded in the near future\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import pkg_resources\n",
    "from typing import List\n",
    "from transformers import BertTokenizer\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\"\"\"\n",
    "SMI_REGEX_PATTERN: str\n",
    "    SMILES regex pattern for tokenization. Designed by Schwaller et. al.\n",
    "\n",
    "References\n",
    "\n",
    ".. [1]  Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee\n",
    "        ACS Central Science 2019 5 (9): Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\n",
    "        1572-1583 DOI: 10.1021/acscentsci.9b00576\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "SMI_REGEX_PATTERN = r\"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|\n",
    "#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "# add vocab_file dict\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "\n",
    "def get_default_tokenizer():\n",
    "  default_vocab_path = (pkg_resources.resource_filename(\"deepchem\",\n",
    "                                                        \"feat/tests/vocab.txt\"))\n",
    "  return SmilesTokenizer(default_vocab_path)\n",
    "\n",
    "\n",
    "class SmilesTokenizer(BertTokenizer):\n",
    "  \"\"\"\n",
    "    Creates the SmilesTokenizer class. The tokenizer heavily inherits from the BertTokenizer\n",
    "    implementation found in Huggingface's transformers library. It runs a WordPiece tokenization\n",
    "    algorithm over SMILES strings using the tokenisation SMILES regex developed by Schwaller et. al.\n",
    "\n",
    "    Please see https://github.com/huggingface/transformers\n",
    "    and https://github.com/rxn4chemistry/rxnfp for more details.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "    >>> current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    >>> vocab_path = os.path.join(current_dir, 'tests/data', 'vocab.txt')\n",
    "    >>> tokenizer = SmilesTokenizer(vocab_path)\n",
    "    >>> print(tokenizer.encode(\"CC(=O)OC1=CC=CC=C1C(=O)O\"))\n",
    "    [12, 16, 16, 17, 22, 19, 18, 19, 16, 20, 22, 16, 16, 22, 16, 16, 22, 16, 20, 16, 17, 22, 19, 18, 19, 13]\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1]  Schwaller, Philippe; Probst, Daniel; Vaucher, Alain C.; Nair, Vishnu H; Kreutter, David;\n",
    "            Laino, Teodoro; et al. (2019): Mapping the Space of Chemical Reactions using Attention-Based Neural\n",
    "            Networks. ChemRxiv. Preprint. https://doi.org/10.26434/chemrxiv.9897365.v3\n",
    "\n",
    "    Notes\n",
    "    ----\n",
    "    This class requires huggingface's transformers and tokenizers libraries to be installed.\n",
    "    \"\"\"\n",
    "  vocab_files_names = VOCAB_FILES_NAMES\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      vocab_file: str = '',\n",
    "      # unk_token=\"[UNK]\",\n",
    "      # sep_token=\"[SEP]\",\n",
    "      # pad_token=\"[PAD]\",\n",
    "      # cls_token=\"[CLS]\",\n",
    "      # mask_token=\"[MASK]\",\n",
    "      **kwargs):\n",
    "    \"\"\"Constructs a SmilesTokenizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_file: str\n",
    "            Path to a SMILES character per line vocabulary file.\n",
    "            Default vocab file is found in deepchem/feat/tests/data/vocab.txt\n",
    "        \"\"\"\n",
    "\n",
    "    super().__init__(vocab_file, **kwargs)\n",
    "    # take into account special tokens in max length\n",
    "    self.max_len_single_sentence = self.max_len - 2\n",
    "    self.max_len_sentences_pair = self.max_len - 3\n",
    "\n",
    "    if not os.path.isfile(vocab_file):\n",
    "      raise ValueError(\n",
    "          \"Can't find a vocab file at path '{}'.\".format(vocab_file))\n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    self.highest_unused_index = max(\n",
    "        [i for i, v in enumerate(self.vocab.keys()) if v.startswith(\"[unused\")])\n",
    "    self.ids_to_tokens = collections.OrderedDict(\n",
    "        [(ids, tok) for tok, ids in self.vocab.items()])\n",
    "    self.basic_tokenizer = BasicSmilesTokenizer()\n",
    "    self.init_kwargs[\"max_len\"] = self.max_len\n",
    "\n",
    "  @property\n",
    "  def vocab_size(self):\n",
    "    return len(self.vocab)\n",
    "\n",
    "  @property\n",
    "  def vocab_list(self):\n",
    "    return list(self.vocab.keys())\n",
    "\n",
    "  def _tokenize(self, text: str):\n",
    "    \"\"\"\n",
    "        Tokenize a string into a list of tokens.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text: str\n",
    "            Input string sequence to be tokenized.\n",
    "        \"\"\"\n",
    "\n",
    "    split_tokens = [token for token in self.basic_tokenizer.tokenize(text)]\n",
    "    return split_tokens\n",
    "\n",
    "  def _convert_token_to_id(self, token):\n",
    "    \"\"\"\n",
    "        Converts a token (str/unicode) in an id using the vocab.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token: str\n",
    "            String token from a larger sequence to be converted to a numerical id.\n",
    "        \"\"\"\n",
    "\n",
    "    return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "  def _convert_id_to_token(self, index):\n",
    "    \"\"\"\n",
    "        Converts an index (integer) in a token (string/unicode) using the vocab.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index: int\n",
    "            Integer index to be converted back to a string-based token as part of a larger sequence.\n",
    "        \"\"\"\n",
    "\n",
    "    return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "  def convert_tokens_to_string(self, tokens: List[str]):\n",
    "    \"\"\" Converts a sequence of tokens (string) in a single string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: List[str]\n",
    "            List of tokens for a given string sequence.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_string: str\n",
    "            Single string from combined tokens.\n",
    "        \"\"\"\n",
    "\n",
    "    out_string: str = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "    return out_string\n",
    "\n",
    "  def add_special_tokens_ids_single_sequence(self, token_ids: List[int]):\n",
    "    \"\"\"\n",
    "        Adds special tokens to the a sequence for sequence classification tasks.\n",
    "        A BERT sequence has the following format: [CLS] X [SEP]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        token_ids: list[int]\n",
    "            list of tokenized input ids. Can be obtained using the encode or encode_plus methods.\n",
    "        \"\"\"\n",
    "\n",
    "    return [self.cls_token_id] + token_ids + [self.sep_token_id]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMI_REGEX_PATTERN = r\"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class RegexSmilesTokenizer:\n",
    "  \"\"\" Run basic SMILES tokenization using a regex pattern developed by Schwaller et. al [1]. \n",
    "  \n",
    "  \n",
    "  References:\n",
    "  ----------\n",
    "  [1]  Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee\n",
    "        ACS Central Science 2019 5 (9): Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\n",
    "        1572-1583 DOI: 10.1021/acscentsci.9b00576   \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, regex_pattern: str = SMI_REGEX_PATTERN):\n",
    "    \"\"\" Constructs a BasicSMILESTokenizer.\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        regex: string\n",
    "         SMILES token regex\n",
    "    \"\"\"\n",
    "    self.regex_pattern = regex_pattern\n",
    "    self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\" Basic Tokenization of a SMILES.\n",
    "    \"\"\"\n",
    "    tokens = [token for token in self.regex.findall(text)]\n",
    "    return tokens\n",
    "  \n",
    "  def add_special_tokens_single_sequence(self, tokens: List[str]):\n",
    "    return [self.cls_token] + tokens + [self.sep_token]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'C',\n",
       " '(',\n",
       " '=',\n",
       " 'O',\n",
       " ')',\n",
       " 'O',\n",
       " 'C',\n",
       " '1',\n",
       " '=',\n",
       " 'C',\n",
       " 'C',\n",
       " '=',\n",
       " 'C',\n",
       " 'C',\n",
       " '=',\n",
       " 'C',\n",
       " '1',\n",
       " 'C',\n",
       " '(',\n",
       " '=',\n",
       " 'O',\n",
       " ')',\n",
       " 'O']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexSmilesTokenizer()\n",
    "tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
