{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generation(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Test various components of the generation process.\n",
    "    \n",
    "    Args:\n",
    "        model: The Hyformer model\n",
    "        tokenizer: The tokenizer used with the model\n",
    "    \"\"\"\n",
    "    # Move model to cuda if available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a simple input sequence\n",
    "    prefix = \"CC\"  # Simple SMILES string\n",
    "    input_ids = tokenizer(prefix, task=\"lm\")['input_ids']\n",
    "    prefix_input_ids = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    print(\"1. Testing basic generation:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        outputs = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=5,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=1.0,\n",
    "            top_k=25\n",
    "        )\n",
    "        print(\"✓ Basic generation successful\")\n",
    "        print(f\"Input SMILES: {prefix}\")\n",
    "        print(f\"Generated SMILES: {tokenizer.decode(outputs[0])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Basic generation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n2. Testing EOS token handling:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Force early EOS by using low temperature\n",
    "        outputs = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=10,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=0.1,\n",
    "            top_k=25\n",
    "        )\n",
    "        # Check if output contains EOS and PAD tokens\n",
    "        has_eos = tokenizer.eos_token_id in outputs[0]\n",
    "        has_pad = tokenizer.pad_token_id in outputs[0]\n",
    "        print(f\"✓ EOS token present: {has_eos}\")\n",
    "        print(f\"✓ PAD token present: {has_pad}\")\n",
    "        print(f\"Generated sequence: {[int(x) for x in outputs[0]]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EOS token test failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n3. Testing KV caching:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Generate with and without caching and compare time\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=10,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=False\n",
    "        )\n",
    "        no_cache_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = model.generate(\n",
    "            prefix_input_ids=prefix_input_ids,\n",
    "            num_tokens_to_generate=10,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        cache_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"✓ Generation time without cache: {no_cache_time:.4f}s\")\n",
    "        print(f\"✓ Generation time with cache: {cache_time:.4f}s\")\n",
    "        print(f\"✓ Speedup from caching: {no_cache_time/cache_time:.2f}x\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ KV caching test failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n4. Testing batch generation:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Create a batch of 3 sequences\n",
    "        batch_input_ids = torch.cat([prefix_input_ids] * 3, dim=0)\n",
    "        outputs = model.generate(\n",
    "            prefix_input_ids=batch_input_ids,\n",
    "            num_tokens_to_generate=5,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        print(f\"✓ Batch generation successful\")\n",
    "        print(f\"Batch size: {outputs.shape[0]}\")\n",
    "        for i, seq in enumerate(outputs):\n",
    "            print(f\"Sequence {i}: {tokenizer.decode(seq)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch generation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n5. Testing sampling parameters:\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        # Test different temperatures\n",
    "        temps = [0.1, 1.0, 2.0]\n",
    "        for temp in temps:\n",
    "            outputs = model.generate(\n",
    "                prefix_input_ids=prefix_input_ids,\n",
    "                num_tokens_to_generate=5,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                temperature=temp,\n",
    "                top_k=25\n",
    "            )\n",
    "            print(f\"Temperature {temp}: {tokenizer.decode(outputs[0])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Sampling parameter test failed: {str(e)}\")\n",
    "\n",
    "# Usage example:\n",
    "test_generation(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
