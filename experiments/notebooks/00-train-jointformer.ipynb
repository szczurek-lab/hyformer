{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "2024-08-06 10:48:12.215290: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-06 10:48:12.353162: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 10:48:12.353193: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 10:48:12.353198: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-06 10:48:12.414273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "REPOSITORY_DIR = '/home/adamizdebski/projects/jointformer'\n",
    "DATA_DIR = '/home/adamizdebski/files/data'\n",
    "OUTPUT_DIR = '/home/adamizdebski/files/jointformer/results/chemberta2/moleculenet'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = '/home/adamizdebski/projects/jointformer/configs/datasets/guacamol'\n",
    "PATH_TO_TOKENIZER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/tokenizers/smiles'\n",
    "PATH_TO_MODEL_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/jointformer_test'\n",
    "PATH_TO_TRAINER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/trainers/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Datsaset\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='val')\n",
    "test_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='test')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify dataset\n",
    "\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def verify_dataset(dataset):\n",
    "    nonvalid_molecule_idx = []\n",
    "    nonvalid_target_idx = []\n",
    "\n",
    "    for idx, (smiles, target) in enumerate(tqdm(dataset)):\n",
    "        try:\n",
    "            Chem.MolFromSmiles(smiles)\n",
    "        except:\n",
    "            nonvalid_molecule_idx.append(idx)\n",
    "        if not torch.all(target == target):\n",
    "            nonvalid_target_idx.append(idx) \n",
    "    \n",
    "    return {\n",
    "        'nonvalid_molecule_idx': nonvalid_molecule_idx,\n",
    "        'nonvalid_target_idx': nonvalid_target_idx\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Random seed set to 1337\n",
      "INFO: tokens per iteration set to: 256\n"
     ]
    }
   ],
   "source": [
    "trainer_config = TrainerConfig.from_config_file(PATH_TO_TRAINER_CONFIG)\n",
    "\n",
    "trainer = Trainer(\n",
    "    config=trainer_config,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Evaluation at step 0: train loss 6.7554, val loss 6.7475\n",
      "INFO: iter 100: loss 5.658967 on physchem task, lr 0.000600, time 188.87ms, mfu 0.00%\n",
      "INFO: Evaluation at step 200: train loss 4.6947, val loss 4.6986\n",
      "INFO: Validation loss: 4.6986\n",
      "INFO: Best validation loss: 1000000000.0000\n",
      "INFO: Checkpoint updated at iteration 200\n",
      "INFO: iter 200: loss 4.621058 on physchem task, lr 0.000300, time 7651.05ms, mfu 0.00%\n",
      "INFO: iter 300: loss 0.081883 on generation task, lr 0.000001, time 203.44ms, mfu 0.00%\n",
      "INFO: Evaluation at step 400: train loss 4.4628, val loss 4.4695\n",
      "INFO: Validation loss: 4.4695\n",
      "INFO: Best validation loss: 4.6986\n",
      "INFO: Checkpoint updated at iteration 400\n",
      "INFO: iter 400: loss 0.095249 on generation task, lr 0.000001, time 7737.54ms, mfu 0.00%\n",
      "INFO: iter 500: loss 4.371212 on physchem task, lr 0.000001, time 197.72ms, mfu 0.00%\n",
      "INFO: Evaluation at step 600: train loss 4.4683, val loss 4.4618\n",
      "INFO: Validation loss: 4.4618\n",
      "INFO: Best validation loss: 4.4695\n",
      "INFO: Checkpoint updated at iteration 600\n",
      "INFO: iter 600: loss 0.079082 on physchem task, lr 0.000001, time 8662.40ms, mfu 0.00%\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.474458694458008"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainer._init_data_loaders()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer.get_training_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = trainer.model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blabla/test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('blabla', '', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointformer-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
