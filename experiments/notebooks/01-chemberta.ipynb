{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-10 13:48:35.271505: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-10 13:48:35.445266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-10 13:48:35.445329: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-10 13:48:35.445358: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-10 13:48:35.493491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Incorporate this https://github.com/seyonechithrananda/bert-loves-chemistry/blob/master/chemberta/utils/roberta_regression.py#L138\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU not detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "REPOSITORY_DIR = '/home/adamizdebski/projects/jointformer'\n",
    "DATA_DIR = '/home/adamizdebski/files/data'\n",
    "OUTPUT_DIR = '/home/adamizdebski/files/jointformer/results/chemberta2/moleculenet'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = '/home/adamizdebski/projects/jointformer/configs/datasets/molecule_net/lipo'\n",
    "PATH_TO_TOKENIZER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/tokenizers/chemberta'\n",
    "PATH_TO_CHEMBERTA_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/chemberta'\n",
    "PATH_TO_MODEL_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/jointformer_test'\n",
    "PATH_TO_TRAINER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/trainers/finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='val')\n",
    "test_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='test')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForRegression were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MLM and are newly initialized: ['regression.dense.bias', 'regression.dense.weight', 'regression.out_proj.bias', 'regression.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig.from_config_file(PATH_TO_CHEMBERTA_CONFIG)\n",
    "model = AutoModel.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig.from_config_file(PATH_TO_TRAINER_CONFIG)\n",
    "\n",
    "trainer = Trainer(\n",
    "    config=trainer_config,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    "    )\n",
    "trainer._init_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = trainer.get_training_batch()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.get_loss(**inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.8823, device='cuda:0'),\n",
       " 'Y_pred': tensor([[-0.0263],\n",
       "         [-0.0886],\n",
       "         [ 0.1256],\n",
       "         [-0.0776],\n",
       "         [-0.0566],\n",
       "         [-0.0208],\n",
       "         [ 0.0161],\n",
       "         [ 0.0104],\n",
       "         [-0.0599],\n",
       "         [-0.1337],\n",
       "         [-0.1310],\n",
       "         [-0.1062],\n",
       "         [ 0.0072],\n",
       "         [-0.0688],\n",
       "         [ 0.0245],\n",
       "         [-0.0207]], device='cuda:0')}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1602163160049317"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "\n",
    "ckpt_dir = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "eval_dir = os.path.join(OUTPUT_DIR, 'eval')\n",
    "\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "if not os.path.exists(eval_dir):\n",
    "    os.makedirs(eval_dir)\n",
    "\n",
    "# set the parameters\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 2\n",
    "patience = 15\n",
    "learning_rate = 0.00001\n",
    "manual_seed = 112\n",
    "\n",
    "wandb_kwargs = {'name' : 'chemberta2-moleculenet-esol'}\n",
    "\n",
    "model_args = {\n",
    "    'evaluate_each_epoch': True,\n",
    "    'evaluate_during_training_verbose': True,\n",
    "    'evaluate_during_training' : True,\n",
    "    'best_model_dir' : ckpt_dir,\n",
    "    'no_save': False,\n",
    "    'save_eval_checkpoints': False,\n",
    "    'save_model_every_epoch': False,\n",
    "    'save_best_model' : True,\n",
    "    'save_steps': -1,\n",
    "    'num_train_epochs': EPOCHS,\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_patience': patience,\n",
    "    'early_stopping_delta': 0.001,\n",
    "    'early_stopping_metrics': 'eval_loss',\n",
    "    'early_stopping_metrics_minimize': True,\n",
    "    'early_stopping_consider_epochs' : True,\n",
    "    'fp16' : False,\n",
    "    'optimizer' : \"AdamW\",\n",
    "    'adam_betas' : (0.95, 0.999),\n",
    "    'learning_rate' : learning_rate,\n",
    "    'manual_seed': manual_seed,\n",
    "    'train_batch_size' : BATCH_SIZE,\n",
    "    'eval_batch_size' : BATCH_SIZE,\n",
    "    'logging_steps' : 2,\n",
    "    'auto_weights': True, # change to true\n",
    "    'wandb_project': 'chemberta',\n",
    "    'wandb_kwargs': wandb_kwargs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train', seed=0, num_samples=100)\n",
    "train_df = train_dataset.get_data_frame()\n",
    "\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train', seed=0, num_samples=100)\n",
    "val_df = val_dataset.get_data_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CCc1c(C)[nH]c2c1C(=O)C(CN1CCOCC1)CC2', tensor([1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel('roberta', 'DeepChem/ChemBERTa-5M-MLM', args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [27:03, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:617\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m         )\n\u001b[1;32m    613\u001b[0m         train_examples \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    614\u001b[0m             train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    615\u001b[0m             train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    616\u001b[0m         )\n\u001b[0;32m--> 617\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    621\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    622\u001b[0m     train_dataset,\n\u001b[1;32m    623\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mtrain_sampler,\n\u001b[1;32m    624\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    625\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:1824\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1824\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_classification_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_cache\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/simpletransformers/classification/classification_utils.py:249\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    244\u001b[0m             (text_a[i : i \u001b[38;5;241m+\u001b[39m chunksize], \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer, args\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    246\u001b[0m         ]\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(args\u001b[38;5;241m.\u001b[39mprocess_count) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 249\u001b[0m         examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_data_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     examples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    258\u001b[0m         key: torch\u001b[38;5;241m.\u001b[39mcat([example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples])\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = model.train_model(train_df, eval_df=val_df, output_dir=ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-5M-MLM\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-5M-MLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer(\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "tokenizer(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CCc1c(C)[nH]c2c1C(=O)C(CN1CCOCC1)CC2', tensor([1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 21,  1, 32,  3,  6, 24, 29,  4,  3, 15, 33, 21, 13, 23,  4, 14, 28,\n",
       "        15,  2, 33,  1,  6, 19, 22, 11,  8, 27, 10,  2, 30, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a random numpy array of shape (3, 4)\n",
    "random_array = np.random.randn(10000, 200).astype(np.float32)\n",
    "random_array.tofile('random_array.bin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(100 - 64, (32,))\n",
    "data = np.memmap('random_array.bin', dtype=np.float32, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37802866, -0.36042753,  0.3643469 ,  0.3643469 , -1.0327613 ,\n",
       "        0.8884671 , -1.2017971 ,  2.5359983 ,  0.3449528 , -0.88895214,\n",
       "        0.40260556,  1.2032892 , -0.36042753, -0.3175705 ,  1.4858595 ,\n",
       "        1.4858595 ,  2.5359983 , -0.89773315, -0.88895214,  0.13865697,\n",
       "       -0.89773315, -0.3175705 ,  0.3643469 ,  0.14209104, -0.26975426,\n",
       "        0.3643469 ,  1.4858595 ,  0.13865697, -0.89773315, -0.36042753,\n",
       "        0.5191463 , -1.0327613 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-10 11:25:42.070521: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-10 11:25:42.101455: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-10 11:25:42.101491: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-10 11:25:42.101497: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-10 11:25:42.107749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from jointformer.models.chemberta2 import RobertaForRegression\n",
    "from transformers import RobertaConfig, RobertaTokenizerFast, Trainer, TrainingArguments\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME_OR_PATH = \"DeepChem/ChemBERTa-5M-MLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME_OR_PATH\n",
    ")\n",
    "config.num_labels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForRegression were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MLM and are newly initialized: ['regression.dense.bias', 'regression.dense.weight', 'regression.out_proj.bias', 'regression.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForRegression.from_pretrained(\"DeepChem/ChemBERTa-5M-MLM\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(FLAGS.seed)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "    FLAGS.tokenizer_path, max_len=FLAGS.max_tokenizer_len, use_auth_token=True\n",
    ")\n",
    "\n",
    "finetune_datasets = get_finetune_datasets(dataset_name, tokenizer, is_molnet)\n",
    "\n",
    "if FLAGS.pretrained_model_name_or_path:\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        FLAGS.pretrained_model_name_or_path, use_auth_token=True\n",
    "    )\n",
    "else:\n",
    "    config = RobertaConfig(\n",
    "        vocab_size=FLAGS.vocab_size,\n",
    "        max_position_embeddings=FLAGS.max_position_embeddings,\n",
    "        num_attention_heads=FLAGS.num_attention_heads,\n",
    "        num_hidden_layers=FLAGS.num_hidden_layers,\n",
    "        type_vocab_size=FLAGS.type_vocab_size,\n",
    "        is_gpu=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "if dataset_type == \"classification\":\n",
    "    model_class = RobertaForSequenceClassification\n",
    "    config.num_labels = finetune_datasets.num_labels\n",
    "\n",
    "elif dataset_type == \"regression\":\n",
    "    model_class = RobertaForRegression\n",
    "    config.num_labels = 1\n",
    "    config.norm_mean = finetune_datasets.norm_mean\n",
    "    config.norm_std = finetune_datasets.norm_std\n",
    "\n",
    "state_dict = prune_state_dict(FLAGS.pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for finetuning and evaluating pre-trained ChemBERTa models on MoleculeNet tasks.\n",
    "\n",
    "[classification]\n",
    "python finetune.py --datasets=bbbp --pretrained_model_name_or_path=DeepChem/ChemBERTa-SM-015\n",
    "\n",
    "[regression]\n",
    "python finetune.py --datasets=delaney --pretrained_model_name_or_path=DeepChem/ChemBERTa-SM-015\n",
    "\n",
    "[csv]\n",
    "python finetune.py --datasets=$HOME/finetune_datasets/logd/ \\\n",
    "                --dataset_types=regression \\\n",
    "                --pretrained_model_name_or_path=DeepChem/ChemBERTa-SM-015 \\\n",
    "                --is_molnet=False\n",
    "\n",
    "[multiple]\n",
    "python finetune.py \\\n",
    "--datasets=bace_classification,bace_regression,bbbp,clearance,clintox,delaney,lipo,tox21 \\\n",
    "--pretrained_model_name_or_path=DeepChem/ChemBERTa-SM-015 \\\n",
    "--n_trials=20 \\\n",
    "--output_dir=finetuning_experiments \\\n",
    "--run_name=sm_015\n",
    "\n",
    "[from scratch (no pretraining)]\n",
    "python finetune.py --datasets=bbbp\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 18:19:51.096423: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-09 18:19:51.552885: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 18:19:51.552991: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 18:19:51.553010: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 18:19:51.744987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from glob import glob\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from absl import app, flags\n",
    "\n",
    "# from chemberta.utils.molnet_dataloader import get_dataset_info, load_molnet_dataset\n",
    "\n",
    "from jointformer.models.chemberta2 import RobertaForRegression\n",
    "\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    matthews_corrcoef,\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from transformers import RobertaConfig, RobertaTokenizerFast, Trainer, TrainingArguments\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Settings\n",
    "flags.DEFINE_string(name=\"output_dir\", default=\"default_dir\", help=\"\")\n",
    "flags.DEFINE_boolean(name=\"overwrite_output_dir\", default=True, help=\"\")\n",
    "flags.DEFINE_string(name=\"run_name\", default=\"default_run\", help=\"\")\n",
    "flags.DEFINE_integer(name=\"seed\", default=0, help=\"Global random seed.\")\n",
    "\n",
    "# Model params\n",
    "flags.DEFINE_string(\n",
    "    name=\"pretrained_model_name_or_path\",\n",
    "    default=None,\n",
    "    help=\"Arg to HuggingFace model.from_pretrained(). Can be either a path to a local model or a model ID on HuggingFace Model Hub. If not given, trains a fresh model from scratch (non-pretrained).\",\n",
    ")\n",
    "flags.DEFINE_boolean(\n",
    "    name=\"freeze_base_model\",\n",
    "    default=False,\n",
    "    help=\"If True, freezes the parameters of the base model during training. Only the classification/regression head parameters will be trained. (Only used when `pretrained_model_name_or_path` is given.)\",\n",
    ")\n",
    "flags.DEFINE_boolean(\n",
    "    name=\"is_molnet\",\n",
    "    default=True,\n",
    "    help=\"If true, assumes all dataset are MolNet datasets.\",\n",
    ")\n",
    "\n",
    "# RobertaConfig params (only for non-pretrained models)\n",
    "flags.DEFINE_integer(name=\"vocab_size\", default=600, help=\"\")\n",
    "flags.DEFINE_integer(name=\"max_position_embeddings\", default=515, help=\"\")\n",
    "flags.DEFINE_integer(name=\"num_attention_heads\", default=6, help=\"\")\n",
    "flags.DEFINE_integer(name=\"num_hidden_layers\", default=6, help=\"\")\n",
    "flags.DEFINE_integer(name=\"type_vocab_size\", default=1, help=\"\")\n",
    "\n",
    "# Train params\n",
    "flags.DEFINE_integer(name=\"logging_steps\", default=10, help=\"\")\n",
    "flags.DEFINE_integer(name=\"early_stopping_patience\", default=3, help=\"\")\n",
    "flags.DEFINE_integer(name=\"num_train_epochs_max\", default=10, help=\"\")\n",
    "flags.DEFINE_integer(name=\"per_device_train_batch_size\", default=64, help=\"\")\n",
    "flags.DEFINE_integer(name=\"per_device_eval_batch_size\", default=64, help=\"\")\n",
    "flags.DEFINE_integer(\n",
    "    name=\"n_trials\",\n",
    "    default=5,\n",
    "    help=\"Number of different hyperparameter combinations to try. Each combination will result in a different finetuned model.\",\n",
    ")\n",
    "flags.DEFINE_integer(\n",
    "    name=\"n_seeds\",\n",
    "    default=5,\n",
    "    help=\"Number of unique random seeds to try. This only applies to the final best model selected after hyperparameter tuning.\",\n",
    ")\n",
    "\n",
    "# Dataset params\n",
    "flags.DEFINE_list(\n",
    "    name=\"datasets\",\n",
    "    default=None,\n",
    "    help=\"Comma-separated list of MoleculeNet dataset names.\",\n",
    ")\n",
    "flags.DEFINE_string(\n",
    "    name=\"split\", default=\"scaffold\", help=\"DeepChem data loader split_type.\"\n",
    ")\n",
    "flags.DEFINE_list(\n",
    "    name=\"dataset_types\",\n",
    "    default=None,\n",
    "    help=\"List of dataset types (ex: classification,regression). Include 1 per dataset, not necessary for MoleculeNet datasets.\",\n",
    ")\n",
    "\n",
    "# Tokenizer params\n",
    "flags.DEFINE_string(\n",
    "    name=\"tokenizer_path\",\n",
    "    default=\"seyonec/SMILES_tokenized_PubChem_shard00_160k\",\n",
    "    help=\"\",\n",
    ")\n",
    "flags.DEFINE_integer(name=\"max_tokenizer_len\", default=512, help=\"\")\n",
    "\n",
    "flags.mark_flag_as_required(\"datasets\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(argv):\n",
    "    if FLAGS.pretrained_model_name_or_path is None:\n",
    "        print(\n",
    "            \"`WARNING: pretrained_model_name_or_path` is None - training a model from scratch.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Instantiating pretrained model from: {FLAGS.pretrained_model_name_or_path}\"\n",
    "        )\n",
    "\n",
    "    is_molnet = FLAGS.is_molnet\n",
    "\n",
    "    # Check that CSV dataset has the proper flags\n",
    "    if not is_molnet:\n",
    "        print(\"Assuming each dataset is a folder containing CSVs...\")\n",
    "        assert (\n",
    "            len(FLAGS.dataset_types) > 0\n",
    "        ), \"Please specify dataset types for csv datasets\"\n",
    "        for dataset_folder in FLAGS.datasets:\n",
    "            assert os.path.exists(os.path.join(dataset_folder, \"train.csv\"))\n",
    "            assert os.path.exists(os.path.join(dataset_folder, \"valid.csv\"))\n",
    "            assert os.path.exists(os.path.join(dataset_folder, \"test.csv\"))\n",
    "\n",
    "    for i in range(len(FLAGS.datasets)):\n",
    "        dataset_name_or_path = FLAGS.datasets[i]\n",
    "        dataset_name = get_dataset_name(dataset_name_or_path)\n",
    "        dataset_type = (\n",
    "            get_dataset_info(dataset_name)[\"dataset_type\"]\n",
    "            if is_molnet\n",
    "            else FLAGS.dataset_types[i]\n",
    "        )\n",
    "\n",
    "        run_dir = os.path.join(FLAGS.output_dir, FLAGS.run_name, dataset_name)\n",
    "\n",
    "        if os.path.exists(run_dir) and not FLAGS.overwrite_output_dir:\n",
    "            print(f\"Run dir already exists for dataset: {dataset_name}\")\n",
    "        else:\n",
    "            print(f\"Finetuning on {dataset_name}\")\n",
    "            finetune_single_dataset(\n",
    "                dataset_name_or_path, dataset_type, run_dir, is_molnet\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prune_state_dict(model_dir):\n",
    "    \"\"\"Remove problematic keys from state dictionary\"\"\"\n",
    "    if not (model_dir and os.path.exists(os.path.join(model_dir, \"pytorch_model.bin\"))):\n",
    "        return None\n",
    "\n",
    "    state_dict_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "    assert os.path.exists(\n",
    "        state_dict_path\n",
    "    ), f\"No `pytorch_model.bin` file found in {model_dir}\"\n",
    "    loaded_state_dict = torch.load(state_dict_path)\n",
    "    state_keys = loaded_state_dict.keys()\n",
    "    keys_to_remove = [\n",
    "        k for k in state_keys if k.startswith(\"regression\") or k.startswith(\"norm\")\n",
    "    ]\n",
    "\n",
    "    new_state_dict = OrderedDict({**loaded_state_dict})\n",
    "    for k in keys_to_remove:\n",
    "        del new_state_dict[k]\n",
    "    return new_state_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RobertaTokenizerFast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m PRETRAINED_MODEL_NAME_OR_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepChem/ChemBERTa-5M-MLM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m MAX_TOKENIZER_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaTokenizerFast\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m         TOKENIZER_PATH\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RobertaTokenizerFast' is not defined"
     ]
    }
   ],
   "source": [
    "TOKENIZER_PATH = \"DeepChem/SmilesTokenizer_PubChem_1M\"\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"DeepChem/ChemBERTa-5M-MLM\"\n",
    "MAX_TOKENIZER_LEN = 512\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "        TOKENIZER_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_datasets = get_finetune_datasets(dataset_name, tokenizer, is_molnet) TODO: check compatibility with new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME_OR_PATH\n",
    ")\n",
    "config.num_labels = 1\n",
    "config.norm_mean = finetune_datasets.norm_mean\n",
    "config.norm_std = finetune_datasets.norm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dataset_type == \"classification\":\n",
    "    model_class = RobertaForSequenceClassification\n",
    "elif dataset_type == \"regression\":\n",
    "    model_class = RobertaForRegression\n",
    "\n",
    "        if FLAGS.pretrained_model_name_or_path:\n",
    "            model = model_class.from_pretrained(\n",
    "                FLAGS.pretrained_model_name_or_path,\n",
    "                config=config,\n",
    "                state_dict=state_dict,\n",
    "                use_auth_token=True,\n",
    "            )\n",
    "            if FLAGS.freeze_base_model:\n",
    "                for name, param in model.base_model.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            model = model_class(config=config)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finetune_single_dataset(dataset_name, dataset_type, run_dir, is_molnet):\n",
    "    torch.manual_seed(FLAGS.seed)\n",
    "\n",
    "    if dataset_type == \"classification\":\n",
    "        model_class = RobertaForSequenceClassification\n",
    "        config.num_labels = finetune_datasets.num_labels\n",
    "\n",
    "    elif dataset_type == \"regression\":\n",
    "        model_class = RobertaForRegression\n",
    "        config.num_labels = 1\n",
    "        config.norm_mean = finetune_datasets.norm_mean\n",
    "        config.norm_std = finetune_datasets.norm_std\n",
    "\n",
    "    state_dict = prune_state_dict(FLAGS.pretrained_model_name_or_path)\n",
    "\n",
    "    def model_init():\n",
    "        if dataset_type == \"classification\":\n",
    "            model_class = RobertaForSequenceClassification\n",
    "        elif dataset_type == \"regression\":\n",
    "            model_class = RobertaForRegression\n",
    "\n",
    "        if FLAGS.pretrained_model_name_or_path:\n",
    "            model = model_class.from_pretrained(\n",
    "                FLAGS.pretrained_model_name_or_path,\n",
    "                config=config,\n",
    "                state_dict=state_dict,\n",
    "                use_auth_token=True,\n",
    "            )\n",
    "            if FLAGS.freeze_base_model:\n",
    "                for name, param in model.base_model.named_parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            model = model_class(config=config)\n",
    "\n",
    "        return model\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        output_dir=run_dir,\n",
    "        overwrite_output_dir=FLAGS.overwrite_output_dir,\n",
    "        per_device_eval_batch_size=FLAGS.per_device_eval_batch_size,\n",
    "        logging_steps=FLAGS.logging_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=None,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=finetune_datasets.train_dataset,\n",
    "        eval_dataset=finetune_datasets.valid_dataset,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=FLAGS.early_stopping_patience)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    def custom_hp_space_optuna(trial):\n",
    "        return {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "            \"num_train_epochs\": trial.suggest_int(\n",
    "                \"num_train_epochs\", 1, FLAGS.num_train_epochs_max\n",
    "            ),\n",
    "            \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "            \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "                \"per_device_train_batch_size\", [FLAGS.per_device_train_batch_size]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    best_trial = trainer.hyperparameter_search(\n",
    "        backend=\"optuna\",\n",
    "        direction=\"minimize\",\n",
    "        hp_space=custom_hp_space_optuna,\n",
    "        n_trials=FLAGS.n_trials,\n",
    "    )\n",
    "\n",
    "    # Set parameters to the best ones from the hp search\n",
    "    for n, v in best_trial.hyperparameters.items():\n",
    "        setattr(trainer.args, n, v)\n",
    "\n",
    "    dir_valid = os.path.join(run_dir, \"results\", \"valid\")\n",
    "    dir_test = os.path.join(run_dir, \"results\", \"test\")\n",
    "    os.makedirs(dir_valid, exist_ok=True)\n",
    "    os.makedirs(dir_test, exist_ok=True)\n",
    "\n",
    "    metrics_valid = {}\n",
    "    metrics_test = {}\n",
    "\n",
    "    # Run with several seeds so we can see std\n",
    "    for random_seed in range(FLAGS.n_seeds):\n",
    "        setattr(trainer.args, \"seed\", random_seed)\n",
    "        trainer.train()\n",
    "        metrics_valid[f\"seed_{random_seed}\"] = eval_model(\n",
    "            trainer,\n",
    "            finetune_datasets.valid_dataset_unlabeled,\n",
    "            dataset_name,\n",
    "            dataset_type,\n",
    "            dir_valid,\n",
    "            random_seed,\n",
    "        )\n",
    "        metrics_test[f\"seed_{random_seed}\"] = eval_model(\n",
    "            trainer,\n",
    "            finetune_datasets.test_dataset,\n",
    "            dataset_name,\n",
    "            dataset_type,\n",
    "            dir_test,\n",
    "            random_seed,\n",
    "        )\n",
    "\n",
    "    with open(os.path.join(dir_valid, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_valid, f)\n",
    "    with open(os.path.join(dir_test, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_test, f)\n",
    "\n",
    "    # Delete checkpoints from hyperparameter search since they use a lot of disk\n",
    "    for d in glob(os.path.join(run_dir, \"run-*\")):\n",
    "        shutil.rmtree(d, ignore_errors=True)\n",
    "\n",
    "\n",
    "def eval_model(trainer, dataset, dataset_name, dataset_type, output_dir, random_seed):\n",
    "    labels = dataset.labels\n",
    "    predictions = trainer.predict(dataset)\n",
    "    fig = plt.figure(dpi=144)\n",
    "\n",
    "    if dataset_type == \"classification\":\n",
    "        if len(np.unique(labels)) <= 2:\n",
    "            y_pred = softmax(predictions.predictions, axis=1)[:, 1]\n",
    "            metrics = {\n",
    "                \"roc_auc_score\": roc_auc_score(y_true=labels, y_score=y_pred),\n",
    "                \"average_precision_score\": average_precision_score(\n",
    "                    y_true=labels, y_score=y_pred\n",
    "                ),\n",
    "            }\n",
    "            sns.histplot(x=y_pred, hue=labels)\n",
    "        else:\n",
    "            y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "            metrics = {\"mcc\": matthews_corrcoef(labels, y_pred)}\n",
    "\n",
    "    elif dataset_type == \"regression\":\n",
    "        y_pred = predictions.predictions.flatten()\n",
    "        metrics = {\n",
    "            \"pearsonr\": pearsonr(y_pred, labels),\n",
    "            \"rmse\": mean_squared_error(y_true=labels, y_pred=y_pred, squared=False),\n",
    "        }\n",
    "        sns.regplot(x=y_pred, y=labels)\n",
    "        plt.xlabel(\"ChemBERTa predictions\")\n",
    "        plt.ylabel(\"Ground truth\")\n",
    "    else:\n",
    "        raise ValueError(dataset_type)\n",
    "\n",
    "    plt.title(f\"{dataset_name} {dataset_type} results\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"results_seed_{random_seed}.png\"))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_finetune_datasets(dataset_name, tokenizer, is_molnet):\n",
    "    if is_molnet:\n",
    "        tasks, (train_df, valid_df, test_df), _ = load_molnet_dataset(\n",
    "            dataset_name, split=FLAGS.split, df_format=\"chemprop\"\n",
    "        )\n",
    "        assert len(tasks) == 1\n",
    "    else:\n",
    "        train_df = pd.read_csv(os.path.join(dataset_name, \"train.csv\"))\n",
    "        valid_df = pd.read_csv(os.path.join(dataset_name, \"valid.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(dataset_name, \"test.csv\"))\n",
    "\n",
    "    train_dataset = FinetuneDataset(train_df, tokenizer)\n",
    "    valid_dataset = FinetuneDataset(valid_df, tokenizer)\n",
    "    valid_dataset_unlabeled = FinetuneDataset(valid_df, tokenizer, include_labels=False)\n",
    "    test_dataset = FinetuneDataset(test_df, tokenizer, include_labels=False)\n",
    "\n",
    "    num_labels = len(np.unique(train_dataset.labels))\n",
    "    norm_mean = [np.mean(np.array(train_dataset.labels), axis=0)]\n",
    "    norm_std = [np.std(np.array(train_dataset.labels), axis=0)]\n",
    "\n",
    "    return FinetuneDatasets(\n",
    "        train_dataset,\n",
    "        valid_dataset,\n",
    "        valid_dataset_unlabeled,\n",
    "        test_dataset,\n",
    "        num_labels,\n",
    "        norm_mean,\n",
    "        norm_std,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset_name(dataset_name_or_path):\n",
    "    return os.path.splitext(os.path.basename(dataset_name_or_path))[0]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FinetuneDatasets:\n",
    "    train_dataset: str\n",
    "    valid_dataset: torch.utils.data.Dataset\n",
    "    valid_dataset_unlabeled: torch.utils.data.Dataset\n",
    "    test_dataset: torch.utils.data.Dataset\n",
    "    num_labels: int\n",
    "    norm_mean: List[float]\n",
    "    norm_std: List[float]\n",
    "\n",
    "\n",
    "class FinetuneDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, include_labels=True):\n",
    "\n",
    "        self.encodings = tokenizer(df[\"smiles\"].tolist(), truncation=True, padding=True)\n",
    "        self.labels = df.iloc[:, 1].values\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.include_labels and self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/seyonechithrananda/bert-loves-chemistry/blob/master/chemberta/finetune/finetune.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointformer-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
