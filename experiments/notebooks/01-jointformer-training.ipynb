{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointformer Training\n",
    "\n",
    "This notebook shows how to train Jointformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "2024-10-29 16:09:01.347627: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-29 16:09:01.375349: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-29 16:09:01.375374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-29 16:09:01.375378: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-29 16:09:01.380846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory of the project\n",
    "\n",
    "REPOSITORY_DIR = '/home/adamizdebski/projects/jointformer'\n",
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "DATA_DIR = '/home/adamizdebski/files/data'\n",
    "OUTPUT_DIR = '/home/adamizdebski/files/jointformer/results/finetune'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = '/home/adamizdebski/projects/jointformer/configs/datasets/molecule_net/scaffold/lipo'\n",
    "PATH_TO_TOKENIZER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/tokenizers/gpt_tokenizer'\n",
    "PATH_TO_MODEL_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/fancy_model_prediction'\n",
    "PATH_TO_TRAINER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/trainers/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Datsaset\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='val')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.55M\n"
     ]
    }
   ],
   "source": [
    "# Init Jointformer\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config, downstream_task=dataset_config.task_type, num_tasks=dataset_config.num_tasks, hidden_dim=256)\n",
    "# model.load_pretrained('ckpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FancyModelForDownstreamPrediction(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(593, 256)\n",
       "    (type_emb): Embedding(2, 256)\n",
       "    (wpe): Embedding(128, 256)\n",
       "    (prop_emb): Linear(in_features=200, out_features=256, bias=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=593, bias=False)\n",
       "  (property_head): Linear(in_features=256, out_features=200, bias=False)\n",
       "  (downstream_prediction_task_head): DownstreamPredictionHead(\n",
       "    (mlp): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (last_layer): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 40, with 6,644,737 parameters\n",
      "num non-decayed parameter tensors: 19, with 4,864 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer_config = TrainerConfig.from_config_file(PATH_TO_TRAINER_CONFIG)\n",
    "trainer = Trainer(\n",
    "    config=trainer_config,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_480/4032920361.py\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/adamizdebski/projects/jointformer/jointformer/trainers/trainer.py\", line 447, in train\n",
      "    self.evaluate()\n",
      "  File \"/home/adamizdebski/projects/jointformer/jointformer/trainers/trainer.py\", line 371, in evaluate\n",
      "    losses = self.estimate_loss()\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/adamizdebski/projects/jointformer/jointformer/trainers/trainer.py\", line 316, in estimate_loss\n",
      "    outputs = self.model.get_loss(**inputs)\n",
      "  File \"/home/adamizdebski/projects/jointformer/jointformer/models/fancy_model.py\", line 324, in get_loss\n",
      "    outputs[\"loss\"] = F.mse_loss(outputs[\"logits_prediction\"].flatten(), properties.flatten(), 'mean')\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/functional.py\", line 3338, in mse_loss\n",
      "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/functional.py\", line 76, in broadcast_tensors\n",
      "    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]\n",
      "RuntimeError: The size of tensor a (252) must match the size of tensor b (2) at non-singleton dimension 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/adamizdebski/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1 and 200x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/jointformer/jointformer/models/fancy_model.py:304\u001b[0m, in \u001b[0;36mFancyModelForDownstreamPrediction.forward\u001b[0;34m(self, input_ids, input_labels, attention_mask, properties, next_token_only, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor, input_labels: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    303\u001b[0m             properties: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, next_token_only: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 304\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_token_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m next_token_only:\n\u001b[1;32m    307\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/projects/jointformer/jointformer/models/fancy_model.py:83\u001b[0m, in \u001b[0;36mFancyModel.forward\u001b[0;34m(self, input_ids, input_labels, attention_mask, properties, next_token_only, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Embedding of the property tokens\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m properties \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     prop_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprop_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# property embeddings of shape (b, 1, n_embd)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     type_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     85\u001b[0m     type_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mtype_emb(type_tokens)\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jointformer-experiments/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1 and 200x256)"
     ]
    }
   ],
   "source": [
    "trainer._init_data_loaders()\n",
    "\n",
    "batch = next(iter(trainer.train_loader))\n",
    "batch.to('cuda')\n",
    "model.to('cuda')\n",
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits_prediction'].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6961e-01,  2.6618e-01, -2.2679e-01, -6.1008e-01,  9.9106e-03,\n",
       "          4.1189e-01,  1.4348e-01,  1.1176e-01,  6.5966e-01, -1.7233e+00,\n",
       "          1.2009e+00, -3.4084e-01,  1.7629e+00, -1.9667e-01, -1.2688e+00,\n",
       "          9.9074e-02, -8.8256e-01,  3.0045e-01,  1.8028e-01,  3.0680e-01,\n",
       "         -5.4947e-01,  1.6914e+00,  3.4691e-01, -6.9734e-01, -1.9162e+00,\n",
       "         -9.3154e-01, -9.1630e-01, -8.9729e-01, -1.7263e+00, -1.6298e+00,\n",
       "          7.5439e-01, -5.4682e-02, -6.0041e-01, -1.3833e+00,  8.2129e-01,\n",
       "         -8.0695e-01, -9.0263e-02,  4.9857e-01,  2.0863e+00, -7.5034e-01,\n",
       "         -7.6549e-01, -7.6526e-01, -9.7997e-01,  3.2173e-01,  6.4486e-01,\n",
       "          1.2544e+00,  7.0545e-01,  6.0114e-01,  6.6038e-01, -4.0036e-01,\n",
       "          1.3329e+00, -6.0918e-01,  1.2346e+00, -1.1820e+00, -1.5737e-01,\n",
       "         -3.0718e-01,  5.4611e-01,  8.7681e-01,  1.4073e+00, -1.7978e+00,\n",
       "         -5.6693e-01,  1.0519e+00,  1.4003e+00,  1.6440e+00, -2.4737e-01,\n",
       "         -8.5858e-02, -9.4875e-01, -7.1532e-01,  5.9511e-01,  1.5091e+00,\n",
       "         -2.7882e-01, -2.0022e-01, -8.7815e-01,  4.8187e-01,  1.8483e+00,\n",
       "          3.3924e-01,  1.9345e-01,  3.2948e-01, -8.3187e-02,  1.6214e+00,\n",
       "         -7.5849e-01,  1.1151e-01, -6.8873e-01,  1.8868e+00, -1.9811e+00,\n",
       "         -1.9283e-01, -3.6961e-01, -4.4678e-01,  1.0233e+00, -9.2262e-01,\n",
       "         -7.6125e-01, -4.0042e-01,  8.5241e-01,  8.2948e-01, -6.4210e-02,\n",
       "         -2.2286e+00, -1.5574e+00, -2.9327e-01,  1.4921e+00,  1.0524e+00,\n",
       "          5.7672e-01,  7.4651e-01,  4.8086e-01, -2.3122e-01, -4.9042e-01,\n",
       "         -1.1288e+00, -8.8902e-01,  1.0592e-01, -7.5122e-01,  1.1453e+00,\n",
       "          3.9141e-01, -8.6239e-01,  4.0596e-02, -4.2072e-01, -1.5670e-03,\n",
       "          2.1636e+00,  1.5753e+00, -2.1838e-01,  9.2997e-01, -8.5951e-01,\n",
       "          1.3528e-01, -1.2044e-02, -5.3883e-01, -1.6413e+00,  6.9285e-01,\n",
       "         -1.6473e+00, -1.8792e-01, -1.5075e+00,  5.6241e-01, -2.4920e-01,\n",
       "         -1.0753e+00, -1.3486e+00,  5.5584e-02, -1.1072e-01,  9.5019e-01,\n",
       "         -2.3364e-02, -2.0120e+00, -1.4580e+00, -1.5661e+00,  9.4045e-01,\n",
       "          1.6692e+00, -1.9371e+00, -8.6362e-01, -1.0469e+00,  1.8201e+00,\n",
       "         -1.8387e+00, -5.7779e-01,  1.2347e-01,  4.9526e-01,  6.5618e-01,\n",
       "          4.9842e-02,  9.3556e-01,  5.2837e-01,  9.7242e-02, -8.6780e-01,\n",
       "         -2.4289e-01,  1.6635e-01,  1.1206e+00, -8.9200e-01, -1.7397e+00,\n",
       "          1.2942e+00, -6.8575e-01,  1.1628e+00,  1.0362e-02,  8.3371e-01,\n",
       "         -1.9109e-01,  2.0800e+00,  8.2401e-01, -1.2460e-01, -1.1006e+00,\n",
       "         -1.0253e+00, -3.1990e-01, -6.7079e-01, -1.1088e+00,  5.5913e-01,\n",
       "         -7.2260e-01, -6.4817e-01, -4.2056e-01,  1.4712e+00,  8.0476e-01,\n",
       "         -6.8945e-01,  1.8373e-01, -1.3976e+00,  2.6193e-01, -4.7559e-02,\n",
       "          1.6143e+00,  6.5986e-01,  2.9926e-01,  3.8756e-01,  2.0140e+00,\n",
       "         -1.1807e+00, -5.9320e-01,  3.3890e-01, -1.0986e+00,  8.7229e-01,\n",
       "          9.2782e-01, -2.5893e-01, -8.4114e-01,  7.5700e-01,  1.3971e+00,\n",
       "         -4.8779e-01,  2.8584e-01,  6.6315e-02, -2.1624e+00, -1.6504e+00,\n",
       "          7.9347e-01, -9.1636e-02,  7.8936e-01, -1.1161e+00, -2.7809e-01,\n",
       "         -7.9495e-01, -8.1055e-01,  3.0554e-01,  7.2004e-01,  8.7592e-01,\n",
       "          7.9998e-01, -1.5318e+00,  1.6184e+00,  1.9316e+00,  5.5291e-02,\n",
       "          8.7566e-01,  7.3667e-01, -8.6258e-01,  2.4390e-01,  1.5200e+00,\n",
       "         -4.1696e-01, -7.0663e-01,  1.6263e+00,  1.3515e+00, -1.6831e+00,\n",
       "         -8.6381e-01,  2.2643e+00, -1.1105e+00, -2.9315e-02, -8.7185e-01,\n",
       "          1.8066e+00,  5.0010e-01, -6.9819e-01,  1.1432e+00, -6.5210e-01,\n",
       "         -1.3489e+00, -1.3729e+00, -1.1739e+00,  2.1844e-02,  1.7236e+00,\n",
       "          4.7729e-01, -1.8549e-01,  1.0844e+00,  7.8512e-01, -9.2822e-01,\n",
       "         -2.7267e-02, -3.0872e-01,  8.2228e-01,  1.4874e+00,  1.1835e+00,\n",
       "         -1.1130e+00],\n",
       "        [ 1.2029e+00,  2.0487e+00,  1.2698e-02,  4.3152e-02, -4.6542e-02,\n",
       "          1.3501e+00,  1.2716e+00, -1.3843e+00, -1.2906e+00,  4.7507e-01,\n",
       "         -2.4591e-01,  3.5477e-02, -7.4912e-01,  1.1685e+00,  7.7461e-01,\n",
       "          1.1246e-01,  1.2747e+00, -5.1540e-01, -1.4827e+00,  1.2399e+00,\n",
       "         -3.4552e-02, -5.0157e-01,  1.7583e+00,  1.5234e+00,  7.5158e-01,\n",
       "          7.7103e-01,  1.5707e+00,  1.8000e-01,  5.6559e-01,  7.1176e-01,\n",
       "         -1.4770e+00, -2.9939e-01,  6.5293e-01, -4.7625e-01, -4.1386e-01,\n",
       "         -1.2725e+00, -4.0207e-01,  4.1055e-01, -5.9938e-01,  3.6367e-01,\n",
       "         -3.1983e-01,  6.9737e-01,  1.3135e-02, -6.8749e-01, -1.6509e+00,\n",
       "         -1.1517e+00, -6.1424e-01,  5.5852e-01, -1.3761e+00,  3.6999e-01,\n",
       "          9.3938e-03, -1.8034e-01,  5.4883e-01,  6.1219e-01,  1.3996e+00,\n",
       "         -5.6526e-01, -1.8393e+00, -1.3701e+00, -8.5844e-01,  1.1070e+00,\n",
       "         -1.0589e+00, -9.4248e-01, -4.9050e-01,  3.0436e-01,  1.0151e+00,\n",
       "         -1.7981e+00, -5.9396e-01, -6.6015e-01, -1.8860e+00, -5.0961e-01,\n",
       "          1.2883e+00, -1.5027e+00,  1.6678e+00, -3.3603e-02, -9.6620e-01,\n",
       "         -1.8384e+00, -1.8177e+00, -2.0838e+00, -6.3465e-01, -4.7781e-01,\n",
       "          9.4728e-01, -1.9331e+00,  3.5850e-01, -8.8470e-01,  1.7196e-01,\n",
       "         -1.4980e+00, -1.5738e+00,  6.1581e-01, -2.7641e-02,  1.0247e+00,\n",
       "          7.9604e-01, -1.4686e+00,  7.8074e-01, -1.0652e+00, -1.5283e+00,\n",
       "          6.6759e-01, -4.7219e-01, -1.7025e+00, -2.1838e-01,  6.5026e-01,\n",
       "         -1.5352e+00,  1.5514e+00, -1.0488e-01, -1.1393e+00,  6.5397e-01,\n",
       "          1.1840e-01,  6.4406e-01,  4.0191e-01,  1.9571e+00, -6.8227e-02,\n",
       "         -6.6976e-01,  1.1300e+00, -8.8086e-01,  2.0859e+00,  2.2061e-01,\n",
       "         -4.2536e-01, -4.7244e-01,  1.5148e+00,  1.2015e-01,  8.3855e-01,\n",
       "          1.5974e+00, -2.6781e-01,  1.6852e+00,  2.2316e-01, -3.5782e-01,\n",
       "          1.2324e+00, -1.5855e+00,  2.8504e-01, -1.3393e+00, -1.9083e+00,\n",
       "         -2.0904e-02,  7.0911e-01, -7.5963e-01, -1.2999e+00, -8.5451e-01,\n",
       "         -7.7167e-01,  5.3957e-01,  1.4176e+00,  9.3505e-01,  3.9580e-01,\n",
       "         -5.0966e-01,  9.1460e-01, -3.6769e-01,  8.6280e-01, -7.2657e-01,\n",
       "          3.7809e-01, -1.8482e+00,  1.6212e+00, -4.0534e-01,  2.6704e-01,\n",
       "          1.8088e+00,  1.3853e-01, -9.0407e-02,  7.5503e-01, -8.9486e-01,\n",
       "          1.3870e-01,  6.0452e-01, -5.4482e-01,  3.4225e-01,  8.3655e-01,\n",
       "         -9.4035e-01,  8.8043e-01, -5.0925e-01,  1.2554e+00, -1.1844e+00,\n",
       "          1.2592e+00, -4.8301e-01, -1.4153e-01,  1.9050e+00, -5.7563e-01,\n",
       "         -8.3880e-02, -1.0347e+00,  1.4742e-01,  8.8625e-01,  7.5324e-01,\n",
       "          3.0119e-01,  7.6208e-01,  1.1801e+00, -4.9116e-01,  6.3320e-01,\n",
       "         -7.1702e-01, -3.1900e-01, -1.6131e-01,  1.2137e+00,  1.3784e+00,\n",
       "         -3.7781e-01,  2.6854e-02, -1.9930e+00, -1.3287e+00, -2.9656e-01,\n",
       "          1.5054e+00, -6.4590e-01, -1.1873e-01,  1.3563e+00, -6.2952e-01,\n",
       "         -9.7451e-01, -1.0401e+00,  1.0307e-03, -1.4934e+00, -9.9336e-02,\n",
       "         -2.1463e-02, -1.2282e+00, -1.3371e+00,  4.8506e-01, -2.7999e-01,\n",
       "         -1.1034e+00, -7.7361e-01,  1.1536e+00, -6.6753e-01,  1.0920e+00,\n",
       "          1.6589e+00,  1.2145e+00, -8.2989e-01, -6.8199e-01, -1.5577e+00,\n",
       "         -5.3943e-01,  1.2285e+00, -1.1615e+00, -7.8035e-01,  6.6957e-01,\n",
       "          2.1408e-01,  1.0942e+00,  1.4575e+00, -4.9961e-01, -3.4702e-01,\n",
       "         -9.3995e-01, -5.9241e-01, -1.4284e-01, -4.6615e-01,  2.2844e-01,\n",
       "          8.4963e-01, -5.9691e-01,  1.4911e+00,  1.7596e+00,  1.0077e+00,\n",
       "         -8.5038e-01, -9.8277e-01, -5.3085e-01, -2.6165e-01,  1.1528e+00,\n",
       "          1.4135e-01,  1.1438e+00,  8.3979e-01,  2.4074e+00,  1.4644e-01,\n",
       "         -1.1926e+00,  6.8629e-01, -8.2474e-02,  1.1577e+00,  1.6409e+00,\n",
       "          2.1274e-01, -6.6154e-01,  3.6727e-01, -4.2316e-01, -7.8753e-01,\n",
       "          1.5348e+00]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['embeddings'][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1041, -1.7448], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"properties\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(16, 200)\n",
    "layer = torch.nn.Linear(1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = layer(x.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 200, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaffold'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'splitter'\n",
    "\n",
    "dataset_config[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_to_train_data None\n",
      "True\n",
      "path_to_train_properties None\n",
      "True\n",
      "path_to_val_data None\n",
      "True\n",
      "path_to_val_properties None\n",
      "True\n",
      "path_to_test_data None\n",
      "True\n",
      "path_to_test_properties None\n",
      "True\n",
      "dataset_name molecule_net\n",
      "True\n",
      "target_label lipo\n",
      "True\n",
      "data_filepath None\n",
      "True\n",
      "tatget_filepath None\n",
      "True\n",
      "validate False\n",
      "True\n",
      "standardize False\n",
      "True\n",
      "num_samples None\n",
      "True\n",
      "split val\n",
      "True\n",
      "splitter scaffold\n",
      "True\n",
      "transform None\n",
      "True\n",
      "target_transform None\n",
      "True\n",
      "task_type regression\n",
      "True\n",
      "metric rmse\n",
      "True\n",
      "num_tasks 1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for key, value in dataset_config.__dict__.items():\n",
    "    print(key, value)\n",
    "    print(isinstance(key, str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FancyModelForDownstreamPrediction'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config[\"model_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"model_name\" in model_config.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointformer-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
