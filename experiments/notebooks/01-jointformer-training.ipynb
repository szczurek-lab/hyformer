{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jointformer Training\n",
    "\n",
    "This notebook shows how to train Jointformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory of the project\n",
    "\n",
    "REPOSITORY_DIR = '/home/adamizdebski/projects/jointformer'\n",
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "DATA_DIR = '/home/adamizdebski/files/data'\n",
    "OUTPUT_DIR = '/home/adamizdebski/files/jointformer/results/finetune'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = '/home/adamizdebski/projects/jointformer/configs/datasets/molecule_net/scaffold/lipo'\n",
    "PATH_TO_TOKENIZER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/tokenizers/gpt_tokenizer'\n",
    "PATH_TO_MODEL_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/joint_gpt_prediction'\n",
    "PATH_TO_TRAINER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/trainers/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Datsaset\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='val')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.45M\n"
     ]
    }
   ],
   "source": [
    "# Init Jointformer\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config, downstream_task=dataset_config.task_type, num_tasks=dataset_config.num_tasks, hidden_dim=256)\n",
    "# model.load_pretrained('ckpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointGPTForDownstreamPrediction(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(593, 256)\n",
       "    (wpe): Embedding(128, 256)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=256, out_features=768, bias=False)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=593, bias=False)\n",
       "  (downstream_prediction_task_head): DownstreamPredictionHead(\n",
       "    (mlp): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (last_layer): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer_config = TrainerConfig.from_config_file(PATH_TO_TRAINER_CONFIG)\n",
    "trainer = Trainer(\n",
    "    config=trainer_config,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._init_data_loaders()\n",
    "\n",
    "batch = next(iter(trainer.train_loader))\n",
    "batch.to('cuda')\n",
    "model.to('cuda')\n",
    "outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_embeddings': tensor([[[-0.0523, -0.0435, -0.0128,  ..., -0.0171, -0.0043,  0.0739],\n",
       "          [ 0.0247, -0.0027,  0.0099,  ..., -0.0462,  0.0014, -0.0134],\n",
       "          [ 0.0404,  0.0254, -0.0292,  ...,  0.0226,  0.0302,  0.0122],\n",
       "          ...,\n",
       "          [ 0.0059, -0.0195,  0.0278,  ..., -0.0457, -0.0063, -0.0160],\n",
       "          [ 0.0143,  0.0149,  0.0547,  ..., -0.0321, -0.0021,  0.0379],\n",
       "          [ 0.0040, -0.0210,  0.0362,  ..., -0.0282,  0.0392,  0.0006]],\n",
       " \n",
       "         [[-0.0523, -0.0435, -0.0128,  ..., -0.0171, -0.0043,  0.0739],\n",
       "          [ 0.0419,  0.0254,  0.0354,  ..., -0.0138,  0.0017,  0.0224],\n",
       "          [ 0.0251,  0.0234, -0.0165,  ..., -0.0135,  0.0274, -0.0064],\n",
       "          ...,\n",
       "          [ 0.0059, -0.0195,  0.0278,  ..., -0.0457, -0.0063, -0.0160],\n",
       "          [ 0.0143,  0.0149,  0.0547,  ..., -0.0321, -0.0021,  0.0379],\n",
       "          [ 0.0040, -0.0210,  0.0362,  ..., -0.0282,  0.0392,  0.0006]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'embeddings': tensor([[[ 0.8558, -1.2975,  0.0984,  ..., -0.7168,  1.0190,  2.1529],\n",
       "          [ 1.3939, -0.3052, -0.1493,  ..., -0.4674,  0.2266, -0.7072],\n",
       "          [ 0.3334, -0.1243, -0.2324,  ...,  0.5195, -0.6360,  0.7859],\n",
       "          ...,\n",
       "          [ 0.6399,  0.5602,  2.3830,  ...,  0.9579,  1.1553, -1.5289],\n",
       "          [ 0.1585,  1.0242, -0.3480,  ...,  0.5608,  0.6963,  0.4210],\n",
       "          [-0.9652,  1.0408,  0.5340,  ...,  0.8708,  1.0534, -0.6926]],\n",
       " \n",
       "         [[ 1.2864, -0.5005, -0.3110,  ...,  0.1252,  0.5834,  1.7644],\n",
       "          [ 1.6273, -0.7639,  0.4941,  ...,  0.4840,  0.3234,  1.1641],\n",
       "          [ 0.6154, -1.8919, -0.0352,  ..., -0.9556,  0.8479,  1.4458],\n",
       "          ...,\n",
       "          [-0.6621,  0.7942,  1.9641,  ...,  1.3431,  1.6208, -1.2544],\n",
       "          [ 0.4114,  1.4765,  0.3699,  ...,  0.5236,  0.2547,  0.1218],\n",
       "          [-1.4796,  1.2021,  1.0562,  ...,  1.1896,  0.8947,  0.0102]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " 'logits': tensor([[[-0.0491,  0.2310,  0.5288,  ..., -0.3160,  0.1049,  0.7305],\n",
       "          [ 0.0715,  0.0200,  0.1033,  ...,  0.2580,  0.2004, -0.3123],\n",
       "          [ 0.3338,  0.2086,  0.8295,  ...,  0.1032,  0.2270,  0.0475],\n",
       "          ...,\n",
       "          [ 1.4461, -0.3730,  0.3009,  ..., -0.0853,  0.6504,  0.0146],\n",
       "          [ 1.4650, -0.5073,  0.6628,  ...,  0.0527,  0.7107,  0.1306],\n",
       "          [ 1.1848, -0.0200,  1.2887,  ...,  0.2943,  0.6957,  0.2126]],\n",
       " \n",
       "         [[ 0.5691, -0.2069,  0.6940,  ..., -0.0567,  0.1826,  0.8871],\n",
       "          [ 0.4447,  0.2300,  0.5450,  ...,  0.0272,  0.3835,  0.1752],\n",
       "          [ 0.1821,  0.0827,  0.4096,  ..., -0.0352,  0.2274,  0.4877],\n",
       "          ...,\n",
       "          [ 1.2959, -0.5850,  0.3423,  ...,  0.1735,  0.6640, -0.0739],\n",
       "          [ 1.7328, -0.3177,  0.8592,  ...,  0.4384,  1.0076,  0.1961],\n",
       "          [ 1.4587, -0.4050,  0.8792,  ...,  0.0775,  0.5470, -0.0971]]],\n",
       "        device='cuda:0', grad_fn=<CloneBackward0>),\n",
       " 'loss': tensor(6.6305, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'logits_prediction': tensor([[-0.0116],\n",
       "         [-0.0301]], device='cuda:0', grad_fn=<MmBackward0>),\n",
       " 'predictive_loss': tensor(0.6614, device='cuda:0', grad_fn=<MseLossBackward0>)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_loss(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointformer-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
