{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam/Projects/jointformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "%cd .."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T08:02:22.137178599Z",
     "start_time": "2024-04-23T08:02:22.119310415Z"
    }
   },
   "id": "9ef080d1b275eede"
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from jointformer.utils.datasets.smiles.base import SmilesDataset\n",
    "from jointformer.utils.datasets.smiles.guacamol import GuacamolSmilesDataset\n",
    "from jointformer.utils.transforms.smiles.enumerate import SmilesEnumerator\n",
    "from jointformer.utils.tokenizers.smiles.smiles import SmilesTokenizer\n",
    "from jointformer.models.base import Transformer\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:04:02.504215079Z",
     "start_time": "2024-04-23T12:04:02.388530770Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.04M\n"
     ]
    }
   ],
   "source": [
    "# Data // follows PyTorch API :) \n",
    "\n",
    "PATH_TO_DATA = 'data/guacamol/test/smiles.txt'\n",
    "PATH_TO_VOCAB = 'data/vocabularies/deepchem.txt'\n",
    "NUM_SAMPLES = 1000\n",
    "MAX_LENGTH = 128\n",
    "VALIDATE = False\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    SmilesEnumerator(),\n",
    "])\n",
    "\n",
    "dataset = GuacamolSmilesDataset(split='all')\n",
    "tokenizer = SmilesTokenizer(path_to_vocabulary=PATH_TO_VOCAB, max_molecule_length=MAX_LENGTH)\n",
    "model = Transformer(\n",
    "    vocab_size=len(tokenizer), max_seq_len=tokenizer.max_molecule_length, embedding_dim=32, dropout=0.2,\n",
    "    num_layers=2, bias=False, num_heads=8\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:21:04.577893155Z",
     "start_time": "2024-04-23T12:21:04.381133172Z"
    }
   },
   "id": "f6e8abf086b09b6d"
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Output keys: dict_keys(['embeddings', 'attention_probabilities'])\n"
     ]
    }
   ],
   "source": [
    "bs = 2\n",
    "idx = random.sample([idx for idx in range(len(dataset))], bs)\n",
    "smiles = [dataset[i] for i in idx] # custom DL :')\n",
    "inputs = tokenizer(smiles, task='lm') # 'lm', 'mlm', 'predict', 'ae'\n",
    "print(f\"Input keys: {inputs.keys()}\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, task='lm')\n",
    "    print(f\"Output keys: {outputs.keys()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:21:05.341796885Z",
     "start_time": "2024-04-23T12:21:05.263154251Z"
    }
   },
   "id": "ae93ffe3257d325a"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "# Task 1, GuacaMol and general properties like QED\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:21:06.011853537Z",
     "start_time": "2024-04-23T12:21:05.985991281Z"
    }
   },
   "id": "5536f1caf20b292c"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "{'(', ')', '1', '=', 'C', 'N', 'O', 'c'}"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:21:06.689143237Z",
     "start_time": "2024-04-23T12:21:06.659920689Z"
    }
   },
   "id": "75019c10e3e932e9"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc(O)cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(O)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "Cc1cccc(CP(Cc2cccc(C)c2)CC2(Cp3c4ccccc4c4ccccc43)COC2)c1\n",
      "CCOC(=O)C1(C(=O)OCC)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n",
      "CCOC(=O)C1(C(=O)CBr)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc([N+](=O)[O-])cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(C(=O)O)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc(C)cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(Cl)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc(CC)cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "COC(=O)C1(C(C)=O)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n",
      "CC(=O)C1(C(C)=O)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n",
      "CCOC(=O)C1(C(=O)OC)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(CC)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(OC)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CC(=O)C1(C(=O)c2ccccc2)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc([N+](=O)[O-])c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)C1(C(=O)C#N)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "COC(=O)C1(C(=O)OC)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc(OCC)cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(OCC)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1cccc(C)c1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)C1(C(=O)OCC)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CC(=O)C1(C(=O)c2ccccc2)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CC(=O)C1(C(C)=O)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CC(=O)C1(C(=O)Nc2ccccc2)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc(Cl)cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "NC(=S)c1nn(C2OC(CO)C(O)C2O)cp1\n",
      "CCOC(=O)C1(C(=O)CBr)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n",
      "COC(=O)C1(C(C)=O)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(C(=O)O)n12)[N+](=O)[O-]\n",
      "CCOC(=O)c1pc(P(Cl)Cl)c2n1C(N=Nc1ccc(OC)cc1)C(=O)Nc1ccc(C)cc1-2\n",
      "CCOC(=O)C1(C(=O)C#N)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n",
      "CC(=O)C1(C(=O)Nc2ccccc2)C(Cl)C(=O)N1N(c1c(O)ccc2c(P(Cl)Cl)pc(-c3ccccc3)n12)[N+](=O)[O-]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[206], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m vocabulary \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[1;32m      4\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mbasic_tokenizer\u001B[38;5;241m.\u001B[39m_split_into_tokens(x))\n\u001B[1;32m      5\u001B[0m     vocabulary \u001B[38;5;241m=\u001B[39m vocabulary \u001B[38;5;241m|\u001B[39m tokens\n",
      "File \u001B[0;32m~/Projects/jointformer/jointformer/utils/datasets/smiles/base.py:65\u001B[0m, in \u001B[0;36mSmilesDataset.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_current\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for idx, x in enumerate(dataset):\n",
    "    tokens = set(tokenizer.basic_tokenizer._split_into_tokens(x))\n",
    "    vocabulary = vocabulary | tokens\n",
    "    if 'p' in tokens:\n",
    "        print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:23:36.211858718Z",
     "start_time": "2024-04-23T12:23:27.461838121Z"
    }
   },
   "id": "331b0c2616608d0e"
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "{'%10',\n '%11',\n '%12',\n '(',\n ')',\n '-',\n '1',\n '2',\n '3',\n '4',\n '5',\n '6',\n '7',\n '8',\n '9',\n '=',\n 'B',\n 'Br',\n 'C',\n 'Cl',\n 'F',\n 'I',\n 'N',\n 'O',\n 'P',\n 'S',\n '[B-]',\n '[BH-]',\n '[BH2-]',\n '[BH3-]',\n '[B]',\n '[Br+2]',\n '[Br-]',\n '[C+]',\n '[C-]',\n '[CH+]',\n '[CH-]',\n '[CH2+]',\n '[CH2]',\n '[CH]',\n '[Cl+2]',\n '[Cl+3]',\n '[Cl+]',\n '[Cl-]',\n '[F+]',\n '[F-]',\n '[H]',\n '[I+2]',\n '[I+3]',\n '[I+]',\n '[I-]',\n '[IH2]',\n '[N+]',\n '[N-]',\n '[NH+]',\n '[NH-]',\n '[NH2+]',\n '[NH3+]',\n '[N]',\n '[O+]',\n '[O-]',\n '[OH+]',\n '[O]',\n '[P+]',\n '[P-]',\n '[PH2+]',\n '[PH]',\n '[S+]',\n '[S-]',\n '[SH+]',\n '[SH-]',\n '[SH]',\n '[Se+]',\n '[Se-]',\n '[SeH2]',\n '[SeH]',\n '[Se]',\n '[Si-]',\n '[SiH-]',\n '[SiH2]',\n '[SiH]',\n '[Si]',\n '[b-]',\n '[c+]',\n '[c-]',\n '[cH+]',\n '[cH-]',\n '[n+]',\n '[n-]',\n '[nH+]',\n '[nH]',\n '[o+]',\n '[s+]',\n '[se+]',\n '[se]',\n 'b',\n 'c',\n 'n',\n 'o',\n 'p',\n 's'}"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T12:22:23.924892460Z",
     "start_time": "2024-04-23T12:22:23.416570507Z"
    }
   },
   "id": "cc5f603be0f11c5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b6d700da3bae81b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
