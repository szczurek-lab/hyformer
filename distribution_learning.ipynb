{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Distribution Learning Benchmark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26dcdcd117802dc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-25 15:51:43.343402: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-25 15:51:43.345171: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 15:51:43.368987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-25 15:51:43.369004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-25 15:51:43.369672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-25 15:51:43.373831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-25 15:51:43.942838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from hybrid_transformer.configs.tasks.distribution_learning import DistributionLearningConfig\n",
    "from hybrid_transformer.configs.trainers.trainer import TrainerConfig\n",
    "\n",
    "from hybrid_transformer.utils.datasets.auto import AutoDataset\n",
    "from hybrid_transformer.utils.tokenizers.auto import AutoTokenizer\n",
    "\n",
    "from hybrid_transformer.utils.runtime import set_seed, get_device\n",
    "\n",
    "from hybrid_transformer.configs.models.model import ModelConfig\n",
    "from hybrid_transformer.models.auto import AutoModel\n",
    "\n",
    "from hybrid_transformer.configs.trainers.trainer import TrainerConfig\n",
    "from hybrid_transformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T14:51:44.506156575Z",
     "start_time": "2023-12-25T14:51:41.876886587Z"
    }
   },
   "id": "388ac09832efcd15"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 38.06M\n",
      "Running on a single device!\n",
      "tokens per iteration will be: 40,960\n",
      "Using cuda device\n",
      "compiling the model... (takes a ~minute)\n",
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "# 1. Args and Config \n",
    "\n",
    "args = {}\n",
    "args['out_dir'] = 'guacamol'\n",
    "args['experiment_config'] = 'guacamol' # GPT or HT\n",
    "\n",
    "\n",
    "task_config = DistributionLearningConfig(validate=False, split='test', target_label='zale', out_dir='./results/test/')\n",
    "model_config = ModelConfig(num_layers=2)\n",
    "trainer_config = TrainerConfig(batch_size=8, eval_iters=2)\n",
    "\n",
    "dataset = AutoDataset.from_config(task_config)\n",
    "tokenizer = AutoTokenizer.from_config(task_config)\n",
    "model = AutoModel.from_config(model_config)\n",
    "trainer = Trainer(config=trainer_config, model=model, train_dataset=dataset, eval_dataset=dataset, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T14:51:48.445814328Z",
     "start_time": "2023-12-25T14:51:46.566953238Z"
    }
   },
   "id": "b6acebc3b3f46dc4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/trainers/trainer.py:234\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_lr(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_num)\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_num \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaster_process:\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_num \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_only:\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/trainers/trainer.py:196\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 196\u001B[0m     losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimate_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m     raw_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodule \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mddp \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel  \u001B[38;5;66;03m# unwrap DDP container if needed\u001B[39;00m\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, val loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/hybrid-transformer/hybrid_transformer/trainers/trainer.py:179\u001B[0m, in \u001B[0;36mTrainer.estimate_loss\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    177\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_batch(split, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx:\n\u001B[0;32m--> 179\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meos_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meos_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m     losses[k] \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munsupervised_loss\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m    183\u001B[0m out[split] \u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    326\u001B[0m dynamic_ctx\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__enter__\u001B[39m()\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:490\u001B[0m, in \u001B[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001B[0;34m(frame, cache_entry, frame_state)\u001B[0m\n\u001B[1;32m    487\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compile_lock, _disable_current_modes():\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_entry\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:641\u001B[0m, in \u001B[0;36mconvert_frame.<locals>._convert_frame\u001B[0;34m(frame, cache_size, hooks, frame_state)\u001B[0m\n\u001B[1;32m    639\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 641\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43minner_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    642\u001B[0m     counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframes\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:133\u001B[0m, in \u001B[0;36mwrap_convert_context.<locals>._fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m cleanup \u001B[38;5;241m=\u001B[39m setup_compile_debug()\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     cleanup\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:389\u001B[0m, in \u001B[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001B[0;34m(frame, cache_entry, hooks, frame_state)\u001B[0m\n\u001B[1;32m    376\u001B[0m compile_id \u001B[38;5;241m=\u001B[39m CompileId(frame_id, frame_compile_id)\n\u001B[1;32m    378\u001B[0m signpost_event(\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_convert_frame_assert._compile\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     },\n\u001B[1;32m    387\u001B[0m )\n\u001B[0;32m--> 389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_globals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_locals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_builtins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompiler_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43mone_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexport_constraints\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframe_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompile_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompile_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:569\u001B[0m, in \u001B[0;36m_compile\u001B[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compile_context(CompileContext(compile_id)):\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 569\u001B[0m         guarded_code \u001B[38;5;241m=\u001B[39m \u001B[43mcompile_inner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mone_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    570\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m guarded_code\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    572\u001B[0m         Unsupported,\n\u001B[1;32m    573\u001B[0m         TorchRuntimeError,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    578\u001B[0m         ValidationException,\n\u001B[1;32m    579\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:491\u001B[0m, in \u001B[0;36m_compile.<locals>.compile_inner\u001B[0;34m(code, one_graph, hooks, transform)\u001B[0m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mcount():\n\u001B[1;32m    490\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 491\u001B[0m         out_code \u001B[38;5;241m=\u001B[39m \u001B[43mtransform_code_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    492\u001B[0m         orig_code_map[out_code] \u001B[38;5;241m=\u001B[39m code\n\u001B[1;32m    493\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1028\u001B[0m, in \u001B[0;36mtransform_code_object\u001B[0;34m(code, transformations, safe)\u001B[0m\n\u001B[1;32m   1025\u001B[0m instructions \u001B[38;5;241m=\u001B[39m cleaned_instructions(code, safe)\n\u001B[1;32m   1026\u001B[0m propagate_line_nums(instructions)\n\u001B[0;32m-> 1028\u001B[0m \u001B[43mtransformations\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstructions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:458\u001B[0m, in \u001B[0;36m_compile.<locals>.transform\u001B[0;34m(instructions, code_options)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tracing(tracer\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mtracing_context):\n\u001B[0;32m--> 458\u001B[0m         \u001B[43mtracer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (exc\u001B[38;5;241m.\u001B[39mRestartAnalysis, exc\u001B[38;5;241m.\u001B[39mSkipFrame):\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2074\u001B[0m, in \u001B[0;36mInstructionTranslator.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2073\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m-> 2074\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:724\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    720\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mpush_tx(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m (\n\u001B[1;32m    722\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minstruction_pointer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    723\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput\u001B[38;5;241m.\u001B[39mshould_exit\n\u001B[0;32m--> 724\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    725\u001B[0m     ):\n\u001B[1;32m    726\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m BackendCompilerFailed:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:688\u001B[0m, in \u001B[0;36mInstructionTranslatorBase.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    684\u001B[0m         unimplemented(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmissing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minst\u001B[38;5;241m.\u001B[39mopname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    685\u001B[0m     TracingContext\u001B[38;5;241m.\u001B[39mset_current_loc(\n\u001B[1;32m    686\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_filename, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineno, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\n\u001B[1;32m    687\u001B[0m     )\n\u001B[0;32m--> 688\u001B[0m     \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inst\u001B[38;5;241m.\u001B[39mopname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRETURN_VALUE\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:439\u001B[0m, in \u001B[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001B[0;34m(self, inst)\u001B[0m\n\u001B[1;32m    436\u001B[0m     reason \u001B[38;5;241m=\u001B[39m GraphCompileReason(excp\u001B[38;5;241m.\u001B[39mmsg, user_stack)\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrestore_graphstate(state)\n\u001B[0;32m--> 439\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_subgraph\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreason\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreason\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    440\u001B[0m cg \u001B[38;5;241m=\u001B[39m PyCodegen(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    441\u001B[0m cleanup: List[Instruction] \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:857\u001B[0m, in \u001B[0;36mOutputGraph.compile_subgraph\u001B[0;34m(self, tx, partial_convert, reason)\u001B[0m\n\u001B[1;32m    854\u001B[0m output \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m count_calls(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pass2\u001B[38;5;241m.\u001B[39mgraph_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    856\u001B[0m     output\u001B[38;5;241m.\u001B[39mextend(\n\u001B[0;32m--> 857\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_and_call_fx_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpass2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_output_vars\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    858\u001B[0m     )\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pass2\u001B[38;5;241m.\u001B[39mgraph_outputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    861\u001B[0m         output\u001B[38;5;241m.\u001B[39mappend(pass2\u001B[38;5;241m.\u001B[39mcreate_store(graph_output_var))\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/contextlib.py:81\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:957\u001B[0m, in \u001B[0;36mOutputGraph.compile_and_call_fx_graph\u001B[0;34m(self, tx, rv, root)\u001B[0m\n\u001B[1;32m    952\u001B[0m graph_tabular_log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, lazy_format_graph_tabular(name, gm))\n\u001B[1;32m    953\u001B[0m graph_sizes_log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    954\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, LazyString(\u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_graph_sizes_log_str(name))\n\u001B[1;32m    955\u001B[0m )\n\u001B[0;32m--> 957\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_user_compiler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    958\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m disable(compiled_fn)\n\u001B[1;32m    960\u001B[0m counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstats\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munique_graphs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1024\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     unimplemented_with_warning(e, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot_tx\u001B[38;5;241m.\u001B[39mf_code, msg)\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BackendCompilerFailed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompiler_fn, e)\u001B[38;5;241m.\u001B[39mwith_traceback(\n\u001B[1;32m   1025\u001B[0m         e\u001B[38;5;241m.\u001B[39m__traceback__\n\u001B[1;32m   1026\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m signpost_event(\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutputGraph.call_user_compiler\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1036\u001B[0m     },\n\u001B[1;32m   1037\u001B[0m )\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_fn\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1009\u001B[0m, in \u001B[0;36mOutputGraph.call_user_compiler\u001B[0;34m(self, gm)\u001B[0m\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mverify_correctness:\n\u001B[1;32m   1008\u001B[0m     compiler_fn \u001B[38;5;241m=\u001B[39m WrapperBackend(compiler_fn)\n\u001B[0;32m-> 1009\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexample_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1010\u001B[0m _step_logger()(logging\u001B[38;5;241m.\u001B[39mINFO, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone compiler function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(compiled_fn), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompiler_fn did not return callable\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001B[0m, in \u001B[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001B[0;34m(gm, example_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     compiled_gm \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m compiled_gm\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/__init__.py:1568\u001B[0m, in \u001B[0;36m_TorchCompileInductorWrapper.__call__\u001B[0;34m(self, model_, inputs_)\u001B[0m\n\u001B[1;32m   1565\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_, inputs_):\n\u001B[1;32m   1566\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_inductor\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompile_fx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compile_fx\n\u001B[0;32m-> 1568\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompile_fx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig_patches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1150\u001B[0m, in \u001B[0;36mcompile_fx\u001B[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001B[0m\n\u001B[1;32m   1143\u001B[0m tracing_context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1144\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext\u001B[38;5;241m.\u001B[39mget() \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext(fake_mode)\n\u001B[1;32m   1145\u001B[0m )\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m V\u001B[38;5;241m.\u001B[39mset_fake_mode(fake_mode), torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mtracing(  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1148\u001B[0m     tracing_context\n\u001B[1;32m   1149\u001B[0m ), compiled_autograd\u001B[38;5;241m.\u001B[39mdisable():\n\u001B[0;32m-> 1150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43maot_autograd\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfw_compiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_compiler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbw_compiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbw_compiler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1153\u001B[0m \u001B[43m        \u001B[49m\u001B[43minference_compiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minference_compiler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecompositions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecompositions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartition_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartition_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_inference_input_mutations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1157\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs_\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:55\u001B[0m, in \u001B[0;36maot_autograd.<locals>.compiler_fn\u001B[0;34m(gm, example_inputs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# NB: NOT cloned!\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m enable_aot_logging(), patch_config:\n\u001B[0;32m---> 55\u001B[0m         cg \u001B[38;5;241m=\u001B[39m \u001B[43maot_module_simplified\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m         counters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maot_autograd\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m disable(cg)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3891\u001B[0m, in \u001B[0;36maot_module_simplified\u001B[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001B[0m\n\u001B[1;32m   3875\u001B[0m aot_config \u001B[38;5;241m=\u001B[39m AOTConfig(\n\u001B[1;32m   3876\u001B[0m     fw_compiler\u001B[38;5;241m=\u001B[39mfw_compiler,\n\u001B[1;32m   3877\u001B[0m     bw_compiler\u001B[38;5;241m=\u001B[39mbw_compiler,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3887\u001B[0m     no_tangents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   3888\u001B[0m )\n\u001B[1;32m   3890\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m compiled_autograd\u001B[38;5;241m.\u001B[39mdisable():\n\u001B[0;32m-> 3891\u001B[0m     compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_aot_dispatcher_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunctional_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3893\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3894\u001B[0m \u001B[43m        \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3895\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3897\u001B[0m \u001B[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001B[39;00m\n\u001B[1;32m   3898\u001B[0m \u001B[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001B[39;00m\n\u001B[1;32m   3899\u001B[0m \u001B[38;5;66;03m# historically returned a function that was not the boxed calling\u001B[39;00m\n\u001B[1;32m   3900\u001B[0m \u001B[38;5;66;03m# convention.  This should get fixed...\u001B[39;00m\n\u001B[1;32m   3901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39mruntime_args):\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:3429\u001B[0m, in \u001B[0;36mcreate_aot_dispatcher_function\u001B[0;34m(flat_fn, flat_args, aot_config)\u001B[0m\n\u001B[1;32m   3426\u001B[0m compiler_fn \u001B[38;5;241m=\u001B[39m partial(aot_wrapper_dedupe, compiler_fn\u001B[38;5;241m=\u001B[39mcompiler_fn)\n\u001B[1;32m   3427\u001B[0m \u001B[38;5;66;03m# You can put more passes here\u001B[39;00m\n\u001B[0;32m-> 3429\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfake_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfw_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_config\u001B[38;5;241m.\u001B[39mis_export:\n\u001B[1;32m   3432\u001B[0m     mutated_user_inp_locs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3433\u001B[0m         idx \u001B[38;5;241m-\u001B[39m aot_config\u001B[38;5;241m.\u001B[39mnum_params_buffers\n\u001B[1;32m   3434\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m fw_metadata\u001B[38;5;241m.\u001B[39mmutated_inp_indices\n\u001B[1;32m   3435\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m aot_config\u001B[38;5;241m.\u001B[39mnum_params_buffers\n\u001B[1;32m   3436\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2212\u001B[0m, in \u001B[0;36maot_wrapper_dedupe\u001B[0;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001B[0m\n\u001B[1;32m   2209\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   2211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ok:\n\u001B[0;32m-> 2212\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mleaf_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfw_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2214\u001B[0m \u001B[38;5;66;03m# export path: ban duplicate inputs for now, add later if requested.\u001B[39;00m\n\u001B[1;32m   2215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_config\u001B[38;5;241m.\u001B[39mis_export:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:2392\u001B[0m, in \u001B[0;36maot_wrapper_synthetic_base\u001B[0;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001B[0m\n\u001B[1;32m   2390\u001B[0m \u001B[38;5;66;03m# Happy path: we don't need synthetic bases\u001B[39;00m\n\u001B[1;32m   2391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synthetic_base_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mflat_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maot_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfw_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfw_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2394\u001B[0m \u001B[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001B[39;00m\n\u001B[1;32m   2395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_config\u001B[38;5;241m.\u001B[39mis_export:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1573\u001B[0m, in \u001B[0;36maot_dispatch_base\u001B[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001B[0m\n\u001B[1;32m   1571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext\u001B[38;5;241m.\u001B[39mget():\n\u001B[1;32m   1572\u001B[0m         torch\u001B[38;5;241m.\u001B[39m_guards\u001B[38;5;241m.\u001B[39mTracingContext\u001B[38;5;241m.\u001B[39mget()\u001B[38;5;241m.\u001B[39mfw_metadata \u001B[38;5;241m=\u001B[39m fw_metadata\n\u001B[0;32m-> 1573\u001B[0m     compiled_fw \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfw_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1575\u001B[0m \u001B[38;5;66;03m# This boxed_call handling happens inside create_runtime_wrapper as well.\u001B[39;00m\n\u001B[1;32m   1576\u001B[0m \u001B[38;5;66;03m# However, create_runtime_wrapper does not expect the rng offsets in the\u001B[39;00m\n\u001B[1;32m   1577\u001B[0m \u001B[38;5;66;03m# output. So, we have to create another wrapper and take out the offset. As\u001B[39;00m\n\u001B[1;32m   1578\u001B[0m \u001B[38;5;66;03m# a result, we have to account for not boxed_call compilers as well.\u001B[39;00m\n\u001B[1;32m   1579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(compiled_fw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_boxed_call\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1092\u001B[0m, in \u001B[0;36mcompile_fx.<locals>.fw_compiler_base\u001B[0;34m(model, example_inputs, is_inference)\u001B[0m\n\u001B[1;32m   1070\u001B[0m     \u001B[38;5;66;03m# We makes the following assumption\u001B[39;00m\n\u001B[1;32m   1071\u001B[0m     \u001B[38;5;66;03m# For inference\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m     \u001B[38;5;66;03m#   len(orig_model_outputs) == len(model_outputs)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1080\u001B[0m     \u001B[38;5;66;03m# To make things safe, we'll use original_output_start_index field\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m     \u001B[38;5;66;03m# set by AOTAutograd to decide where the original module outputs start.\u001B[39;00m\n\u001B[1;32m   1083\u001B[0m     user_visible_outputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   1084\u001B[0m         n\u001B[38;5;241m.\u001B[39mname\n\u001B[1;32m   1085\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m model_outputs[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1089\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(n, torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mNode)\n\u001B[1;32m   1090\u001B[0m     }\n\u001B[0;32m-> 1092\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1093\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1094\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1095\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_fixed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1096\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcudagraphs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcudagraphs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgraph_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_inference\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_inference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1099\u001B[0m \u001B[43m    \u001B[49m\u001B[43mboxed_forward_device_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforward_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1100\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_visible_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_visible_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py:80\u001B[0m, in \u001B[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001B[0;34m(gm, example_inputs, **kwargs)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m config\u001B[38;5;241m.\u001B[39mrepro_after \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynamo\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maot\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001B[39;00m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# with fake inputs\u001B[39;00m\n\u001B[0;32m---> 80\u001B[0m     inner_compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mcompiler_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001B[39;00m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;66;03m# need a different serialization strategy\u001B[39;00m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mrepro_after \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maot\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/debug.py:228\u001B[0m, in \u001B[0;36mDebugContext.wrap.<locals>.inner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m DebugContext():\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/contextlib.py:81\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:54\u001B[0m, in \u001B[0;36mtime_and_log.<locals>.wrap.<locals>.newFunction\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(old_func)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnewFunction\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mold_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:341\u001B[0m, in \u001B[0;36mcompile_fx_inner\u001B[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt)\u001B[0m\n\u001B[1;32m    328\u001B[0m graph_args \u001B[38;5;241m=\u001B[39m [gm, example_inputs]\n\u001B[1;32m    329\u001B[0m graph_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcudagraphs\u001B[39m\u001B[38;5;124m\"\u001B[39m: cudagraphs,\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_fixed\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_fixed,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayout_opt\u001B[39m\u001B[38;5;124m\"\u001B[39m: layout_opt,\n\u001B[1;32m    339\u001B[0m }\n\u001B[0;32m--> 341\u001B[0m compiled_graph: CompiledFxGraph \u001B[38;5;241m=\u001B[39m \u001B[43mfx_codegen_and_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgraph_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgraph_kwargs\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m    343\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aot_mode:\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m compiled_graph\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:565\u001B[0m, in \u001B[0;36mfx_codegen_and_compile\u001B[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt)\u001B[0m\n\u001B[1;32m    563\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    564\u001B[0m             context\u001B[38;5;241m.\u001B[39moutput_strides\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 565\u001B[0m compiled_fn \u001B[38;5;241m=\u001B[39m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_to_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m graph\u001B[38;5;241m.\u001B[39mdisable_cudagraphs:\n\u001B[1;32m    568\u001B[0m     BoxedBool\u001B[38;5;241m.\u001B[39mdisable(cudagraphs)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:970\u001B[0m, in \u001B[0;36mGraphLowering.compile_to_fn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m AotCodeCache\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;28mself\u001B[39m, code, cuda\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcuda)\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile_to_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcall\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:938\u001B[0m, in \u001B[0;36mGraphLowering.compile_to_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    934\u001B[0m \u001B[38;5;129m@dynamo_timed\u001B[39m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompile_to_module\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcodecache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyCodeCache\n\u001B[0;32m--> 938\u001B[0m     code, linemap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcodegen\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    939\u001B[0m     linemap \u001B[38;5;241m=\u001B[39m [(line_no, node\u001B[38;5;241m.\u001B[39mstack_trace) \u001B[38;5;28;01mfor\u001B[39;00m line_no, node \u001B[38;5;129;01min\u001B[39;00m linemap]\n\u001B[1;32m    940\u001B[0m     key, path \u001B[38;5;241m=\u001B[39m PyCodeCache\u001B[38;5;241m.\u001B[39mwrite(code)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/graph.py:913\u001B[0m, in \u001B[0;36mGraphLowering.codegen\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    909\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscheduler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Scheduler\n\u001B[1;32m    911\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_wrapper_code()\n\u001B[0;32m--> 913\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler \u001B[38;5;241m=\u001B[39m \u001B[43mScheduler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# mypy can't figure this out\u001B[39;00m\n\u001B[1;32m    915\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscheduler\u001B[38;5;241m.\u001B[39mcodegen()\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001B[0m, in \u001B[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (dynamo_timed)\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    188\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m--> 189\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     time_spent \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m t0\n\u001B[1;32m    191\u001B[0m compilation_time_metrics[key]\u001B[38;5;241m.\u001B[39mappend(time_spent)\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:971\u001B[0m, in \u001B[0;36mScheduler.__init__\u001B[0;34m(self, nodes)\u001B[0m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnodes \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    966\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavailable_buffer_names \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    967\u001B[0m     \u001B[38;5;241m*\u001B[39mV\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mgraph_inputs\u001B[38;5;241m.\u001B[39mkeys(),\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;241m*\u001B[39mV\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mconstants\u001B[38;5;241m.\u001B[39mkeys(),\n\u001B[1;32m    969\u001B[0m }\n\u001B[0;32m--> 971\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnodes \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_scheduler_node\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;66;03m# some new constants could have been created above\u001B[39;00m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavailable_buffer_names\u001B[38;5;241m.\u001B[39mupdate(V\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mconstants\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:971\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnodes \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    966\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavailable_buffer_names \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    967\u001B[0m     \u001B[38;5;241m*\u001B[39mV\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mgraph_inputs\u001B[38;5;241m.\u001B[39mkeys(),\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;241m*\u001B[39mV\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mconstants\u001B[38;5;241m.\u001B[39mkeys(),\n\u001B[1;32m    969\u001B[0m }\n\u001B[0;32m--> 971\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnodes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_scheduler_node\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m nodes]\n\u001B[1;32m    973\u001B[0m \u001B[38;5;66;03m# some new constants could have been created above\u001B[39;00m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavailable_buffer_names\u001B[38;5;241m.\u001B[39mupdate(V\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mconstants\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:1037\u001B[0m, in \u001B[0;36mScheduler.create_scheduler_node\u001B[0;34m(self, node)\u001B[0m\n\u001B[1;32m   1035\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m NopKernelSchedulerNode(\u001B[38;5;28mself\u001B[39m, node)\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node, (ir\u001B[38;5;241m.\u001B[39mComputedBuffer, ir\u001B[38;5;241m.\u001B[39mTemplateBuffer)):\n\u001B[0;32m-> 1037\u001B[0m     group_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_backend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgroup_fn\n\u001B[1;32m   1038\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SchedulerNode(\u001B[38;5;28mself\u001B[39m, node, group_fn)\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node, ir\u001B[38;5;241m.\u001B[39mExternKernel):\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:1642\u001B[0m, in \u001B[0;36mScheduler.get_backend\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m   1640\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_backend\u001B[39m(\u001B[38;5;28mself\u001B[39m, device: torch\u001B[38;5;241m.\u001B[39mdevice):\n\u001B[1;32m   1641\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackends:\n\u001B[0;32m-> 1642\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackends[device] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_backend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1643\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackends[device]\n",
      "File \u001B[0;32m~/miniconda3/envs/hybrid-transformer/lib/python3.11/site-packages/torch/_inductor/scheduler.py:1634\u001B[0m, in \u001B[0;36mScheduler.create_backend\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m   1630\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1631\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice_props\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice_props\u001B[38;5;241m.\u001B[39mmajor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice_props\u001B[38;5;241m.\u001B[39mminor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# noqa: B950\u001B[39;00m\n\u001B[1;32m   1632\u001B[0m         )\n\u001B[1;32m   1633\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1634\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1635\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# noqa: B950\u001B[39;00m\n\u001B[1;32m   1636\u001B[0m         )\n\u001B[1;32m   1638\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m device_scheduling(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mBackendCompilerFailed\u001B[0m: backend='inductor' raised:\nRuntimeError: Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T14:52:05.269705674Z",
     "start_time": "2023-12-25T14:51:56.821249112Z"
    }
   },
   "id": "90267322f41e1692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7898c51dab1353af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
