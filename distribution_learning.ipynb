{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Run Distribution Learning Benchmark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26dcdcd117802dc"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from hybrid_transformer.configs.tasks.distribution_learning import DistributionLearningConfig\n",
    "from hybrid_transformer.configs.trainers.trainer import TrainerConfig\n",
    "\n",
    "from hybrid_transformer.utils.datasets.auto import AutoDataset\n",
    "from hybrid_transformer.utils.tokenizers.auto import AutoTokenizer\n",
    "\n",
    "from hybrid_transformer.utils.runtime import set_seed, get_device\n",
    "\n",
    "from hybrid_transformer.configs.models.model import ModelConfig\n",
    "from hybrid_transformer.models.auto import AutoModel\n",
    "\n",
    "from hybrid_transformer.configs.trainers.trainer import TrainerConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:44:41.228453246Z",
     "start_time": "2023-12-22T14:44:41.087347269Z"
    }
   },
   "id": "388ac09832efcd15"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "number of parameters: 38.06M\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(588, 512)\n    (wpe): Embedding(128, 512)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x HybridTransformerBlock(\n        (ln_1): LayerNorm()\n        (attn_1): HybridSelfAttention(\n          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n          (kv_proj): Linear(in_features=512, out_features=1024, bias=False)\n          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (fc): Linear(in_features=512, out_features=2048, bias=False)\n          (gelu): GELU(approximate='none')\n          (proj): Linear(in_features=2048, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=512, out_features=588, bias=False)\n  (prediction_head): Linear(in_features=512, out_features=1, bias=False)\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Args and Config \n",
    "\n",
    "args = {}\n",
    "args['out_dir'] = 'guacamol'\n",
    "args['experiment_config'] = 'guacamol' # GPT or HT\n",
    "\n",
    "device = get_device()\n",
    "set_seed()\n",
    "\n",
    "task_config = DistributionLearningConfig(validate=False, split='test', target_label='zale', out_dir='./results/test/')\n",
    "model_config = ModelConfig()\n",
    "\n",
    "dataset = AutoDataset.from_config(task_config)\n",
    "tokenizer = AutoTokenizer.from_config(task_config)\n",
    "model = AutoModel.from_config(model_config)\n",
    "model.train()\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:48:29.136213433Z",
     "start_time": "2023-12-22T14:48:28.599334656Z"
    }
   },
   "id": "b6acebc3b3f46dc4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "inputs = tokenizer.get_inputs(dataset, 'lm', 2, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:18:11.979011081Z",
     "start_time": "2023-12-22T13:18:11.951842136Z"
    }
   },
   "id": "c9e796dde89d0b96"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "outputs = model(input_ids=inputs['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:18:13.013440256Z",
     "start_time": "2023-12-22T13:18:12.886383555Z"
    }
   },
   "id": "5721ca3e6729e8be"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Make a train loop "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:15:07.007175905Z",
     "start_time": "2023-12-22T14:15:06.960598401Z"
    }
   },
   "id": "a8bdf9d520231807"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfig()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:44:44.433355435Z",
     "start_time": "2023-12-22T14:44:44.406373905Z"
    }
   },
   "id": "38ac8b2f8673f1ed"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:46:09.939162468Z",
     "start_time": "2023-12-22T14:46:09.898291997Z"
    }
   },
   "id": "6fff3f8482a903f0"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 61,440\n"
     ]
    }
   ],
   "source": [
    "# various inits, derived attributes, I/O setup\n",
    "\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(task_config.out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[trainer_config.dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:48:33.232524550Z",
     "start_time": "2023-12-22T14:48:33.204264139Z"
    }
   },
   "id": "df018e3a14b4b079"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:50:21.199233734Z",
     "start_time": "2023-12-22T14:50:21.157450222Z"
    }
   },
   "id": "d701279b8641cfef"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    if meta_vocab_size is None:\n",
    "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    # resume training from a checkpoint.\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    # create the model\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    # read off the created config params, so we can store them into checkpoint correctly\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d94681974622292"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 63, with 38,115,840 parameters\n",
      "num non-decayed parameter tensors: 25, with 12,800 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if trainer_config.init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T14:52:21.259518614Z",
     "start_time": "2023-12-22T14:52:20.396796745Z"
    }
   },
   "id": "b6f23ec2c614460d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3541cee71e9fb6f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###\n",
    "# training loop # train function definition\n",
    "###\n",
    "\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        # eval function\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17b127ec4c9bdf6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
