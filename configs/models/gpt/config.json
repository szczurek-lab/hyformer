{
  "model_type": "GPT",
  "bias": false,
  "dropout": 0.1,
  "embedding_dim": 512,
  "layer_norm_eps": 1e-05,
  "max_seq_len": 128,
  "num_heads": 8,
  "num_layers": 12,
  "transformers_version": "4.36.1",
  "vocab_size": 588
}
